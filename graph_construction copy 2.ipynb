{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252b411a-36f0-43b0-992d-93d8e405a755",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnx\u001b[39;00m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "import time\n",
    "import traceback \n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import random \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a07472-85a1-417e-b92f-e5ccf005b66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "def process_data(otu_file_path, metadata_file_path, output_dir,\n",
    "                 sample_id='Sample', condition_id='Study.Group'):\n",
    "    \"\"\"\n",
    "    Reads OTU table and metadata, aligns them, cleans and normalizes\n",
    "    the OTU table to relative abundances. Saves processed files.\n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\" Reading OTU table at: {otu_file_path}\")\n",
    "    print(f\" Reading metadata at: {metadata_file_path}\")\n",
    "\n",
    "    processed_data_dir = os.path.join(output_dir, \"01_Processed_Data\")\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    sep_otu = '\\t' if otu_file_path.lower().endswith('.tsv') else ','\n",
    "    sep_meta = '\\t' if metadata_file_path.lower().endswith('.tsv') else ','\n",
    "\n",
    " \n",
    "    otu_table = pd.read_csv(otu_file_path, index_col=0, sep=sep_otu)\n",
    "    print(f\"  Initial OTU table shape: {otu_table.shape}\")\n",
    "    metadata = pd.read_csv(metadata_file_path, sep=sep_meta)\n",
    "    print(f\"  Initial Metadata shape: {metadata.shape}\")\n",
    "\n",
    "   \n",
    "    #Metadata validation\n",
    "    if sample_id not in metadata.columns:\n",
    "        raise ValueError(f\"Sample ID column '{sample_id}' not found in metadata. Available columns: {metadata.columns.tolist()}\")\n",
    "    if condition_id not in metadata.columns:\n",
    "        raise ValueError(f\"Condition ID column '{condition_id}' not found in metadata. Available columns: {metadata.columns.tolist()}\")\n",
    "    \n",
    "    print(f\"  Using Sample ID column: '{sample_id}'\")\n",
    "    print(f\"  Using Condition ID column: '{condition_id}'\")\n",
    "\n",
    "    metadata[sample_id] = metadata[sample_id].astype(str)\n",
    "\n",
    "    if not metadata[sample_id].is_unique:\n",
    "        num_duplicates = metadata[sample_id].duplicated().sum()\n",
    "        print(f\"  Warning: Sample ID column '{sample_id}' contains {num_duplicates} duplicate values. Keeping first occurrence of each.\")\n",
    "        metadata = metadata.drop_duplicates(subset=[sample_id], keep='first')\n",
    "        print(f\"  Metadata shape after dropping duplicates: {metadata.shape}\")\n",
    "    metadata.set_index(sample_id, inplace=True)\n",
    "    otu_table.index = otu_table.index.astype(str)\n",
    "\n",
    "    common_samples = otu_table.index.intersection(metadata.index)\n",
    "    print(f\"  Found {len(common_samples)} common samples between OTU table and metadata.\")\n",
    "    if len(common_samples) == 0:\n",
    "        raise ValueError(\"No common samples found. Check Sample ID matching and formatting\")\n",
    "    otu_table = otu_table.loc[common_samples]\n",
    "    metadata = metadata.loc[common_samples]\n",
    "    \n",
    "    print(f\"  Cleaned feature table shape: {otu_table.shape}\")\n",
    "    print(f\"  Cleaned metadata shape: {metadata.shape}\")\n",
    "\n",
    "    otu_table = otu_table.astype(float)\n",
    "\n",
    "    initial_otus = otu_table.shape[1]\n",
    "    otu_table = otu_table.loc[:, (otu_table > 0).any(axis=0)] \n",
    "    otus_removed = initial_otus - otu_table.shape[1]\n",
    "    if otus_removed > 0:\n",
    "        print(f\"  Removed {otus_removed} OTUs that were all zero across samples.\")\n",
    "\n",
    "    sample_sums = otu_table.sum(axis=1)\n",
    "    valid_samples_mask = sample_sums > 1e-9 \n",
    "    if (~valid_samples_mask).any():\n",
    "        num_zero_sum = (~valid_samples_mask).sum()\n",
    "        print(f\"  Warning: Removing {num_zero_sum} samples with zero total abundance before normalization.\")\n",
    "        otu_table = otu_table.loc[valid_samples_mask]\n",
    "        metadata = metadata.loc[valid_samples_mask]\n",
    "        sample_sums = sample_sums.loc[valid_samples_mask]\n",
    "\n",
    "    \n",
    "    # Normalize to relative abundance\n",
    "    otu_table = otu_table.div(sample_sums, axis=0)\n",
    "    otu_table = otu_table.fillna(0) #This technically shouldn't do anything due to check above\n",
    "    final_otu_table = otu_table\n",
    "    print(f\"  Normalized OTU table to relative abundances. Final shape: {final_otu_table.shape}\")\n",
    "\n",
    "    # Grouping \n",
    "    otu_table_grouped = None\n",
    "    if condition_id in metadata.columns and not final_otu_table.empty:\n",
    "       \n",
    "         if not metadata.empty:\n",
    "            \n",
    "             metadata[condition_id] = metadata[condition_id].astype(str)\n",
    "             otu_table_grouped = final_otu_table.groupby(metadata[condition_id]).mean()\n",
    "             print(f\"  Grouped table shape (for info): {otu_table_grouped.shape}\") \n",
    "\n",
    "    # Save Processed Data\n",
    "    ft_path = os.path.join(processed_data_dir, \"feature_table_normalized.csv\")\n",
    "    meta_path = os.path.join(processed_data_dir, \"metadata_aligned.csv\")\n",
    "    final_otu_table.to_csv(ft_path)\n",
    "    metadata.to_csv(meta_path)\n",
    "    print(f\"  Saved normalized feature table to: {ft_path}\")\n",
    "    print(f\"  Saved aligned metadata to: {meta_path}\")\n",
    "\n",
    "    print(\" Data processing function finished.\")\n",
    "\n",
    "    return final_otu_table, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32219c56-0ccd-4740-a945-f2f1acf84a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_sample_weights(sample_rel_otu):\n",
    "    \"\"\"Calculates pairwise weights within a single sample based on relative abundance.\"\"\"\n",
    "    sample_rel_otu = np.asarray(sample_rel_otu)\n",
    "    n_species = len(sample_rel_otu)\n",
    "\n",
    "    binary_sample = (sample_rel_otu > 1e-9).astype(int)\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample) # Co-occurrence matrix for particular sample\n",
    "    sample_matrix = np.tile(sample_rel_otu, (n_species, 1)) # Repeat sample row into a matrix\n",
    "\n",
    "    original_array = sample_rel_otu \n",
    "    non_zero_mask = original_array > 1e-9 # Mask for species present in the sample\n",
    "\n",
    "    # Calculate 1/abundance only for present species\n",
    "    inverted_non_zero_elements = np.zeros_like(original_array, dtype=float)\n",
    "    inverted_non_zero_elements = np.divide(1.0, original_array, where=non_zero_mask, out=inverted_non_zero_elements)\n",
    "    inv_diag = np.diag(inverted_non_zero_elements) # Diagonal matrix of 1/abundance_i\n",
    "\n",
    "    # Calculate ratios: R_ij = abundance_j / abundance_i\n",
    "    ratios = np.matmul(inv_diag, sample_matrix)\n",
    "\n",
    "    # Calculate weights: W_ij = 2 / R_ij = 2 * abundance_i / abundance_j\n",
    "    weights = np.zeros_like(ratios)\n",
    "    non_diagonal_mask = ~np.eye(n_species, dtype=bool) # Exclude self-interactions\n",
    "    \n",
    "    # Calculate weights only where ratios are valid (non-zero) and not on the diagonal\n",
    "    valid_ratios_mask = (np.abs(ratios) > 1e-9) & non_diagonal_mask\n",
    "    weights = np.divide(2.0, ratios, where=valid_ratios_mask, out=weights)\n",
    "\n",
    "    # Symmetrization using upper triangle: W'_ij = W_ij, W'_ji = W_ij for i < j\n",
    "    weights_new = np.triu(weights, k=1) # Take the upper triangle (excluding diagonal)\n",
    "    weights_new = weights_new + weights_new.T # Make it symmetric by adding its transpose\n",
    "    weights_new[~np.isfinite(weights_new)] = 0 # Handle potential infinities/NaNs from division by ~zero\n",
    "\n",
    "    # sample_binary_matrix indicates co-occurrence (1 if both i and j are present, 0 otherwise)\n",
    "    return weights_new, sample_binary_matrix\n",
    "\n",
    "def compute_all_weights(otu_data_subset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes an average weight matrix across a set of samples (e.g., bootstrap replicate).\n",
    "    Input is assumed to be relative abundances for the samples in the subset.\n",
    "    \"\"\"\n",
    "    \n",
    "    otu_data_subset = np.asarray(otu_data_subset)\n",
    "    relative_raw = otu_data_subset\n",
    "\n",
    "    num_samples, num_species = relative_raw.shape\n",
    "    \n",
    "    combined_weights = np.zeros((num_species, num_species))\n",
    "    \n",
    "    # This cooc_matrix sums the sample co-occurrence matrices (1 if both present in sample)\n",
    "    total_cooc_matrix = np.zeros((num_species, num_species))\n",
    "\n",
    "    valid_samples_count = 0\n",
    "    for i in range(num_samples):\n",
    "        sample = relative_raw[i, :]\n",
    "\n",
    "        w, cooc = compute_sample_weights(sample)\n",
    "    \n",
    "        if w.shape == combined_weights.shape and cooc.shape == total_cooc_matrix.shape:\n",
    "            combined_weights += w # Sum of individual sample weights W_ij\n",
    "            total_cooc_matrix += cooc # Sum of co-occurrence indicators (counts how many samples have both i and j)\n",
    "            valid_samples_count += 1\n",
    "      \n",
    "    # Average weights: Divide sum of weights by number of times species co-occurred across the samples in the subset\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # average_weight_matrix = combined_weights / total_cooc_matrix\n",
    "        average_weight_matrix = np.divide(combined_weights, total_cooc_matrix,\n",
    "                                          where=total_cooc_matrix!=0,\n",
    "                                          out=np.zeros_like(combined_weights))\n",
    "    average_weight_matrix[~np.isfinite(average_weight_matrix)] = 0 \n",
    "\n",
    "    # total_cooc_matrix represents the count of samples where both species co-occurred in this subset\n",
    "    return average_weight_matrix, total_cooc_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150eb29-786a-4f18-b209-8f04255782dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bootstrap_population(observed_data_df, condition_group, output_dir_base,\n",
    "                                  n_boots): \n",
    "    \"\"\"\n",
    "    Generates bootstrap weight matrices for a given condition's data.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\" Bootstrapping {n_boots} replicates for condition: {condition_group}\")\n",
    "    # Define output path for intermediate bootstrap results\n",
    "    condition_results_dir = os.path.join(output_dir_base, \"Intermediates\", condition_group)\n",
    "    matrices_dir = os.path.join(condition_results_dir, \"matrices\")\n",
    "    os.makedirs(matrices_dir, exist_ok=True)\n",
    "    print(f\"   Bootstrap intermediate matrices will be saved in: {matrices_dir}\")\n",
    "\n",
    "    # Input is expected to be relative abundances already (the subset for the condition)\n",
    "    raw_data = observed_data_df.to_numpy()\n",
    "    n_samples, num_species = raw_data.shape\n",
    "    if n_samples == 0 or num_species < 2: # Need at least 2 species for interactions\n",
    "        print(f\"   ERROR: Not enough data for bootstrapping ({n_samples} samples, {num_species} species).\")\n",
    "        return None\n",
    "    \n",
    "    # Generate bootstrap datasets (indices get resampled data)\n",
    "    # Each bootstrap dataset contains n_samples drawn with replacement from the original n_samples\n",
    "    bootstrap_indices_list = [np.random.choice(n_samples, size=n_samples, replace=True) for _ in range(n_boots)]\n",
    "    bstrap_otus_datasets = [raw_data[indices] for indices in bootstrap_indices_list]\n",
    "\n",
    "    # Function to process a single bootstrap replicate dataset\n",
    "    def process_bootstrap_sample(b, otu_sample_replicate):\n",
    "        # Calculate the average weight matrix across the samples within this bootstrap replicate\n",
    "        w, _ = compute_all_weights(otu_sample_replicate)\n",
    "\n",
    "        if w.shape == (num_species, num_species):\n",
    "            matrix_path = os.path.join(matrices_dir, f\"bstrap_weight_matrix_{b}.csv\")\n",
    "            np.savetxt(matrix_path, w, delimiter=\",\")\n",
    "            return w\n",
    "        else: \n",
    "             print(f\"   Warning: Shape mismatch for bootstrap replicate {b}. Expected ({num_species},{num_species}), got {w.shape}\")\n",
    "             return np.zeros((num_species, num_species))\n",
    "\n",
    "    # Run the bootstrap replicate processing in parallel\n",
    "    results_matrices = Parallel(n_jobs=-1)(delayed(process_bootstrap_sample)(b, otu_rep)\n",
    "                                             for b, otu_rep in enumerate(tqdm(bstrap_otus_datasets, desc=f\"  Bootstrapping {condition_group}\", leave=False, ncols=100)))\n",
    "\n",
    "    # Filter out potential None or incorrectly shaped arrays before stacking\n",
    "    valid_results_matrices = [m for m in results_matrices if isinstance(m, np.ndarray) and m.shape == (num_species, num_species)]\n",
    "\n",
    "    if not valid_results_matrices:\n",
    "        print(f\"   ERROR: No valid bootstrap matrices were generated for {condition_group}.\")\n",
    "        if not os.listdir(matrices_dir): os.rmdir(matrices_dir)\n",
    "        if not os.listdir(condition_results_dir): os.rmdir(condition_results_dir)\n",
    "        return None\n",
    "    elif len(valid_results_matrices) < n_boots * 0.5: # Warning if less than half succeeded\n",
    "        print(f\"   WARNING: Only {len(valid_results_matrices)}/{n_boots} bootstrap replicates generated valid matrices for {condition_group}.\")\n",
    "\n",
    "\n",
    "    print(\"   Calculating mean and std deviation across bootstrap matrices...\")\n",
    "    stacked_matrices = np.stack(valid_results_matrices, axis=0)\n",
    "    \n",
    "    # Calculate the mean and std dev for each edge weight across all bootstrap replicates\n",
    "    bstrap_means = np.mean(stacked_matrices, axis=0)\n",
    "    bstrap_stds = np.std(stacked_matrices, axis=0)\n",
    "\n",
    "    # Save the overall mean and std dev matrices derived from the bootstrap population\n",
    "    np.savetxt(os.path.join(condition_results_dir, f\"means.csv\"), bstrap_means, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(condition_results_dir, f\"stds.csv\"), bstrap_stds, delimiter=\",\")\n",
    "    print(f\"   Saved mean and std matrices to: {condition_results_dir}\")\n",
    "    return condition_results_dir\n",
    "\n",
    "def filtering_pvals_for_each_sample(df_cond, condition_group, bstrap_intermed_dir,\n",
    "                                      output_dir_base, pval_thresh,\n",
    "                                      delete_bootstrap_matrices=True):\n",
    "    \"\"\"\n",
    "    Filters each individual sample graph based on comparison to the bootstrap distributions derived from the *condition's samples.\n",
    "    Saves filtered weights matrix and graphml graph per sample.\n",
    "    Saves condition-level summary and filtered OTU table for the condition.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"  Filtering individual samples for condition: {condition_group} (p-value < {pval_thresh} removes edge)\")\n",
    "    matrices_dir = os.path.join(bstrap_intermed_dir, \"matrices\") \n",
    "\n",
    "    condition_summary_dir = os.path.join(output_dir_base, \"Condition_Summaries\")\n",
    "    sample_results_base_dir = os.path.join(output_dir_base, \"Individual_Samples_by_Condition\", condition_group)\n",
    "    os.makedirs(condition_summary_dir, exist_ok=True)\n",
    "    os.makedirs(sample_results_base_dir, exist_ok=True) \n",
    "\n",
    "    bs_data_3d = None\n",
    "    num_replicates_loaded = 0\n",
    "    expected_shape = None\n",
    "\n",
    "\n",
    "\n",
    "    bootstrap_files = [f for f in os.listdir(matrices_dir) if f.startswith('bstrap_weight_matrix_') and f.endswith('.csv')]\n",
    "    if not bootstrap_files:\n",
    "        print(f\"   ERROR: No bootstrap matrix files found in {matrices_dir}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    bootstrap_matrices = []\n",
    "    print(f\"   Loading bootstrap matrices from {matrices_dir}...\")\n",
    "    for f in tqdm(bootstrap_files, desc=\"    Loading matrices\", leave=False):\n",
    "        file_path = os.path.join(matrices_dir, f)\n",
    "       \n",
    "        m = np.loadtxt(file_path, delimiter=\",\", dtype=float)\n",
    "        if m.ndim == 0: \n",
    "            m = np.array([[m]])\n",
    "\n",
    "        if expected_shape is None and m.ndim == 2:\n",
    "            expected_shape = m.shape\n",
    "            if expected_shape[0] < 2 or expected_shape[1] < 2:\n",
    "                 print(f\"   Warning: First loaded matrix {f} has shape {expected_shape}. This might be incorrect. Resetting expected shape.\")\n",
    "                 expected_shape = None \n",
    "                 continue\n",
    "            print(f\"    Expected matrix shape: {expected_shape}\")\n",
    "\n",
    "        if expected_shape is not None:\n",
    "            if m.ndim == 2 and m.shape == expected_shape:\n",
    "                bootstrap_matrices.append(m)\n",
    "            elif m.ndim == 2:\n",
    "                print(f\"   Warning: Shape mismatch loading {f}. Expected {expected_shape}, got {m.shape}. Skipping.\")\n",
    "\n",
    "\n",
    "    if not bootstrap_matrices:\n",
    "         print(f\"   ERROR: Could not load ANY valid bootstrap matrices with consistent shape from {matrices_dir}. Check intermediate files and bootstrapping logs.\")\n",
    "         if os.path.exists(matrices_dir) and not os.listdir(matrices_dir): os.rmdir(matrices_dir)\n",
    "         if os.path.exists(bstrap_intermed_dir) and not os.listdir(bstrap_intermed_dir): os.rmdir(bstrap_intermed_dir)\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    bs_data_3d = np.stack(bootstrap_matrices, axis=0)\n",
    "    num_replicates_loaded = bs_data_3d.shape[0]\n",
    "    print(f\"   Loaded {num_replicates_loaded} bootstrap matrices into memory.\")\n",
    "\n",
    "    # Optional Deletion \n",
    "    if delete_bootstrap_matrices:\n",
    "        print(f\"     Attempting to delete individual bootstrap matrix files from {matrices_dir}...\")\n",
    "        deleted_count = 0\n",
    "\n",
    "        for f in bootstrap_files: \n",
    "            file_path = os.path.join(matrices_dir, f)\n",
    "           \n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "                deleted_count += 1\n",
    "\n",
    "        if os.path.exists(matrices_dir) and not os.listdir(matrices_dir):\n",
    "            os.rmdir(matrices_dir)\n",
    "\n",
    "    data_cond = df_cond.to_numpy()\n",
    "    num_samples, num_species = data_cond.shape\n",
    "\n",
    "\n",
    "    if bs_data_3d is None or bs_data_3d.shape[1:] != (num_species, num_species):\n",
    "        print(f\"   ERROR: Dimension mismatch or missing bootstrap data after stacking. Bootstrap shape {bs_data_3d.shape if bs_data_3d is not None else 'None'}, \"\n",
    "              f\"condition data has {num_species} species. Cannot proceed.\")\n",
    "        return None, None\n",
    "\n",
    "    species_names = df_cond.columns \n",
    "\n",
    "    bs_weight_distributions = defaultdict(list)\n",
    "    print(\"   Pre-calculating bootstrap distributions for edges...\")\n",
    "\n",
    "    for i in range(num_species):\n",
    "        for j in range(i + 1, num_species):\n",
    "         \n",
    "            weights_for_edge = bs_data_3d[:, i, j]\n",
    "            \n",
    "            finite_weights = weights_for_edge[np.isfinite(weights_for_edge)]\n",
    "            if len(finite_weights) > 0:\n",
    "                bs_weight_distributions[i, j] = finite_weights.tolist()\n",
    "\n",
    "    # Free up memory? Not sure if that's how this works \n",
    "    del bootstrap_matrices\n",
    "    del bs_data_3d\n",
    "    print(\"   Bootstrap distributions extracted, memory released (probably).\")\n",
    "\n",
    "\n",
    "    print(f\"   Processing {num_samples} samples for filtering...\")\n",
    "    filtering_summary_info = {}\n",
    "    filtered_otu_table_data = df_cond.copy()\n",
    "    samples_processed_count = 0\n",
    "\n",
    "    sample_iterator = tqdm(range(num_samples), desc=f\"     Filtering {condition_group}\")\n",
    "\n",
    "    for counter in sample_iterator:\n",
    "        sample_name = df_cond.index[counter]\n",
    "        sample_data_rel = data_cond[counter, :] \n",
    "\n",
    "        sample_output_dir = os.path.join(sample_results_base_dir, sample_name)\n",
    "        os.makedirs(sample_output_dir, exist_ok=True)\n",
    "\n",
    "        sample_weights_unfiltered, _ = compute_sample_weights(sample_data_rel)\n",
    "       \n",
    "        filtered_sample_weights = sample_weights_unfiltered.copy() # Start with unfiltered weights\n",
    "        total_edges_in_sample = 0 # Count edges initially present in the sample's graph\n",
    "        edges_removed_count = 0 # Count edges removed by p-value filtering\n",
    "\n",
    "        # Iterate through upper triangle of the weight matrix\n",
    "        for i in range(num_species):\n",
    "            for j in range(i + 1, num_species):\n",
    "                sample_w_ij = sample_weights_unfiltered[i, j] # The weight in this specific sample\n",
    "\n",
    "                # Only consider edges that exist (non-zero, finite) in this sample\n",
    "                if abs(sample_w_ij) > 1e-9 and np.isfinite(sample_w_ij):\n",
    "                    total_edges_in_sample += 1\n",
    "                    retain_edge = True # Assume we keep the edge initially\n",
    "\n",
    "                    # Get the bootstrap distribution for this edge (i, j)\n",
    "                    if (i, j) in bs_weight_distributions:\n",
    "                        bs_dist = bs_weight_distributions[i, j]\n",
    "                  \n",
    "                        # Need at least 2 points for variance, and non-zero variance\n",
    "                        if len(bs_dist) > 1 and np.std(bs_dist) > 1e-9:\n",
    "                            \n",
    "                            with np.errstate(invalid='ignore'): \n",
    "                                t_stat, p_val = stats.ttest_1samp(a=bs_dist, popmean=sample_w_ij,\n",
    "                                                                    alternative='two-sided', nan_policy='omit')\n",
    "\n",
    "                            # Decision: Remove edge if p-value is significant (low) and finite\n",
    "                            if p_val < pval_thresh and np.isfinite(p_val):\n",
    "                                retain_edge = False # Mark edge for removal\n",
    "\n",
    "                    # Apply filtering decision\n",
    "                    if not retain_edge:\n",
    "                        filtered_sample_weights[i, j] = filtered_sample_weights[j, i] = 0 # Remove edge\n",
    "                        edges_removed_count += 1\n",
    "\n",
    "\n",
    "        # 3. Save the filtered weights matrix for this sample\n",
    "        filtered_matrix_path = os.path.join(sample_output_dir, f\"weights_filtered.csv\")\n",
    "        np.savetxt(filtered_matrix_path, filtered_sample_weights, delimiter=\",\")\n",
    "\n",
    "\n",
    "        G_filtered = nx.Graph()\n",
    "        present_species_indices = np.where(sample_data_rel > 1e-9)[0]\n",
    "        nodes_added_to_graph = set()\n",
    "        for idx in present_species_indices:\n",
    "             if idx < len(species_names): \n",
    "                  node_name = species_names[idx]\n",
    "                  G_filtered.add_node(node_name, relab=sample_data_rel[idx])\n",
    "                  nodes_added_to_graph.add(node_name)\n",
    "\n",
    "        edges_added_count = 0\n",
    "        for i in range(num_species):\n",
    "            for j in range(i + 1, num_species):\n",
    "                if abs(filtered_sample_weights[i, j]) > 1e-9 and np.isfinite(filtered_sample_weights[i, j]):\n",
    "                     if i < len(species_names) and j < len(species_names):\n",
    "                          node_i = species_names[i]; node_j = species_names[j]\n",
    "                          if node_i in nodes_added_to_graph and node_j in nodes_added_to_graph:\n",
    "                               G_filtered.add_edge(node_i, node_j, weight=filtered_sample_weights[i, j])\n",
    "                               edges_added_count += 1\n",
    "\n",
    "        # Save filtered graphml\n",
    "        filtered_graph_path = os.path.join(sample_output_dir, f\"graph_filtered.graphml\") \n",
    "\n",
    "        # Optional: Remove isolated nodes before saving?\n",
    "        # isolated = list(nx.isolates(G_filtered))\n",
    "        # G_filtered.remove_nodes_from(isolated)\n",
    "        \n",
    "        nx.write_graphml(G_filtered, filtered_graph_path) \n",
    "\n",
    "        remaining_nodes = list(G_filtered.nodes())\n",
    "        cols_to_zero_out = df_cond.columns[~df_cond.columns.isin(remaining_nodes)]\n",
    "        if not cols_to_zero_out.empty:\n",
    "            filtered_otu_table_data.loc[sample_name, cols_to_zero_out] = 0\n",
    "\n",
    "\n",
    "        edges_kept_count = G_filtered.number_of_edges()\n",
    "        edges_filtered_out_actual = total_edges_in_sample - edges_kept_count\n",
    "\n",
    "        filtering_summary_info[sample_name] = {\n",
    "            \"nodes_in\": len(nodes_added_to_graph), # Nodes initially present with abundance > 0\n",
    "            \"edges_unfiltered\": total_edges_in_sample, # Edges calculated from original sample abundances\n",
    "            \"edges_removed_by_pval\": edges_filtered_out_actual, # Edges removed by p-value check\n",
    "            \"prop_removed\": (edges_filtered_out_actual / total_edges_in_sample if total_edges_in_sample > 0 else 0),\n",
    "            \"nodes_out\": G_filtered.number_of_nodes(), # Nodes remaining in the final graph\n",
    "            \"edges_out\": edges_kept_count # Edges remaining in the final graph\n",
    "        }\n",
    "        samples_processed_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"   Saving condition-level summaries for {condition_group}...\")\n",
    "    summary_df = pd.DataFrame.from_dict(filtering_summary_info, orient=\"index\")\n",
    "    summary_csv_path = os.path.join(condition_summary_dir, f\"filtering_summary_{condition_group}.csv\")\n",
    "\n",
    "    summary_df.to_csv(summary_csv_path)\n",
    "    print(f\"     Saved filtering summary: {summary_csv_path}\")\n",
    "\n",
    "    filtered_otu_table_path = os.path.join(condition_summary_dir, f\"feature_table_filtered_{condition_group}.csv\")\n",
    "    filtered_otu_table_data.to_csv(filtered_otu_table_path)\n",
    "    print(f\"     Filtered OTU table saved: {filtered_otu_table_path}\")\n",
    "\n",
    "    print(f\"  Finished filtering for condition {condition_group}. Samples processed: {samples_processed_count}/{num_samples}\")\n",
    "    return filtered_otu_table_data, summary_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f40c7-8259-41e3-8ed7-f197d6988eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_bootstrap_filtering_per_condition(feature_table, metadata, output_dir,\n",
    "                                            condition_id_col, num_bootstraps,\n",
    "                                            p_value_threshold, min_samples_bootstrap):\n",
    "    \"\"\"\n",
    "    Orchestrates bootstrapping and p-value filtering for each condition.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Bootstrap P-Value Filtering Workflow ---\")\n",
    "    \n",
    "    # Create base directory for this run's parameters\n",
    "    run_params_str = f\"P_{p_value_threshold}_N_{num_bootstraps}\"\n",
    "    bootstrap_base_dir = os.path.join(output_dir, f\"02_Bootstrap_Filtering_({run_params_str})\")\n",
    "    \n",
    "    os.makedirs(bootstrap_base_dir, exist_ok=True) # Base for this specific run\n",
    "    print(f\" Bootstrap filtering outputs will be saved under: '{bootstrap_base_dir}'\")\n",
    "\n",
    "    all_conditions = metadata[condition_id_col].unique()\n",
    "   \n",
    "\n",
    "    print(f\" Processing {len(all_conditions)} conditions based on: '{condition_id_col}'\")\n",
    "    print(f\" Parameters: Bootstraps={num_bootstraps}, P-Value Threshold={p_value_threshold}, Min Samples={min_samples_bootstrap}\")\n",
    "\n",
    "    all_filtered_otus = {}\n",
    "    all_summaries = {}\n",
    "    conditions_processed_count = 0\n",
    "    conditions_skipped_count = 0\n",
    "\n",
    "  \n",
    "    for condition_group in all_conditions:\n",
    "        condition_sample_ids = metadata[metadata[condition_id_col] == condition_group].index\n",
    "\n",
    "        valid_condition_sample_ids = condition_sample_ids.intersection(feature_table.index)\n",
    "\n",
    "        if len(valid_condition_sample_ids) < min_samples_bootstrap:\n",
    "            print(f\" Skipping condition '{condition_group}': Only {len(valid_condition_sample_ids)} valid samples found (min required: {min_samples_bootstrap}).\")\n",
    "            conditions_skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        df_cond = feature_table.loc[valid_condition_sample_ids]\n",
    "  \n",
    "        print(\"  Starting bootstrap population generation...\")\n",
    "        bstrap_intermed_dir = create_bootstrap_population(\n",
    "            observed_data_df=df_cond, \n",
    "            condition_group=condition_group, \n",
    "            output_dir_base=bootstrap_base_dir, \n",
    "            n_boots=num_bootstraps\n",
    "        )\n",
    "\n",
    "        if bstrap_intermed_dir is None:\n",
    "             print(f\"  ERROR: Bootstrapping failed for condition '{condition_group}'. Skipping filtering step.\")\n",
    "             conditions_skipped_count += 1\n",
    "             continue\n",
    "                \n",
    "        print(\"  Bootstrap population generation finished.\")\n",
    "\n",
    "        print(\"  Starting p-value filtering for samples...\")\n",
    "        filtered_otu_table, summary_df = filtering_pvals_for_each_sample(\n",
    "            df_cond=df_cond, \n",
    "            condition_group=condition_group, \n",
    "            bstrap_intermed_dir=bstrap_intermed_dir, # Path to intermediates (matrices/, means.csv, stds.csv)\n",
    "            output_dir_base=bootstrap_base_dir, # Pass the run-specific base dir\n",
    "            pval_thresh=p_value_threshold,\n",
    "            delete_bootstrap_matrices=True # Default set to True to clean up space\n",
    "        )\n",
    "        print(\"  P-value filtering for samples finished.\")\n",
    "\n",
    "        if filtered_otu_table is not None:\n",
    "             all_filtered_otus[condition_group] = filtered_otu_table \n",
    "        if summary_df is not None:\n",
    "             all_summaries[condition_group] = summary_df \n",
    "        conditions_processed_count += 1\n",
    "\n",
    "    # --- Workflow Finish ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Bootstrap P-Value Filtering Workflow Finished.\")\n",
    "    print(f\" Conditions processed: {conditions_processed_count}\")\n",
    "    print(f\" Conditions skipped (due to sample size or errors): {conditions_skipped_count}\")\n",
    "    print(\"=\"*50)\n",
    "    return all_filtered_otus, all_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9108fe-22ef-4baa-ae66-559f9cae5fef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration Settings ---\n",
      "OTU Table Path: ../data/rvc/OTU_table_full.csv\n",
      "Metadata Path: ../data/rvc/metadata.tsv\n",
      "Sample ID Column: 'Sample-ID'\n",
      "Condition ID Column: 'Group ID'\n",
      "Base Output Directory: 'outputs'\n",
      "\n",
      "Workflow Selection:\n",
      "  Run Feature Processing & Alignment: True\n",
      "  Run Bootstrap P-Value Filtering (p<0.05, N=10, min_samples=5): True\n",
      "------------------------------\n",
      "\n",
      "--- Starting Data Processing and Analysis Workflow ---\n",
      "Script started at: 2025-04-25 13:36:10\n",
      "\n",
      "Output directory confirmed: '/Users/nandini.gadhia/Documents/projects/ot_omics/notebooks/outputs'\n",
      "\n",
      "[Step 1] Processing input data...\n",
      " Reading OTU table at: ../data/rvc/OTU_table_full.csv\n",
      " Reading metadata at: ../data/rvc/metadata.tsv\n",
      "  Initial OTU table shape: (88, 740)\n",
      "  Initial Metadata shape: (88, 8)\n",
      "  Using Sample ID column: 'Sample-ID'\n",
      "  Using Condition ID column: 'Group ID'\n",
      "  Found 88 common samples between OTU table and metadata.\n",
      "  Cleaned feature table shape: (88, 740)\n",
      "  Cleaned metadata shape: (88, 7)\n",
      "  Normalized OTU table to relative abundances. Final shape: (88, 740)\n",
      "  Grouped table shape (for info): (5, 740)\n",
      "  Saved normalized feature table to: outputs/01_Processed_Data/feature_table_normalized.csv\n",
      "  Saved aligned metadata to: outputs/01_Processed_Data/metadata_aligned.csv\n",
      " Data processing function finished.\n",
      "[Step 1] Data processing finished.\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 2] Running Bootstrap P-Value Filtering (p < 0.05, N = 10)...\n",
      "\n",
      "--- Running Bootstrap P-Value Filtering Workflow ---\n",
      " Bootstrap filtering outputs will be saved under: 'outputs/02_Bootstrap_Filtering_(P_0.05_N_10)'\n",
      " Processing 5 conditions based on: 'Group ID'\n",
      " Parameters: Bootstraps=10, P-Value Threshold=0.05, Min Samples=5\n",
      "  Starting bootstrap population generation...\n",
      " Bootstrapping 10 replicates for condition: UV-C\n",
      "   Bootstrap intermediate matrices will be saved in: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C/matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating mean and std deviation across bootstrap matrices...\n",
      "   Saved mean and std matrices to: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C\n",
      "  Bootstrap population generation finished.\n",
      "  Starting p-value filtering for samples...\n",
      "  Filtering individual samples for condition: UV-C (p-value < 0.05 removes edge)\n",
      "   Loading bootstrap matrices from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C/matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading matrices:  60%|██████    | 6/10 [00:00<00:00, 27.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Expected matrix shape: (740, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 10 bootstrap matrices into memory.\n",
      "     Attempting to delete individual bootstrap matrix files from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C/matrices...\n",
      "   Pre-calculating bootstrap distributions for edges...\n",
      "   Bootstrap distributions extracted, memory released (probably).\n",
      "   Processing 20 samples for filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving condition-level summaries for UV-C...\n",
      "     Saved filtering summary: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/filtering_summary_UV-C.csv\n",
      "     Filtered OTU table saved: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/feature_table_filtered_UV-C.csv\n",
      "  Finished filtering for condition UV-C. Samples processed: 20/20\n",
      "  P-value filtering for samples finished.\n",
      "  Starting bootstrap population generation...\n",
      " Bootstrapping 10 replicates for condition: UV-UC\n",
      "   Bootstrap intermediate matrices will be saved in: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-UC/matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating mean and std deviation across bootstrap matrices...\n",
      "   Saved mean and std matrices to: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-UC\n",
      "  Bootstrap population generation finished.\n",
      "  Starting p-value filtering for samples...\n",
      "  Filtering individual samples for condition: UV-UC (p-value < 0.05 removes edge)\n",
      "   Loading bootstrap matrices from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-UC/matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading matrices:  60%|██████    | 6/10 [00:00<00:00, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Expected matrix shape: (740, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 10 bootstrap matrices into memory.\n",
      "     Attempting to delete individual bootstrap matrix files from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-UC/matrices...\n",
      "   Pre-calculating bootstrap distributions for edges...\n",
      "   Bootstrap distributions extracted, memory released (probably).\n",
      "   Processing 20 samples for filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving condition-level summaries for UV-UC...\n",
      "     Saved filtering summary: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/filtering_summary_UV-UC.csv\n",
      "     Filtered OTU table saved: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/feature_table_filtered_UV-UC.csv\n",
      "  Finished filtering for condition UV-UC. Samples processed: 20/20\n",
      "  P-value filtering for samples finished.\n",
      "  Starting bootstrap population generation...\n",
      " Bootstrapping 10 replicates for condition: MV-C\n",
      "   Bootstrap intermediate matrices will be saved in: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/MV-C/matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating mean and std deviation across bootstrap matrices...\n",
      "   Saved mean and std matrices to: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/MV-C\n",
      "  Bootstrap population generation finished.\n",
      "  Starting p-value filtering for samples...\n",
      "  Filtering individual samples for condition: MV-C (p-value < 0.05 removes edge)\n",
      "   Loading bootstrap matrices from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/MV-C/matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading matrices:  30%|███       | 3/10 [00:00<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Expected matrix shape: (740, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 10 bootstrap matrices into memory.\n",
      "     Attempting to delete individual bootstrap matrix files from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/MV-C/matrices...\n",
      "   Pre-calculating bootstrap distributions for edges...\n",
      "   Bootstrap distributions extracted, memory released (probably).\n",
      "   Processing 20 samples for filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving condition-level summaries for MV-C...\n",
      "     Saved filtering summary: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/filtering_summary_MV-C.csv\n",
      "     Filtered OTU table saved: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/feature_table_filtered_MV-C.csv\n",
      "  Finished filtering for condition MV-C. Samples processed: 20/20\n",
      "  P-value filtering for samples finished.\n",
      "  Starting bootstrap population generation...\n",
      " Bootstrapping 10 replicates for condition: V-C\n",
      "   Bootstrap intermediate matrices will be saved in: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/V-C/matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating mean and std deviation across bootstrap matrices...\n",
      "   Saved mean and std matrices to: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/V-C\n",
      "  Bootstrap population generation finished.\n",
      "  Starting p-value filtering for samples...\n",
      "  Filtering individual samples for condition: V-C (p-value < 0.05 removes edge)\n",
      "   Loading bootstrap matrices from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/V-C/matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading matrices:  30%|███       | 3/10 [00:00<00:00, 24.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Expected matrix shape: (740, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 10 bootstrap matrices into memory.\n",
      "     Attempting to delete individual bootstrap matrix files from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/V-C/matrices...\n",
      "   Pre-calculating bootstrap distributions for edges...\n",
      "   Bootstrap distributions extracted, memory released (probably).\n",
      "   Processing 20 samples for filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving condition-level summaries for V-C...\n",
      "     Saved filtering summary: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/filtering_summary_V-C.csv\n",
      "     Filtered OTU table saved: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/feature_table_filtered_V-C.csv\n",
      "  Finished filtering for condition V-C. Samples processed: 20/20\n",
      "  P-value filtering for samples finished.\n",
      "  Starting bootstrap population generation...\n",
      " Bootstrapping 10 replicates for condition: UV-C10\n",
      "   Bootstrap intermediate matrices will be saved in: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C10/matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calculating mean and std deviation across bootstrap matrices...\n",
      "   Saved mean and std matrices to: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C10\n",
      "  Bootstrap population generation finished.\n",
      "  Starting p-value filtering for samples...\n",
      "  Filtering individual samples for condition: UV-C10 (p-value < 0.05 removes edge)\n",
      "   Loading bootstrap matrices from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C10/matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Loading matrices:  60%|██████    | 6/10 [00:00<00:00, 28.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Expected matrix shape: (740, 740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 10 bootstrap matrices into memory.\n",
      "     Attempting to delete individual bootstrap matrix files from outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Intermediates/UV-C10/matrices...\n",
      "   Pre-calculating bootstrap distributions for edges...\n",
      "   Bootstrap distributions extracted, memory released (probably).\n",
      "   Processing 8 samples for filtering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving condition-level summaries for UV-C10...\n",
      "     Saved filtering summary: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/filtering_summary_UV-C10.csv\n",
      "     Filtered OTU table saved: outputs/02_Bootstrap_Filtering_(P_0.05_N_10)/Condition_Summaries/feature_table_filtered_UV-C10.csv\n",
      "  Finished filtering for condition UV-C10. Samples processed: 8/8\n",
      "  P-value filtering for samples finished.\n",
      "\n",
      "==================================================\n",
      "Bootstrap P-Value Filtering Workflow Finished.\n",
      " Conditions processed: 5\n",
      " Conditions skipped (due to sample size or errors): 0\n",
      "==================================================\n",
      "[Step 2] Bootstrap P-Value Filtering finished.\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "--- Workflow Finished ---\n",
      "Total execution time: 169.37 seconds\n",
      "Output saved in base directory: /Users/nandini.gadhia/Documents/projects/ot_omics/notebooks/outputs\n",
      "Check subdirectories for results from enabled analyses:\n",
      " - 01_Processed_Data/\n",
      " - 02_Bootstrap_Filtering_(P_0.05_N_10)/\n",
      "==================================================\n",
      "\n",
      "Script finished at: 2025-04-25 13:39:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"--- Configuration Settings ---\")\n",
    "\n",
    "    otu_file_path = '../data/rvc/OTU_table_full.csv'\n",
    "    metadata_file_path = '../data/rvc/metadata.tsv'\n",
    "    sample_id_col = 'Sample-ID'\n",
    "    condition_id_col = 'Group ID' \n",
    "\n",
    "    # Base output directory for all results\n",
    "    output_dir = \"outputs\"\n",
    "\n",
    "\n",
    "    run_feature_processing = True \n",
    "    run_bootstrap_pval_filtering = True \n",
    "\n",
    "   #Choose params\n",
    "    num_bootstraps_pval = 10\n",
    "    pval_threshold = 0.05\n",
    "    min_samples_pval = 5 # Minimum samples required per condition for bootstrapping\n",
    "\n",
    "    random_seed = 42 # This should mean its the same random numbers every time\n",
    "\n",
    "    # Print Configuration\n",
    "    print(f\"OTU Table Path: {otu_file_path}\")\n",
    "    print(f\"Metadata Path: {metadata_file_path}\")\n",
    "    print(f\"Sample ID Column: '{sample_id_col}'\")\n",
    "    print(f\"Condition ID Column: '{condition_id_col}'\")\n",
    "    print(f\"Base Output Directory: '{output_dir}'\")\n",
    "    print(f\"\\nWorkflow Selection:\")\n",
    "    print(f\"  Run Feature Processing & Alignment: {run_feature_processing}\")\n",
    "    print(f\"  Run Bootstrap P-Value Filtering (p<{pval_threshold}, N={num_bootstraps_pval}, min_samples={min_samples_pval}): {run_bootstrap_pval_filtering}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Workflwo\n",
    "    print(\"\\n--- Starting Data Processing and Analysis Workflow ---\")\n",
    "    print(f\"Script started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    feature_table = None\n",
    "    metadata = None\n",
    "\n",
    "    # Create base output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nOutput directory confirmed: '{os.path.abspath(output_dir)}'\")\n",
    "\n",
    "\n",
    "    if run_feature_processing:\n",
    "        print(\"\\n[Step 1] Processing input data...\")\n",
    "        feature_table, metadata = process_data(\n",
    "            otu_file_path, metadata_file_path, output_dir,\n",
    "            sample_id=sample_id_col,\n",
    "            condition_id=condition_id_col\n",
    "        )\n",
    "   \n",
    "        if feature_table is None or metadata is None or feature_table.empty or metadata.empty:\n",
    "            raise ValueError(\"Data processing failed to return valid feature table or metadata.\")\n",
    "        print(\"[Step 1] Data processing finished.\")\n",
    "        print(\"-\"*50)\n",
    "    else:\n",
    "        print(\"\\n[Step 1] Processing input data SKIPPED\")\n",
    "        print(\"   Attempting to load previously processed data...\")\n",
    "        ft_path = os.path.join(output_dir, \"01_Processed_Data\", \"feature_table_normalized.csv\")\n",
    "        meta_path = os.path.join(output_dir, \"01_Processed_Data\", \"metadata_aligned.csv\")\n",
    "        feature_table = pd.read_csv(ft_path, index_col=0)\n",
    "        metadata = pd.read_csv(meta_path, index_col=0)\n",
    "        # Ensure indices are strings after loading\n",
    "        feature_table.index = feature_table.index.astype(str)\n",
    "        metadata.index = metadata.index.astype(str)\n",
    "        print(f\"   Loaded feature table {feature_table.shape} and metadata {metadata.shape}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    # 2. Run Bootstrap P-Value Filtering \n",
    "    if run_bootstrap_pval_filtering:\n",
    "        print(f\"\\n[Step 2] Running Bootstrap P-Value Filtering (p < {pval_threshold}, N = {num_bootstraps_pval})...\")\n",
    "       \n",
    "        filtered_otu_tables_by_cond, filter_summaries_by_cond = run_bootstrap_filtering_per_condition(\n",
    "            feature_table=feature_table,\n",
    "            metadata=metadata,\n",
    "            output_dir=output_dir,\n",
    "            condition_id_col=condition_id_col,\n",
    "            num_bootstraps=num_bootstraps_pval,\n",
    "            p_value_threshold=pval_threshold,\n",
    "            min_samples_bootstrap=min_samples_pval\n",
    "        )\n",
    "        print(\"[Step 2] Bootstrap P-Value Filtering finished.\")\n",
    "        print(\"-\"*50)\n",
    "    else:\n",
    "        print(\"\\n[Step 2] Bootstrap P-Value Filtering Workflow SKIPPED\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Workflow Finished ---\")\n",
    "    print(f\"Total execution time: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "    print(f\"Output saved in base directory: {os.path.abspath(output_dir)}\")\n",
    "    print(\"Check subdirectories for results from enabled analyses:\")\n",
    "    if run_feature_processing: print(\" - 01_Processed_Data/\")\n",
    "    if run_bootstrap_pval_filtering:\n",
    "        run_params_str = f\"P_{pval_threshold}_N_{num_bootstraps_pval}\"\n",
    "        print(f\" - 02_Bootstrap_Filtering_({run_params_str})/\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"\\nScript finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

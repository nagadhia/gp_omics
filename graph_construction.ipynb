{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats.mstats import gmean\n",
    "from sklearn import covariance\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000542c8b5f1e84456c00f2a31ff2e5c</th>\n",
       "      <th>a534dd4959a8a823a499b837b709715b</th>\n",
       "      <th>4c9939a93db7b14a028245155492ac8e</th>\n",
       "      <th>aca4d218ca812c0421b3c8709895e90c</th>\n",
       "      <th>d294afbdfe8e32d772413bcae59257b7</th>\n",
       "      <th>77cfd84b41bc7a823d2fd3a07720f630</th>\n",
       "      <th>6639c219ee15084dad8de89c81d44743</th>\n",
       "      <th>c684492de5864e145aec85ba05996dce</th>\n",
       "      <th>a7ebbb1185a05884eb1f058a51a5a9f1</th>\n",
       "      <th>5aa81c4f07a78cffd5a3c235982cc88d</th>\n",
       "      <th>...</th>\n",
       "      <th>a0bded2b30070209e19bcdc9d47d2081</th>\n",
       "      <th>72d886315a5c7b9f02f8f19ea083d30b</th>\n",
       "      <th>3194a7a0b76e6688ad149980c3e556cc</th>\n",
       "      <th>f27cf7196efd3065029265a3550a31cc</th>\n",
       "      <th>ae978e65d6bf95e056983eec6feb4cef</th>\n",
       "      <th>bc7596a48a79d815b0ebe2a9421583b1</th>\n",
       "      <th>b6c24e652e819aca2a529eb68c689452</th>\n",
       "      <th>47892ef739987064f592b8525bb99c1d</th>\n",
       "      <th>f544fec8e6156c817bde06b9303ffbac</th>\n",
       "      <th>687a887797cbe32633fe89f3e7b45157</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1-10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1-2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1-3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1-4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000542c8b5f1e84456c00f2a31ff2e5c  a534dd4959a8a823a499b837b709715b  \\\n",
       "T1-1                                0.0                               0.0   \n",
       "T1-10                               0.0                               0.0   \n",
       "T1-2                                0.0                               0.0   \n",
       "T1-3                                0.0                               0.0   \n",
       "T1-4                                0.0                               0.0   \n",
       "\n",
       "       4c9939a93db7b14a028245155492ac8e  aca4d218ca812c0421b3c8709895e90c  \\\n",
       "T1-1                                0.0                          0.000000   \n",
       "T1-10                               0.0                          0.000000   \n",
       "T1-2                                0.0                          0.009635   \n",
       "T1-3                                0.0                          0.005251   \n",
       "T1-4                                0.0                          0.000000   \n",
       "\n",
       "       d294afbdfe8e32d772413bcae59257b7  77cfd84b41bc7a823d2fd3a07720f630  \\\n",
       "T1-1                                0.0                               0.0   \n",
       "T1-10                               0.0                               0.0   \n",
       "T1-2                                0.0                               0.0   \n",
       "T1-3                                0.0                               0.0   \n",
       "T1-4                                0.0                               0.0   \n",
       "\n",
       "       6639c219ee15084dad8de89c81d44743  c684492de5864e145aec85ba05996dce  \\\n",
       "T1-1                                0.0                               0.0   \n",
       "T1-10                               0.0                               0.0   \n",
       "T1-2                                0.0                               0.0   \n",
       "T1-3                                0.0                               0.0   \n",
       "T1-4                                0.0                               0.0   \n",
       "\n",
       "       a7ebbb1185a05884eb1f058a51a5a9f1  5aa81c4f07a78cffd5a3c235982cc88d  \\\n",
       "T1-1                                0.0                          0.000000   \n",
       "T1-10                               0.0                          0.000309   \n",
       "T1-2                                0.0                          0.000000   \n",
       "T1-3                                0.0                          0.000000   \n",
       "T1-4                                0.0                          0.000000   \n",
       "\n",
       "       ...  a0bded2b30070209e19bcdc9d47d2081  \\\n",
       "T1-1   ...                          0.015140   \n",
       "T1-10  ...                          0.014302   \n",
       "T1-2   ...                          0.003689   \n",
       "T1-3   ...                          0.046213   \n",
       "T1-4   ...                          0.000000   \n",
       "\n",
       "       72d886315a5c7b9f02f8f19ea083d30b  3194a7a0b76e6688ad149980c3e556cc  \\\n",
       "T1-1                                0.0                               0.0   \n",
       "T1-10                               0.0                               0.0   \n",
       "T1-2                                0.0                               0.0   \n",
       "T1-3                                0.0                               0.0   \n",
       "T1-4                                0.0                               0.0   \n",
       "\n",
       "       f27cf7196efd3065029265a3550a31cc  ae978e65d6bf95e056983eec6feb4cef  \\\n",
       "T1-1                                0.0                          0.000000   \n",
       "T1-10                               0.0                          0.001891   \n",
       "T1-2                                0.0                          0.000000   \n",
       "T1-3                                0.0                          0.005774   \n",
       "T1-4                                0.0                          0.000000   \n",
       "\n",
       "       bc7596a48a79d815b0ebe2a9421583b1  b6c24e652e819aca2a529eb68c689452  \\\n",
       "T1-1                           0.000000                               0.0   \n",
       "T1-10                          0.006267                               0.0   \n",
       "T1-2                           0.001032                               0.0   \n",
       "T1-3                           0.003280                               0.0   \n",
       "T1-4                           0.000000                               0.0   \n",
       "\n",
       "       47892ef739987064f592b8525bb99c1d  f544fec8e6156c817bde06b9303ffbac  \\\n",
       "T1-1                                0.0                               0.0   \n",
       "T1-10                               0.0                               0.0   \n",
       "T1-2                                0.0                               0.0   \n",
       "T1-3                                0.0                               0.0   \n",
       "T1-4                                0.0                               0.0   \n",
       "\n",
       "       687a887797cbe32633fe89f3e7b45157  \n",
       "T1-1                                0.0  \n",
       "T1-10                               0.0  \n",
       "T1-2                                0.0  \n",
       "T1-3                                0.0  \n",
       "T1-4                                0.0  \n",
       "\n",
       "[5 rows x 442 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data(otu_file_path, metadata_file_path, distance_matrix_file_path=None, \n",
    "                 sample_id='Sample', condition_id='Study.Group', phylo=False):\n",
    "    \n",
    "    \"\"\"Processes OTU table, metadata, and optionally a phylogenetic (or otherwise) distance matrix.\"\"\"\n",
    "    \n",
    "    sep_otu = '\\t' if otu_file_path.endswith('.tsv') else ','\n",
    "    sep_meta = '\\t' if metadata_file_path.endswith('.tsv') else ','\n",
    "    \n",
    "    otu_table = pd.read_csv(otu_file_path, index_col=0, sep=sep_otu)\n",
    "    metadata = pd.read_csv(metadata_file_path, sep=sep_meta)\n",
    "    \n",
    "    if sample_id not in metadata or condition_id not in metadata:\n",
    "        raise ValueError(\"Missing required columns in metadata.\")\n",
    "    \n",
    "    metadata.set_index(sample_id, inplace=True)\n",
    "    common_samples = otu_table.index.intersection(metadata.index)\n",
    "    \n",
    "    otu_table = otu_table.loc[common_samples]\n",
    "    metadata = metadata.loc[common_samples]\n",
    "    \n",
    "    otu_table = otu_table.loc[:, (otu_table != 0).any(axis=0)]  # Drop all-zero columns\n",
    "    otu_table = otu_table.div(otu_table.sum(axis=1), axis=0)  # Normalize to get relative abundances\n",
    "    \n",
    "    otu_table_grouped = otu_table.groupby(metadata[condition_id]).mean()\n",
    "    \n",
    "    phylo_distances = None\n",
    "    if phylo and distance_matrix_file_path:\n",
    "        phylo_distances = pd.read_csv(distance_matrix_file_path, index_col=0)\n",
    "        phylo_distances = phylo_distances.loc[otu_table.columns, otu_table.columns].to_numpy()\n",
    "    \n",
    "    metadata['condition'] = metadata.index.str[0] + '-' + metadata[condition_id].astype(str)\n",
    "    #metadata.index = metadata['condition']\n",
    "    #otu_table.index = metadata.index\n",
    "    \n",
    "    return otu_table, metadata, phylo_distances, metadata['condition'].to_numpy(), otu_table_grouped\n",
    "\n",
    "feature_table, metadata, phylo_distances, labels, otu_table_grouped = process_data(\n",
    "    \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\",\n",
    "    \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\", \n",
    "    sample_id='Sample-ID', condition_id='Group ID'\n",
    ")\n",
    "\n",
    "feature_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute sample weights for each sample\n",
    "def compute_sample_weights(sample_rel_otu):\n",
    "    binary_sample = [1 if num != 0 else 0 for num in sample_rel_otu]\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample)\n",
    "    sample_matrix = np.tile(sample_rel_otu, (len(sample_rel_otu), 1))\n",
    "    \n",
    "    original_array = np.diag(sample_matrix)\n",
    "    non_zero_mask = original_array != 0\n",
    "    inverted_non_zero_elements = 1 / original_array[non_zero_mask]\n",
    "    result_array = np.zeros_like(original_array, dtype=float)\n",
    "    result_array[non_zero_mask] = inverted_non_zero_elements\n",
    "    inv_diag = np.diag(result_array)\n",
    "    \n",
    "    ratios = np.matmul(inv_diag, sample_matrix)\n",
    "    weights = np.add(np.triu(ratios, k=1), np.tril(ratios, k=-1).T)\n",
    "    non_zero_mask = weights != 0\n",
    "    reciprocal_non_zero_elements = 2 / weights[non_zero_mask]\n",
    "    weights[non_zero_mask] = reciprocal_non_zero_elements\n",
    "    weights_new = np.add(np.triu(weights, k=1), np.triu(weights, k=-1).T)\n",
    "    \n",
    "    return weights_new, sample_binary_matrix\n",
    "\n",
    "# Compute weights for all samples\n",
    "def compute_all_weights(raw_data):\n",
    "    relative_raw = normalize(raw_data, axis=1, norm='l1')\n",
    "    combined_weights = np.zeros((len(raw_data[0]), len(raw_data[0])))\n",
    "    cooc_matrix = np.zeros((len(raw_data[0]), len(raw_data[0])))\n",
    "    \n",
    "    for sample in relative_raw:\n",
    "        w, cooc = compute_sample_weights(sample)\n",
    "        combined_weights += w\n",
    "        cooc_matrix += cooc\n",
    "\n",
    "    cooc_matrix_with_ones = np.where(cooc_matrix == 0, 1, cooc_matrix)\n",
    "    final_matrix = np.divide(combined_weights, cooc_matrix_with_ones)\n",
    "    \n",
    "    return final_matrix, cooc_matrix\n",
    "\n",
    "# Create bootstrap samples\n",
    "def create_bootstrap_population(observed_data, n=100, condition_group=\"Control\", output_dir=\"../outputs/bstrap_results\"):\n",
    "    if condition_group not in labels:\n",
    "        raise ValueError(\"Invalid condition group\")\n",
    "    \n",
    "    print(f\"Bootstrapping {n} samples for condition: {condition_group}\")\n",
    "    directory = f\"{output_dir}/bstrap_results_{condition_group}\"\n",
    "    os.makedirs(f\"{directory}/matrices\", exist_ok=True)\n",
    "    \n",
    "    raw_data = observed_data.to_numpy()\n",
    "    bstrap_otus = [raw_data[np.random.randint(0, len(raw_data), len(raw_data))] for _ in range(n)]\n",
    "    \n",
    "    def process_bootstrap_sample(b, otu_sample):\n",
    "        w, _ = compute_all_weights(otu_sample)\n",
    "        np.savetxt(f\"{directory}/matrices/bstrap_weight_matrix_{b}.csv\", w)\n",
    "        return w.flatten()\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(process_bootstrap_sample)(b, otu) for b, otu in enumerate(bstrap_otus))\n",
    "    \n",
    "    bstrap_means = np.mean(results, axis=0)\n",
    "    bstrap_stds = np.std(results, axis=0)\n",
    "    np.savetxt(f\"{directory}/_means_{condition_group}.csv\", bstrap_means)\n",
    "    np.savetxt(f\"{directory}/_stds_{condition_group}.csv\", bstrap_stds)\n",
    "\n",
    "\n",
    "def filtering_pvals_for_each_sample(df_cond, condition_group, thresh=0.1, output_dir=\"../outputs/bstrap_results\"):\n",
    "    directory = f\"{output_dir}/bstrap_results_{condition_group}/matrices/\"\n",
    "    unfiltered_matrices_dir = f\"{output_dir}/unfiltered_matrices/\"\n",
    "    os.makedirs(unfiltered_matrices_dir, exist_ok=True)\n",
    "    \n",
    "    filtered_matrices_dir = f\"{output_dir}/filtered_matrices/\"\n",
    "    os.makedirs(filtered_matrices_dir, exist_ok=True)\n",
    "    filtered_graphs_dir = f\"{output_dir}/filtered_graphs/\"\n",
    "    os.makedirs(filtered_graphs_dir, exist_ok=True)  \n",
    "\n",
    "    # Load bootstrap results\n",
    "    files = [np.loadtxt(directory + f) for f in os.listdir(directory)]\n",
    "    data_cond = df_cond.to_numpy()\n",
    "    \n",
    "    # Track species presence\n",
    "    species_occur = np.count_nonzero(data_cond, axis=0)\n",
    "    species_appear = np.where(species_occur > 0)[0]\n",
    "    species_appear_names = df_cond.columns[species_appear]\n",
    "    \n",
    "    # Dictionary to store bootstrap results for each edge\n",
    "    bs_cdf = defaultdict(list)\n",
    "    for dat in files:\n",
    "        for i, j in zip(*np.triu_indices(dat.shape[0], k=1)):\n",
    "            bs_cdf[i, j].append(dat[i, j])\n",
    "\n",
    "    for k in bs_cdf:\n",
    "        bs_cdf[k].sort()\n",
    "\n",
    "    # Normalize the data (relative abundance)\n",
    "    rel_ab = data_cond / data_cond.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Dictionary to store filtering summary per sample\n",
    "    filtering_summary_info = {}\n",
    "\n",
    "    # Create a copy of the OTU table to store filtered values\n",
    "    filtered_otu_table = df_cond.copy()\n",
    "\n",
    "    # Process each sample\n",
    "    for counter, da in enumerate(data_cond):\n",
    "        sample_name = df_cond.index[counter]\n",
    "        filtering_summary_info[sample_name] = {}  # Store info per sample\n",
    "        \n",
    "        nodes_in = np.count_nonzero(da)\n",
    "        filtering_summary_info[sample_name][\"nodes_in\"] = nodes_in\n",
    "\n",
    "        sample_weights = compute_sample_weights(da)[0]\n",
    "        unfiltered_matrix_path = f\"{unfiltered_matrices_dir}weights_{sample_name}.csv\"\n",
    "        np.savetxt(unfiltered_matrix_path, sample_weights, delimiter=\",\")\n",
    "\n",
    "        total_edges = 0\n",
    "        filtered_edges = 0  \n",
    "\n",
    "        for i, j in zip(*np.triu_indices(sample_weights.shape[0], k=1)):\n",
    "            if sample_weights[i, j] > 0:\n",
    "                total_edges += 1\n",
    "                \n",
    "                if (i, j) in bs_cdf and len(bs_cdf[i, j]) > 1:\n",
    "                    bs = bs_cdf[i, j]\n",
    "                    mean_w, std_w = np.mean(bs), np.std(bs)\n",
    "                    \n",
    "                    if std_w > 0:\n",
    "                        _, p_val = stats.ttest_1samp(bs, popmean=sample_weights[i, j], alternative='two-sided')\n",
    "                    else:\n",
    "                        p_val = 1.0  \n",
    "\n",
    "                    if p_val < thresh:\n",
    "                        sample_weights[i, j] = sample_weights[j, i] = 0\n",
    "                        filtered_edges += 1\n",
    "\n",
    "        filtering_summary_info[sample_name][\"edges_in\"] = total_edges\n",
    "        filtering_summary_info[sample_name][\"edges_filtered\"] = filtered_edges\n",
    "        filtering_summary_info[sample_name][\"prop_filtered\"] = (\n",
    "            filtered_edges / total_edges if total_edges > 0 else 0\n",
    "        )\n",
    "\n",
    "        filtered_matrix_path = f\"{filtered_matrices_dir}/filtered_weights_{sample_name}.csv\"\n",
    "        np.savetxt(filtered_matrix_path, sample_weights, delimiter=\",\")\n",
    "        print(f\" Saved filtered adjacency matrix: {filtered_matrix_path}\")\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for i in range(sample_weights.shape[0]):\n",
    "            for j in range(i + 1, sample_weights.shape[0]):\n",
    "                if sample_weights[i, j] > 0:\n",
    "                    G.add_edge(df_cond.columns[i], df_cond.columns[j], weight=sample_weights[i, j])\n",
    "        \n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "        nodes_out = len(G.nodes)\n",
    "        filtering_summary_info[sample_name][\"nodes_out\"] = nodes_out\n",
    "\n",
    "        filtered_graph_path = f\"{filtered_graphs_dir}/filtered_graph_sample_{counter + 1}.gml\"\n",
    "        print(f\"âœ… Saved filtered graph: {filtered_graph_path}\")\n",
    "\n",
    "        print(\n",
    "            f\"Sample {counter + 1}: Nodes In = {nodes_in}, \"\n",
    "            f\"Edges In = {total_edges}, Filtered Edges = {filtered_edges}, \"\n",
    "            f\"Nodes Out = {nodes_out}, Proportion Filtered = {filtering_summary_info[sample_name]['prop_filtered']:.4f}\"\n",
    "        )\n",
    "\n",
    "        remaining_nodes = list(G.nodes())\n",
    "        filtered_otu_table.loc[sample_name, ~df_cond.columns.isin(remaining_nodes)] = 0\n",
    "\n",
    "    # Save the filtered OTU table\n",
    "    filtered_otu_table_path = f\"{output_dir}/filtered_otu_table_{condition_group}.csv\"\n",
    "    filtered_otu_table.to_csv(filtered_otu_table_path)\n",
    "    print(f\"ðŸ“„ Filtered OTU table saved at: {filtered_otu_table_path}\")\n",
    "\n",
    "    # Summary DataFrame to track filtering results\n",
    "    summary_df = pd.DataFrame.from_dict(filtering_summary_info, orient=\"index\")\n",
    "    summary_csv_path = f\"{output_dir}/filtering_summary_{condition_group}.csv\"\n",
    "    summary_df.to_csv(summary_csv_path)\n",
    "    print(f\" ðŸ“„ Saved filtering summary CSV: {summary_csv_path}\")\n",
    "\n",
    "    return filtered_otu_table, filtered_otu_table_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\"\n",
    "metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\"\n",
    "\n",
    "\n",
    "feature_table, metadata, phylo_distances, labels, otu_table_grouped = process_data(\n",
    "    otu_file_path, metadata_file_path, sample_id='Sample-ID', condition_id='Group ID'\n",
    ")\n",
    "output_dir = \"/Users/nandini.gadhia/Documents/projects/ot_omics/outputs/bstrap_results_rvc\"\n",
    "\n",
    "condition_groups = metadata[\"Group ID\"].unique()\n",
    "\n",
    "graphs_list = []\n",
    "\n",
    "for condition_group in labels:  \n",
    "    \n",
    "    print(f\"\\nProcessing condition group: {condition_group}\")\n",
    "    \n",
    "    samples_in_condition = list(metadata[metadata['condition'] == condition_group].index)\n",
    "    condition_feature_table = feature_table.loc[samples_in_condition]\n",
    "\n",
    "    if condition_feature_table.empty:\n",
    "        print(f\"No samples found for condition group: {condition_group}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Run bootstrapping for the current condition group\n",
    "    create_bootstrap_population(condition_feature_table, n=100, condition_group=condition_group, output_dir=output_dir)\n",
    "\n",
    "    # Perform filtering for each sample individually based on p-values\n",
    "    graphs = filtering_pvals_for_each_sample(condition_feature_table, condition_group, thresh=0.0005, output_dir=output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_weights(sample_rel_otu):\n",
    "    \"\"\"Compute weights between species based on relative abundance.\"\"\"\n",
    "    binary_sample = [1 if num != 0 else 0 for num in sample_rel_otu]\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample)\n",
    "    sample_matrix = np.tile(sample_rel_otu, (len(sample_rel_otu), 1))\n",
    "    \n",
    "    original_array = np.diag(sample_matrix)\n",
    "    non_zero_mask = original_array != 0\n",
    "    inverted_non_zero_elements = 1 / original_array[non_zero_mask]\n",
    "    result_array = np.zeros_like(original_array, dtype=float)\n",
    "    result_array[non_zero_mask] = inverted_non_zero_elements\n",
    "    inv_diag = np.diag(result_array)\n",
    "    \n",
    "    ratios = np.matmul(inv_diag, sample_matrix)\n",
    "    weights = np.add(np.triu(ratios, k=1), np.tril(ratios, k=-1).T)\n",
    "    non_zero_mask = weights != 0\n",
    "    reciprocal_non_zero_elements = 2 / weights[non_zero_mask]\n",
    "    weights[non_zero_mask] = reciprocal_non_zero_elements\n",
    "    weights_new = np.add(np.triu(weights, k=1), np.triu(weights, k=-1).T)\n",
    "    \n",
    "    return weights_new, sample_binary_matrix\n",
    "\n",
    "def aggregate_samples_for_condition(feature_table, metadata, condition_group):\n",
    "    \"\"\"Aggregate samples by averaging for each condition.\"\"\"\n",
    "    samples_in_condition = metadata[metadata['Group ID'] == condition_group].index\n",
    "    condition_feature_table = feature_table.loc[samples_in_condition]\n",
    "    aggregated_feature_table = condition_feature_table.mean(axis=0)\n",
    "    return aggregated_feature_table, samples_in_condition\n",
    "\n",
    "def create_graph_from_aggregated_data(aggregated_data, threshold=0.1):\n",
    "    \"\"\"Create a graph where nodes are species with nonzero relative abundance and edges represent weights.\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Filter species with nonzero relative abundance\n",
    "    nonzero_species = aggregated_data[aggregated_data > 0]\n",
    "\n",
    "    # Add nodes only for species with nonzero relative abundance, storing relab as a node attribute\n",
    "    for species in nonzero_species.index:\n",
    "        G.add_node(species, relab=nonzero_species[species])  # Store relative abundance as node attribute\n",
    "\n",
    "    # Calculate weights between species based on their interactions\n",
    "    for i in range(len(nonzero_species)):\n",
    "        for j in range(i + 1, len(nonzero_species)):\n",
    "            species_i = nonzero_species.index[i]\n",
    "            species_j = nonzero_species.index[j]\n",
    "            \n",
    "            # Compute the weights between species (using the sample_weights function)\n",
    "            sample_i = nonzero_species[species_i]\n",
    "            sample_j = nonzero_species[species_j]\n",
    "            sample_weights, _ = compute_sample_weights(np.array([sample_i, sample_j]))\n",
    "            \n",
    "            weight = sample_weights[0, 1]  # Get the weight between species i and species j\n",
    "            \n",
    "            if weight > threshold:\n",
    "                G.add_edge(species_i, species_j, weight=weight)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def filter_feature_table(feature_table, graph_nodes):\n",
    "    \"\"\"Filter the feature table to retain only species (nodes) present in the final graph.\"\"\"\n",
    "    return feature_table[graph_nodes]\n",
    "\n",
    "def create_condition_graphs(feature_table, metadata, output_dir):\n",
    "    \"\"\"Create and save graphs representing each condition dynamically from metadata.\"\"\"\n",
    "    condition_groups = metadata[\"Group ID\"].unique()\n",
    "\n",
    "    # Create directories to save graphs and filtered tables for each condition\n",
    "    for condition_group in condition_groups:\n",
    "        os.makedirs(f\"{output_dir}/{condition_group}\", exist_ok=True)\n",
    "\n",
    "    # Iterate over all condition groups\n",
    "    for condition_group in condition_groups:\n",
    "        print(f\"\\nProcessing condition group: {condition_group}\")\n",
    "        \n",
    "        # Aggregate the data for the condition group\n",
    "        aggregated_data, samples_in_condition = aggregate_samples_for_condition(feature_table, metadata, condition_group)\n",
    "        \n",
    "        # Create a graph from the aggregated data\n",
    "        G = create_graph_from_aggregated_data(aggregated_data)\n",
    "        \n",
    "        # Get the nodes retained in the final graph\n",
    "        graph_nodes = list(G.nodes)\n",
    "\n",
    "        # Filter the feature table to retain only these nodes\n",
    "        feature_table_filtered_condition = filter_feature_table(feature_table.loc[samples_in_condition], graph_nodes)\n",
    "\n",
    "        # Save the filtered feature table for the condition\n",
    "        filtered_table_path = f\"{output_dir}/{condition_group}/feature_table_filtered_{condition_group}.csv\"\n",
    "        feature_table_filtered_condition.to_csv(filtered_table_path)\n",
    "        print(f\" Saved filtered feature table for {condition_group}: {filtered_table_path}\")\n",
    "\n",
    "        # Save the graph visualization as a PNG image\n",
    "        # visualize_graph(G, f\"{output_dir}/{condition_group}/graph_{condition_group}.png\")\n",
    "        \n",
    "        # Save the graph in XGML format for later analysis\n",
    "        nx.write_graphml(G, f\"{output_dir}/{condition_group}/graph_{condition_group}.xgml\")\n",
    "\n",
    "        print(f\" Graph for condition '{condition_group}' has been saved.\")\n",
    "\n",
    "\n",
    "otu_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\"\n",
    "metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\"\n",
    "\n",
    "\n",
    "# Process data to get the feature table and metadata\n",
    "feature_table, metadata, phylo_distances, labels, otu_table_grouped = process_data(\n",
    "    otu_file_path, metadata_file_path, sample_id='Sample-ID', condition_id='Group ID'\n",
    ")\n",
    "output_dir = \".\"\n",
    "# Create graphs for each condition group (dynamic handling of conditions)\n",
    "create_condition_graphs(feature_table, metadata, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(otu_file_path, metadata_file_path, distance_matrix_file_path=None, \n",
    "                 sample_id='Sample', condition_id='Study.Group', phylo=False):\n",
    "    \n",
    "    \"\"\"Processes OTU table, metadata, and optionally a phylogenetic (or otherwise) distance matrix.\"\"\"\n",
    "    \n",
    "    sep_otu = '\\t' if otu_file_path.endswith('.tsv') else ','\n",
    "    sep_meta = '\\t' if metadata_file_path.endswith('.tsv') else ','\n",
    "    \n",
    "    otu_table = pd.read_csv(otu_file_path, index_col=0, sep=sep_otu)\n",
    "    metadata = pd.read_csv(metadata_file_path, sep=sep_meta)\n",
    "    \n",
    "    if sample_id not in metadata or condition_id not in metadata:\n",
    "        raise ValueError(\"Missing required columns in metadata.\")\n",
    "    \n",
    "    metadata.set_index(sample_id, inplace=True)\n",
    "    common_samples = otu_table.index.intersection(metadata.index)\n",
    "    \n",
    "    otu_table = otu_table.loc[common_samples]\n",
    "    metadata = metadata.loc[common_samples]\n",
    "    \n",
    "    otu_table = otu_table.loc[:, (otu_table != 0).any(axis=0)]  # Drop all-zero columns\n",
    "    otu_table = otu_table.div(otu_table.sum(axis=1), axis=0)  # Normalize to get relative abundances\n",
    "    \n",
    "    otu_table_grouped = otu_table.groupby(metadata[condition_id]).mean()\n",
    "    \n",
    "    phylo_distances = None\n",
    "    if phylo and distance_matrix_file_path:\n",
    "        phylo_distances = pd.read_csv(distance_matrix_file_path, index_col=0)\n",
    "        phylo_distances = phylo_distances.loc[otu_table.columns, otu_table.columns].to_numpy()\n",
    "    \n",
    "    metadata['condition'] = metadata.index.str[0] + '-' + metadata[condition_id].astype(str)\n",
    "    #metadata.index = metadata['condition']\n",
    "    #otu_table.index = metadata.index\n",
    "    \n",
    "    return otu_table, metadata, phylo_distances, metadata['condition'].to_numpy(), otu_table_grouped\n",
    "def compute_sample_weights(sample_rel_otu):\n",
    "    \"\"\"Compute weights between species based on relative abundance.\"\"\"\n",
    "    binary_sample = [1 if num != 0 else 0 for num in sample_rel_otu]\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample)\n",
    "    sample_matrix = np.tile(sample_rel_otu, (len(sample_rel_otu), 1))\n",
    "    \n",
    "    original_array = np.diag(sample_matrix)\n",
    "    non_zero_mask = original_array != 0\n",
    "    inverted_non_zero_elements = 1 / original_array[non_zero_mask]\n",
    "    result_array = np.zeros_like(original_array, dtype=float)\n",
    "    result_array[non_zero_mask] = inverted_non_zero_elements\n",
    "    inv_diag = np.diag(result_array)\n",
    "    \n",
    "    ratios = np.matmul(inv_diag, sample_matrix)\n",
    "    weights = np.add(np.triu(ratios, k=1), np.tril(ratios, k=-1).T)\n",
    "    non_zero_mask = weights != 0\n",
    "    reciprocal_non_zero_elements = 2 / weights[non_zero_mask]\n",
    "    weights[non_zero_mask] = reciprocal_non_zero_elements\n",
    "    weights_new = np.add(np.triu(weights, k=1), np.triu(weights, k=-1).T)\n",
    "    \n",
    "    return weights_new, sample_binary_matrix\n",
    "\n",
    "def aggregate_samples_for_condition(feature_table, metadata, condition_group):\n",
    "    \"\"\"Aggregate samples by averaging for each condition.\"\"\"\n",
    "    samples_in_condition = metadata[metadata['Group ID'] == condition_group].index\n",
    "    condition_feature_table = feature_table.loc[samples_in_condition]\n",
    "    aggregated_feature_table = condition_feature_table.mean(axis=0)\n",
    "    return aggregated_feature_table, samples_in_condition\n",
    "\n",
    "def create_graph_from_aggregated_data(aggregated_data, threshold=0.1):\n",
    "    \"\"\"Create a graph where nodes are species with nonzero relative abundance and edges represent weights.\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Filter species with nonzero relative abundance\n",
    "    nonzero_species = aggregated_data[aggregated_data > 0]\n",
    "\n",
    "    # Add nodes only for species with nonzero relative abundance, storing relab as a node attribute\n",
    "    for species in nonzero_species.index:\n",
    "        G.add_node(species, relab=nonzero_species[species])  # Store relative abundance as node attribute\n",
    "\n",
    "    # Calculate weights between species based on their interactions\n",
    "    for i in range(len(nonzero_species)):\n",
    "        for j in range(i + 1, len(nonzero_species)):\n",
    "            species_i = nonzero_species.index[i]\n",
    "            species_j = nonzero_species.index[j]\n",
    "            \n",
    "            # Compute the weights between species (using the sample_weights function)\n",
    "            sample_i = nonzero_species[species_i]\n",
    "            sample_j = nonzero_species[species_j]\n",
    "            sample_weights, _ = compute_sample_weights(np.array([sample_i, sample_j]))\n",
    "            \n",
    "            weight = sample_weights[0, 1]  # Get the weight between species i and species j\n",
    "            \n",
    "            if weight > threshold:\n",
    "                G.add_edge(species_i, species_j, weight=weight)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def filter_feature_table(feature_table, graph_nodes):\n",
    "    \"\"\"Filter the feature table to retain only species (nodes) present in the final graph.\"\"\"\n",
    "    return feature_table[graph_nodes]\n",
    "\n",
    "def create_condition_graphs(feature_table, metadata, output_dir):\n",
    "    \"\"\"Create and save graphs representing each condition dynamically from metadata.\"\"\"\n",
    "    condition_groups = metadata[\"Group ID\"].unique()\n",
    "\n",
    "    # Create directories to save graphs and filtered tables for each condition\n",
    "    for condition_group in condition_groups:\n",
    "        os.makedirs(f\"{output_dir}/{condition_group}\", exist_ok=True)\n",
    "\n",
    "    # Iterate over all condition groups\n",
    "    for condition_group in condition_groups:\n",
    "        print(f\"\\nProcessing condition group: {condition_group}\")\n",
    "        \n",
    "        # Aggregate the data for the condition group\n",
    "        aggregated_data, samples_in_condition = aggregate_samples_for_condition(feature_table, metadata, condition_group)\n",
    "        \n",
    "        # Create a graph from the aggregated data\n",
    "        G = create_graph_from_aggregated_data(aggregated_data)\n",
    "        \n",
    "        # Get the nodes retained in the final graph\n",
    "        graph_nodes = list(G.nodes)\n",
    "\n",
    "        # Filter the feature table to retain only these nodes\n",
    "        feature_table_filtered_condition = filter_feature_table(feature_table.loc[samples_in_condition], graph_nodes)\n",
    "\n",
    "        # Save the filtered feature table for the condition\n",
    "        filtered_table_path = f\"{output_dir}/{condition_group}/feature_table_filtered_{condition_group}.csv\"\n",
    "        feature_table_filtered_condition.to_csv(filtered_table_path)\n",
    "        print(f\" Saved filtered feature table for {condition_group}: {filtered_table_path}\")\n",
    "\n",
    "        # Save the graph visualization as a PNG image\n",
    "        # visualize_graph(G, f\"{output_dir}/{condition_group}/graph_{condition_group}.png\")\n",
    "        \n",
    "        # Save the graph in XGML format for later analysis\n",
    "        nx.write_graphml(G, f\"{output_dir}/{condition_group}/graph_{condition_group}.xgml\")\n",
    "\n",
    "        print(f\" Graph for condition '{condition_group}' has been saved.\")\n",
    "\n",
    "\n",
    "#otu_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\"\n",
    "#metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\"\n",
    "\n",
    "otu_file_path = \"/Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_otu_table.csv\"\n",
    "metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_metadata.tsv\"\n",
    "# Process data to get the feature table and metadata\n",
    "feature_table, metadata, phylo_distances, labels, otu_table_grouped = process_data(\n",
    "    otu_file_path, metadata_file_path, sample_id='Sample-ID', condition_id='Group ID'\n",
    ")\n",
    "output_dir = \".\"\n",
    "# Create graphs for each condition group (dynamic handling of conditions)\n",
    "create_condition_graphs(feature_table, metadata, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "--- Microbiome Network Analysis Script ---\n",
      "Script started at: 2025-04-19 19:01:15\n",
      "==================================================\n",
      "Output directory: '/Users/nandini.gadhia/Documents/projects/gp_omics/graph_outputs_combined_final'\n",
      "Using Sample ID column: 'Sample-ID'\n",
      "Using Condition ID column: 'Group ID'\n",
      "Edge threshold for filtered graphs: 0.1\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 1/3] Processing input data...\n",
      " Reading OTU table: /Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\n",
      " Reading metadata: /Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\n",
      "  OTU table shape: (38, 740)\n",
      "  Metadata shape before processing: (88, 8)\n",
      "  Metadata columns: ['Sample-ID', 'Forward-read', 'Reverse-read', 'Source', 'Group ID', 'Vaccination', 'DPI', 'Challenged']\n",
      "  Using Sample ID column: 'Sample-ID'\n",
      "  Using Condition ID column: 'Group ID'\n",
      "  OTU table samples: 38, OTUs: 740\n",
      "  Metadata samples: 88\n",
      "  Found 38 common samples between OTU table and metadata.\n",
      "  Aligned feature table shape: (38, 740)\n",
      "  Aligned metadata shape: (38, 7)\n",
      "  Removed 298 OTUs that were all zero.\n",
      "  Normalized OTU table to relative abundances. Final shape: (38, 442)\n",
      " Data processing function finished.\n",
      "[Step 1/3] Data processed successfully (0.01s).\n",
      "  Final feature table dimensions: (38, 442)\n",
      "  Final metadata dimensions: (38, 7)\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 2/3] Creating Individual Sample Graphs...\n",
      "\n",
      "--- Starting INDIVIDUAL Sample Graph Generation (38 samples) ---\n",
      " Individual graphs will be saved in: 'graph_outputs_combined_final/individual_graphs'\n",
      " Processing sample 1/38: T1-1 (0.00s elapsed)\n",
      " Processing sample 38/38: T5-8 (0.71s elapsed)\n",
      "--- Finished INDIVIDUAL Sample Graph Generation ---\n",
      "  Total samples processed: 38\n",
      "  Errors encountered: 0\n",
      "  Total time: 0.72 seconds\n",
      "[Step 2/3] Individual sample graph generation finished.\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 3/3] Creating Condition-Level Graphs...\n",
      "\n",
      "--- Starting CONDITION-LEVEL Graph Generation (4 conditions) ---\n",
      " Conditions found in 'Group ID': ['UV-C', 'UV-UC', 'V-C', 'UV-C10']\n",
      " Condition graphs will be saved in: 'graph_outputs_combined_final/condition_graphs'\n",
      "--- Finished CONDITION-LEVEL Graph Generation (Processed: 4, Skipped: 0) ---\n",
      "[Step 3/3] Condition-level graph generation finished (0.70s).\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "--- All processing finished successfully ---\n",
      "Total execution time: 1.43 seconds\n",
      "Output saved in directory: /Users/nandini.gadhia/Documents/projects/gp_omics/graph_outputs_combined_final\n",
      "==================================================\n",
      "\n",
      "Script finished at: 2025-04-19 19:01:17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "import time # Added for basic progress timing\n",
    "import traceback # For detailed error printing\n",
    "\n",
    "# --- Function to process OTU and Metadata ---\n",
    "def process_data(otu_file_path, metadata_file_path, distance_matrix_file_path=None,\n",
    "                 sample_id='Sample', condition_id='Study.Group', phylo=False):\n",
    "\n",
    "    \"\"\"Processes OTU table, metadata, and optionally a phylogenetic (or otherwise) distance matrix.\"\"\"\n",
    "\n",
    "    print(f\" Reading OTU table: {otu_file_path}\")\n",
    "    print(f\" Reading metadata: {metadata_file_path}\")\n",
    "\n",
    "    # Determine separators\n",
    "    sep_otu = '\\t' if otu_file_path.lower().endswith('.tsv') else ','\n",
    "    sep_meta = '\\t' if metadata_file_path.lower().endswith('.tsv') else ','\n",
    "\n",
    "    # Read files\n",
    "    try:\n",
    "        otu_table = pd.read_csv(otu_file_path, index_col=0, sep=sep_otu)\n",
    "        print(f\"  OTU table shape: {otu_table.shape}\")\n",
    "        metadata = pd.read_csv(metadata_file_path, sep=sep_meta)\n",
    "        print(f\"  Metadata shape before processing: {metadata.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error reading input files: {e}. Please check paths.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing input files: {e}\")\n",
    "\n",
    "    # --- Metadata Column Checks ---\n",
    "    original_metadata_cols = metadata.columns.tolist()\n",
    "    print(f\"  Metadata columns: {original_metadata_cols}\")\n",
    "\n",
    "    # Check for Sample ID column\n",
    "    if sample_id not in metadata.columns:\n",
    "        print(f\"  Warning: Sample ID column '{sample_id}' not found directly. Checking if it's the index...\")\n",
    "        metadata_reset = metadata.reset_index()\n",
    "        # If 'index' column now exists and is the sample ID, rename it\n",
    "        if 'index' in metadata_reset.columns and sample_id == 'index':\n",
    "             print(f\"  Found sample ID '{sample_id}' as index.\")\n",
    "             metadata = metadata_reset\n",
    "        elif sample_id in metadata_reset.columns:\n",
    "            print(f\"  Found sample ID '{sample_id}' after resetting index.\")\n",
    "            metadata = metadata_reset\n",
    "        else:\n",
    "             original_metadata_cols_str = \", \".join(original_metadata_cols)\n",
    "             metadata_reset_cols_str = \", \".join(metadata_reset.columns.tolist())\n",
    "             raise ValueError(f\"Sample ID column '{sample_id}' not found in metadata. Original columns: [{original_metadata_cols_str}]. Columns after reset: [{metadata_reset_cols_str}]\")\n",
    "\n",
    "    # Check for Condition ID column\n",
    "    if condition_id not in metadata.columns:\n",
    "        metadata_cols_str = \", \".join(metadata.columns.tolist())\n",
    "        raise ValueError(f\"Condition ID column '{condition_id}' not found in metadata. Available columns: [{metadata_cols_str}]\")\n",
    "\n",
    "    # --- Sample ID Handling ---\n",
    "    print(f\"  Using Sample ID column: '{sample_id}'\")\n",
    "    print(f\"  Using Condition ID column: '{condition_id}'\")\n",
    "\n",
    "    # Check uniqueness and set index\n",
    "    if not metadata[sample_id].is_unique:\n",
    "        num_duplicates = metadata[sample_id].duplicated().sum()\n",
    "        print(f\"  Warning: Sample ID column '{sample_id}' contains {num_duplicates} duplicate values. Keeping first occurrence of each.\")\n",
    "        metadata = metadata.drop_duplicates(subset=[sample_id], keep='first')\n",
    "        print(f\"  Metadata shape after dropping duplicates: {metadata.shape}\")\n",
    "\n",
    "    try:\n",
    "        metadata.set_index(sample_id, inplace=True)\n",
    "    except KeyError:\n",
    "         # This should theoretically not happen due to checks above, but as a safeguard:\n",
    "         raise ValueError(f\"Failed to set index using Sample ID column '{sample_id}'. Column might be missing despite checks.\")\n",
    "\n",
    "    # --- Align OTU Table and Metadata ---\n",
    "    # Ensure indices are strings for reliable comparison\n",
    "    otu_table.index = otu_table.index.astype(str)\n",
    "    metadata.index = metadata.index.astype(str)\n",
    "    print(f\"  OTU table samples: {otu_table.shape[0]}, OTUs: {otu_table.shape[1]}\")\n",
    "    print(f\"  Metadata samples: {metadata.shape[0]}\")\n",
    "\n",
    "    common_samples = otu_table.index.intersection(metadata.index)\n",
    "    print(f\"  Found {len(common_samples)} common samples between OTU table and metadata.\")\n",
    "\n",
    "    if len(common_samples) == 0:\n",
    "        # Provide more detailed debug info if possible\n",
    "        otu_samples_head = otu_table.index[:5].tolist()\n",
    "        meta_samples_head = metadata.index[:5].tolist()\n",
    "        raise ValueError(f\"No common samples found. Check Sample ID matching and formatting.\\n\"\n",
    "                         f\"  First 5 OTU index values: {otu_samples_head}\\n\"\n",
    "                         f\"  First 5 Metadata index values: {meta_samples_head}\")\n",
    "\n",
    "    otu_table = otu_table.loc[common_samples]\n",
    "    metadata = metadata.loc[common_samples]\n",
    "    print(f\"  Aligned feature table shape: {otu_table.shape}\")\n",
    "    print(f\"  Aligned metadata shape: {metadata.shape}\")\n",
    "\n",
    "    # --- OTU Table Processing ---\n",
    "    # Convert to numeric, handle errors\n",
    "    try:\n",
    "        otu_table = otu_table.astype(float)\n",
    "    except ValueError as e:\n",
    "        non_numeric_cols = otu_table.apply(lambda s: pd.to_numeric(s, errors='coerce').isna().any())\n",
    "        problem_cols = non_numeric_cols[non_numeric_cols].index.tolist()\n",
    "        raise ValueError(f\"OTU table contains non-numeric values. Problematic columns might include: {problem_cols}. Original error: {e}\")\n",
    "\n",
    "    # Drop OTUs that are all zero across remaining samples\n",
    "    initial_otus = otu_table.shape[1]\n",
    "    otu_table = otu_table.loc[:, (otu_table != 0).any(axis=0)]\n",
    "    otus_removed = initial_otus - otu_table.shape[1]\n",
    "    if otus_removed > 0:\n",
    "        print(f\"  Removed {otus_removed} OTUs that were all zero.\")\n",
    "\n",
    "    # Normalize to relative abundances, handle samples with zero sum\n",
    "    sample_sums = otu_table.sum(axis=1)\n",
    "    zero_sum_samples = sample_sums[sample_sums <= 0].index # Check for <= 0\n",
    "    if not zero_sum_samples.empty:\n",
    "        print(f\"  Warning: Samples {zero_sum_samples.tolist()} have zero or negative total abundance. Removing these {len(zero_sum_samples)} samples before normalization.\")\n",
    "        otu_table = otu_table.drop(index=zero_sum_samples)\n",
    "        metadata = metadata.drop(index=zero_sum_samples)\n",
    "        sample_sums = sample_sums.drop(index=zero_sum_samples) # Update sums for division\n",
    "\n",
    "    if otu_table.empty:\n",
    "        raise ValueError(\"OTU table is empty after filtering zero-sum samples. Cannot proceed.\")\n",
    "\n",
    "    otu_table = otu_table.div(sample_sums, axis=0)\n",
    "    print(f\"  Normalized OTU table to relative abundances. Final shape: {otu_table.shape}\")\n",
    "\n",
    "    # --- Grouping (Optional for return) ---\n",
    "    # Grouping after normalization is typical for finding average relative abundance per group\n",
    "    otu_table_grouped = otu_table.groupby(metadata[condition_id]).mean()\n",
    "\n",
    "    # --- Phylogenetic Distances (Optional) ---\n",
    "    phylo_distances = None\n",
    "    if phylo and distance_matrix_file_path:\n",
    "        print(f\" Processing phylogenetic distance matrix: {distance_matrix_file_path}\")\n",
    "        try:\n",
    "            phylo_distances_df = pd.read_csv(distance_matrix_file_path, index_col=0)\n",
    "            # Ensure index/columns match remaining OTUs in otu_table\n",
    "            valid_otus = otu_table.columns.intersection(phylo_distances_df.index).intersection(phylo_distances_df.columns)\n",
    "            if len(valid_otus) < 2:\n",
    "                 print(\"  Warning: Less than 2 common OTUs found between OTU table and distance matrix. Phylogenetic distances cannot be computed effectively.\")\n",
    "            else:\n",
    "                phylo_distances = phylo_distances_df.loc[valid_otus, valid_otus].to_numpy()\n",
    "                print(f\"  Loaded phylogenetic distances for {len(valid_otus)} common OTUs.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  Warning: Phylogenetic distance matrix file not found at {distance_matrix_file_path}. Proceeding without phylogenetic distances.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error reading or processing phylogenetic distance matrix: {e}. Proceeding without it.\")\n",
    "\n",
    "    # Return the processed tables and metadata\n",
    "    print(\" Data processing function finished.\")\n",
    "    return otu_table, metadata, phylo_distances, metadata[condition_id].to_numpy(), otu_table_grouped\n",
    "\n",
    "\n",
    "# --- Function to compute weights within a sample ---\n",
    "def compute_sample_weights(sample_rel_otu):\n",
    "    \"\"\"Compute weights between species based on relative abundance (Original Logic).\"\"\"\n",
    "    sample_rel_otu = np.array(sample_rel_otu)\n",
    "    n_species = len(sample_rel_otu)\n",
    "\n",
    "    # Return zero matrices if calculation isn't possible\n",
    "    if n_species < 2:\n",
    "        return np.zeros((n_species, n_species)), np.zeros((n_species, n_species))\n",
    "\n",
    "    # Binary presence/absence matrix (for potential future use, returned but not used for weights)\n",
    "    binary_sample = (sample_rel_otu > 0).astype(int)\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample)\n",
    "\n",
    "    # Tile the abundance vector to form a matrix\n",
    "    sample_matrix = np.tile(sample_rel_otu, (n_species, 1))\n",
    "\n",
    "    # Calculate inverse diagonal matrix (handle division by zero)\n",
    "    diagonal_elements = np.diag(sample_matrix)\n",
    "    inv_diag_elements = np.zeros_like(diagonal_elements, dtype=float)\n",
    "    non_zero_mask = diagonal_elements != 0\n",
    "    # Add small epsilon to avoid division by zero if any diagonal element is exactly zero but present\n",
    "    # diagonal_elements_safe = diagonal_elements + np.finfo(float).eps\n",
    "    # inv_diag_elements[non_zero_mask] = 1.0 / diagonal_elements_safe[non_zero_mask] # Safer division\n",
    "    inv_diag_elements[non_zero_mask] = 1.0 / diagonal_elements[non_zero_mask] # Original approach\n",
    "    inv_diag_matrix = np.diag(inv_diag_elements)\n",
    "\n",
    "    # Calculate ratio matrix: R = D^(-1) * A\n",
    "    # Use np.dot for matrix multiplication for clarity and potential performance\n",
    "    ratios = np.dot(inv_diag_matrix, sample_matrix)\n",
    "\n",
    "    # Calculate weights based on ratios (original reciprocal logic)\n",
    "    # W_ij = 2 / R_ij if i != j, W_ii = 0\n",
    "    # Need to handle potential division by zero if R_ij is zero\n",
    "    weights = np.zeros_like(ratios)\n",
    "    non_diagonal_mask = ~np.eye(n_species, dtype=bool)\n",
    "    valid_ratios_mask = (ratios != 0) & non_diagonal_mask\n",
    "\n",
    "    weights[valid_ratios_mask] = 2.0 / ratios[valid_ratios_mask]\n",
    "\n",
    "    # Ensure symmetry (although the calculation above should be symmetric if R_ij = 1/R_ji)\n",
    "    # Taking the upper triangle and adding its transpose enforces symmetry robustly\n",
    "    weights_final = np.triu(weights, k=1)\n",
    "    weights_final = weights_final + weights_final.T\n",
    "\n",
    "    # Replace infinities resulting from division by zero with zero (or NaN)\n",
    "    weights_final[~np.isfinite(weights_final)] = 0 # Treat infinite weights as 0 (no connection)\n",
    "\n",
    "    return weights_final, sample_binary_matrix\n",
    "\n",
    "\n",
    "# --- Function to aggregate data for a condition ---\n",
    "def aggregate_samples_for_condition(feature_table, metadata, condition_group, condition_id_col='Group ID'):\n",
    "    \"\"\"Aggregate samples by averaging for each condition.\"\"\"\n",
    "    if condition_id_col not in metadata.columns:\n",
    "        # This should be caught earlier, but double-check\n",
    "        raise ValueError(f\"Condition ID column '{condition_id_col}' not found in metadata during aggregation.\")\n",
    "\n",
    "    # Find samples belonging to the current condition group\n",
    "    samples_in_condition = metadata[metadata[condition_id_col] == condition_group].index\n",
    "\n",
    "    if samples_in_condition.empty:\n",
    "         return pd.Series(dtype=float), pd.Index([]) # No samples for this condition in metadata\n",
    "\n",
    "    # Find which of these samples are actually present in the feature table index\n",
    "    valid_samples = samples_in_condition.intersection(feature_table.index)\n",
    "    if valid_samples.empty:\n",
    "        # Samples listed in metadata but not found in the feature table (e.g., removed due to zero sum)\n",
    "        return pd.Series(dtype=float), pd.Index([])\n",
    "\n",
    "    # Select the rows for these samples\n",
    "    condition_feature_table = feature_table.loc[valid_samples]\n",
    "\n",
    "    # Check for empty or all-NaN data *before* averaging\n",
    "    if condition_feature_table.empty or condition_feature_table.isnull().all().all():\n",
    "         print(f\"  Warning: Data for condition group '{condition_group}' is empty or all NaN after sample selection.\")\n",
    "         return pd.Series(dtype=float), valid_samples # Return empty series but indicate which samples were considered\n",
    "\n",
    "    # Calculate the mean abundance for each species across these samples\n",
    "    aggregated_feature_table = condition_feature_table.mean(axis=0)\n",
    "\n",
    "    # Fill any resulting NaNs (e.g., if a species was NaN for all samples in the group) with 0\n",
    "    aggregated_feature_table = aggregated_feature_table.fillna(0)\n",
    "\n",
    "    return aggregated_feature_table, valid_samples\n",
    "\n",
    "\n",
    "# --- Function to create graph from aggregated data ---\n",
    "def create_graph_from_aggregated_data(aggregated_data, threshold=0.1):\n",
    "    \"\"\"Create a graph based on aggregated data and edge weight threshold.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    # Select species with abundance > 0 in the aggregated data\n",
    "    nonzero_species = aggregated_data[aggregated_data > 0]\n",
    "\n",
    "    # Handle cases with 0 or 1 species\n",
    "    if nonzero_species.empty:\n",
    "        return G # Return empty graph\n",
    "    elif len(nonzero_species) == 1:\n",
    "        species = nonzero_species.index[0]\n",
    "        abundance = nonzero_species.iloc[0]\n",
    "        G.add_node(species, relab=abundance)\n",
    "        return G # Return graph with single node\n",
    "\n",
    "    # Add nodes for all species present\n",
    "    for species, abundance in nonzero_species.items():\n",
    "        G.add_node(species, relab=abundance)\n",
    "\n",
    "    # Calculate pairwise weights using the *same* function as for individual samples\n",
    "    # This applies the ratio logic to the *average* abundances for the condition\n",
    "    species_list = nonzero_species.index.tolist()\n",
    "    abundance_array = nonzero_species.to_numpy()\n",
    "    weights_matrix, _ = compute_sample_weights(abundance_array)\n",
    "\n",
    "    # Add edges where the absolute weight exceeds the threshold\n",
    "    added_edges = 0\n",
    "    for i in range(len(species_list)):\n",
    "        for j in range(i + 1, len(species_list)):\n",
    "            weight = weights_matrix[i, j]\n",
    "            # Check if weight is finite and its absolute value is above threshold\n",
    "            if np.isfinite(weight) and abs(weight) > threshold:\n",
    "                G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "                added_edges += 1\n",
    "    #print(f\"   Aggregated graph: Added {added_edges} edges with abs(weight) > {threshold}\") # Verbose\n",
    "    return G\n",
    "\n",
    "\n",
    "# --- Function to filter feature table based on graph nodes ---\n",
    "def filter_feature_table(feature_table, graph_nodes):\n",
    "    \"\"\"Filter the feature table (samples x species) to retain only species present in the graph.\"\"\"\n",
    "    if not isinstance(feature_table, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'feature_table' must be a pandas DataFrame.\")\n",
    "    if not hasattr(graph_nodes, '__iter__') or isinstance(graph_nodes, str):\n",
    "         raise TypeError(\"Input 'graph_nodes' must be a list or other iterable of node names.\")\n",
    "\n",
    "    # Identify which graph nodes are present as columns in the feature table\n",
    "    valid_nodes = [node for node in graph_nodes if node in feature_table.columns]\n",
    "    # missing_nodes = set(graph_nodes) - set(valid_nodes) # Keep track if needed for warnings\n",
    "\n",
    "    if not valid_nodes:\n",
    "        # Return empty DataFrame with the original sample index if no nodes match\n",
    "        return pd.DataFrame(index=feature_table.index)\n",
    "\n",
    "    # Return the subset of the feature table with only the valid node columns\n",
    "    return feature_table[valid_nodes]\n",
    "\n",
    "\n",
    "# --- Function to create and save Condition-Level Graphs ---\n",
    "def create_condition_graphs(feature_table, metadata, output_dir, condition_id_col='Group ID', edge_threshold=0.1):\n",
    "    \"\"\"Creates and saves CONDITION-LEVEL graphs and associated filtered tables.\"\"\"\n",
    "    if condition_id_col not in metadata.columns:\n",
    "        # Redundant check, but safe\n",
    "        raise ValueError(f\"Condition ID column '{condition_id_col}' not found in metadata.\")\n",
    "\n",
    "    # Get unique condition groups from the specified metadata column\n",
    "    condition_groups = metadata[condition_id_col].unique()\n",
    "    print(f\"\\n--- Starting CONDITION-LEVEL Graph Generation ({len(condition_groups)} conditions) ---\")\n",
    "    print(f\" Conditions found in '{condition_id_col}': {condition_groups.tolist()}\")\n",
    "\n",
    "    # Define the base directory for condition graph outputs\n",
    "    base_condition_dir = os.path.join(output_dir, \"condition_graphs\")\n",
    "    os.makedirs(base_condition_dir, exist_ok=True)\n",
    "    print(f\" Condition graphs will be saved in: '{base_condition_dir}'\")\n",
    "\n",
    "    conditions_processed = 0\n",
    "    conditions_skipped = 0\n",
    "    # Loop through each unique condition group\n",
    "    for condition_group in condition_groups:\n",
    "        # Sanitize the condition name for use in file paths\n",
    "        safe_condition_name = str(condition_group).replace(' ', '_').replace('/', '-').replace('\\\\', '-')\n",
    "        condition_output_dir = os.path.join(base_condition_dir, safe_condition_name)\n",
    "        os.makedirs(condition_output_dir, exist_ok=True) # Ensure specific dir exists\n",
    "\n",
    "        # Aggregate data for the current condition\n",
    "        aggregated_data, samples_in_condition = aggregate_samples_for_condition(\n",
    "            feature_table, metadata, condition_group, condition_id_col=condition_id_col\n",
    "        )\n",
    "\n",
    "        # Skip if no data could be aggregated (no samples or all NaN)\n",
    "        if aggregated_data.empty:\n",
    "            #print(f\"  Skipping condition '{condition_group}': No valid data for aggregation.\")\n",
    "            conditions_skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Create the graph from the aggregated data using the specified threshold\n",
    "        G_condition = create_graph_from_aggregated_data(aggregated_data, threshold=edge_threshold)\n",
    "\n",
    "        # Skip if the resulting graph has no nodes (should only happen if aggregated_data was all zero)\n",
    "        if G_condition.number_of_nodes() == 0:\n",
    "            #print(f\"  Skipping condition '{condition_group}': Generated graph has no nodes.\")\n",
    "            conditions_skipped += 1\n",
    "            continue\n",
    "\n",
    "        conditions_processed += 1\n",
    "        graph_nodes = list(G_condition.nodes())\n",
    "        #print(f\"  Condition '{condition_group}': Graph created ({G_condition.number_of_nodes()} nodes, {G_condition.number_of_edges()} edges).\")\n",
    "\n",
    "        # --- Save Condition Graph (Pickle & GraphML) ---\n",
    "        graph_pickle_path = os.path.join(condition_output_dir, f\"graph_{safe_condition_name}.pkl\")\n",
    "        graphml_path = os.path.join(condition_output_dir, f\"graph_{safe_condition_name}.xgml\")\n",
    "        try:\n",
    "            with open(graph_pickle_path, 'wb') as f: pickle.dump(G_condition, f)\n",
    "            nx.write_graphml(G_condition, graphml_path)\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR saving graph for condition {condition_group}: {e}\")\n",
    "\n",
    "        # --- Filter and Save Feature Table for this Condition's Samples ---\n",
    "        if not samples_in_condition.empty:\n",
    "            try:\n",
    "                # Ensure we only use samples that were actually found in the feature table\n",
    "                valid_samples_in_condition = samples_in_condition.intersection(feature_table.index)\n",
    "                if not valid_samples_in_condition.empty:\n",
    "                    # Select rows for samples in this condition\n",
    "                    feature_table_condition_samples = feature_table.loc[valid_samples_in_condition]\n",
    "                    # Filter columns to include only nodes present in the condition graph\n",
    "                    feature_table_filtered_condition = filter_feature_table(feature_table_condition_samples, graph_nodes)\n",
    "\n",
    "                    # Save the filtered table if it's not empty\n",
    "                    if not feature_table_filtered_condition.empty:\n",
    "                        filtered_table_pickle_path = os.path.join(condition_output_dir, f\"filtered_table_{safe_condition_name}.pkl\")\n",
    "                        filtered_table_csv_path = os.path.join(condition_output_dir, f\"feature_table_filtered_{safe_condition_name}.csv\")\n",
    "                        # Save as Pickle\n",
    "                        with open(filtered_table_pickle_path, 'wb') as f: pickle.dump(feature_table_filtered_condition, f)\n",
    "                        # Save as CSV\n",
    "                        feature_table_filtered_condition.to_csv(filtered_table_csv_path)\n",
    "                    #else: # Be less verbose\n",
    "                        #print(f\"  Condition '{condition_group}': Filtered feature table is empty (no common nodes/species), not saving.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"  ERROR saving filtered feature table for condition {condition_group}: {e}\")\n",
    "\n",
    "    print(f\"--- Finished CONDITION-LEVEL Graph Generation (Processed: {conditions_processed}, Skipped: {conditions_skipped}) ---\")\n",
    "\n",
    "\n",
    "# --- Function to create graph for a single sample ---\n",
    "def create_individual_sample_graph(sample_data_series, threshold=None):\n",
    "    \"\"\"\n",
    "    Creates a NetworkX graph for a single sample's abundance data.\n",
    "\n",
    "    Args:\n",
    "        sample_data_series (pd.Series): A series where index is species, values are abundances.\n",
    "        threshold (float, optional): Edge weight threshold.\n",
    "                                     If None or <= 0, creates \"unfiltered\" (all finite weights).\n",
    "                                     If > 0, creates \"filtered\" (abs(weight) > threshold).\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The generated graph for the sample.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    # Filter species with non-zero abundance in this sample\n",
    "    # Ensure we handle potential non-numeric types gracefully if they slipped through\n",
    "    numeric_sample_data = pd.to_numeric(sample_data_series, errors='coerce').fillna(0)\n",
    "    nonzero_species = numeric_sample_data[numeric_sample_data > 0]\n",
    "\n",
    "    # Handle cases with 0 or 1 species\n",
    "    if nonzero_species.empty:\n",
    "        return G # Return empty graph\n",
    "    elif len(nonzero_species) == 1:\n",
    "        species = nonzero_species.index[0]\n",
    "        abundance = nonzero_species.iloc[0]\n",
    "        G.add_node(species, relab=abundance)\n",
    "        return G # Return graph with single node\n",
    "\n",
    "    # Add nodes for all species present in the sample\n",
    "    for species, abundance in nonzero_species.items():\n",
    "        G.add_node(species, relab=abundance)\n",
    "\n",
    "    # Calculate pairwise weights using the existing function\n",
    "    species_list = nonzero_species.index.tolist()\n",
    "    abundances_array = nonzero_species.to_numpy()\n",
    "    weights_matrix, _ = compute_sample_weights(abundances_array)\n",
    "\n",
    "    # Add edges based on threshold\n",
    "    added_edges_unfiltered = 0\n",
    "    added_edges_filtered = 0\n",
    "    for i in range(len(species_list)):\n",
    "        for j in range(i + 1, len(species_list)):\n",
    "            weight = weights_matrix[i, j]\n",
    "\n",
    "            # Check if weight is valid (finite) before applying threshold logic\n",
    "            if np.isfinite(weight):\n",
    "                # Logic for \"unfiltered\" (threshold is None or <= 0)\n",
    "                if threshold is None or threshold <= 0:\n",
    "                    G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "                    added_edges_unfiltered += 1\n",
    "                # Logic for \"filtered\" (threshold > 0)\n",
    "                elif abs(weight) > threshold:\n",
    "                    G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "                    added_edges_filtered += 1\n",
    "\n",
    "    # Depending on the threshold passed, G will represent either filtered or unfiltered\n",
    "    # The counts added_edges_unfiltered/filtered are just for potential debugging insight\n",
    "    return G\n",
    "\n",
    "\n",
    "# --- Function to loop through samples and save individual graphs ---\n",
    "def create_and_save_individual_graphs(feature_table, base_output_dir, edge_threshold):\n",
    "    \"\"\"\n",
    "    Creates and saves filtered and unfiltered graphs for each individual sample.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting INDIVIDUAL Sample Graph Generation ({len(feature_table)} samples) ---\")\n",
    "    # Define the specific directory for individual graph outputs\n",
    "    individual_dir = os.path.join(base_output_dir, \"individual_graphs\")\n",
    "    os.makedirs(individual_dir, exist_ok=True)\n",
    "    print(f\" Individual graphs will be saved in: '{individual_dir}'\")\n",
    "\n",
    "    num_samples = len(feature_table)\n",
    "    start_time = time.time()\n",
    "    samples_processed = 0\n",
    "    errors_encountered = 0\n",
    "\n",
    "    # Iterate through each sample (row) in the feature table\n",
    "    for i, (sample_id, sample_data) in enumerate(feature_table.iterrows()):\n",
    "        # Sanitize sample_id for use in file/directory paths\n",
    "        # Replace common problematic characters\n",
    "        safe_sample_id = str(sample_id).replace('/', '-').replace('\\\\', '-').replace(':', '-').replace(' ', '_')\n",
    "        sample_output_dir = os.path.join(individual_dir, safe_sample_id)\n",
    "\n",
    "        try:\n",
    "            # Ensure the specific directory for this sample exists\n",
    "            os.makedirs(sample_output_dir, exist_ok=True)\n",
    "\n",
    "            # Print progress periodically\n",
    "            if (i + 1) % 50 == 0 or i == 0 or (i + 1) == num_samples:\n",
    "                 elapsed = time.time() - start_time\n",
    "                 print(f\" Processing sample {i+1}/{num_samples}: {sample_id} ({elapsed:.2f}s elapsed)\")\n",
    "\n",
    "            # --- Create and Save Unfiltered Graph ---\n",
    "            # Pass threshold=None to get the unfiltered version\n",
    "            G_unfiltered = create_individual_sample_graph(sample_data, threshold=None)\n",
    "            # Save only if the graph has at least one node\n",
    "            if G_unfiltered.number_of_nodes() > 0:\n",
    "                unfiltered_path = os.path.join(sample_output_dir, \"graph_unfiltered.pkl\")\n",
    "                with open(unfiltered_path, 'wb') as f:\n",
    "                    pickle.dump(G_unfiltered, f)\n",
    "\n",
    "            # --- Create and Save Filtered Graph ---\n",
    "            # Pass the actual edge_threshold to get the filtered version\n",
    "            G_filtered = create_individual_sample_graph(sample_data, threshold=edge_threshold)\n",
    "            # Save only if the filtered graph has edges (meaning threshold was met)\n",
    "            if G_filtered.number_of_edges() > 0:\n",
    "                 filtered_path = os.path.join(sample_output_dir, \"graph_filtered.pkl\")\n",
    "                 with open(filtered_path, 'wb') as f:\n",
    "                    pickle.dump(G_filtered, f)\n",
    "\n",
    "            samples_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            errors_encountered += 1\n",
    "            print(f\"\\n  ERROR processing sample {sample_id}: {e}\")\n",
    "            # Print traceback for unexpected errors during individual sample processing\n",
    "            # traceback.print_exc()\n",
    "            # Decide whether to continue or stop\n",
    "            # continue # Skip to the next sample if one fails\n",
    "            # break # Stop processing if one sample fails\n",
    "\n",
    "    # Summary after the loop\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"--- Finished INDIVIDUAL Sample Graph Generation ---\")\n",
    "    print(f\"  Total samples processed: {samples_processed}\")\n",
    "    print(f\"  Errors encountered: {errors_encountered}\")\n",
    "    print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # Define file paths (UPDATE THESE PATHS)\n",
    "    \n",
    "    otu_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/OTU_table.csv\"\n",
    "    metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/ot_omics/data/rvc/metadata.tsv\"\n",
    "\n",
    "\n",
    "    # Define key column names from your files (UPDATE THESE)\n",
    "    sample_id_col = 'Sample-ID'     # Column name for sample IDs in metadata (must match index of OTU table after reading)\n",
    "    condition_id_col = 'Group ID'   # Column name for the condition/group in metadata\n",
    "\n",
    "    # Define output directory (UPDATE IF NEEDED)\n",
    "    output_dir = \"graph_outputs_combined_final\" # Base directory for all outputs\n",
    "    # Define edge weight threshold for FILTERED graphs (individual and condition)\n",
    "    graph_edge_threshold = 0.1\n",
    "    # --- End Configuration ---\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"--- Microbiome Network Analysis Script ---\")\n",
    "    print(f\"Script started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    overall_start_time = time.time()\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        # Ensure base output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory: '{os.path.abspath(output_dir)}'\")\n",
    "        print(f\"Using Sample ID column: '{sample_id_col}'\")\n",
    "        print(f\"Using Condition ID column: '{condition_id_col}'\")\n",
    "        print(f\"Edge threshold for filtered graphs: {graph_edge_threshold}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        # 1. Process Data\n",
    "        print(\"\\n[Step 1/3] Processing input data...\")\n",
    "        data_start_time = time.time()\n",
    "        feature_table, metadata, _, _, _ = process_data(\n",
    "            otu_file_path, metadata_file_path,\n",
    "            sample_id=sample_id_col,\n",
    "            condition_id=condition_id_col\n",
    "            # Removed unused return values (phylo_distances, labels, otu_table_grouped)\n",
    "        )\n",
    "        print(f\"[Step 1/3] Data processed successfully ({time.time() - data_start_time:.2f}s).\")\n",
    "        print(f\"  Final feature table dimensions: {feature_table.shape}\")\n",
    "        print(f\"  Final metadata dimensions: {metadata.shape}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        # 2. Create and Save Individual Sample Graphs (Filtered & Unfiltered)\n",
    "        print(\"\\n[Step 2/3] Creating Individual Sample Graphs...\")\n",
    "        create_and_save_individual_graphs(\n",
    "            feature_table,\n",
    "            output_dir, # Pass the base directory\n",
    "            edge_threshold=graph_edge_threshold\n",
    "        )\n",
    "        print(\"[Step 2/3] Individual sample graph generation finished.\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "\n",
    "        # 3. Create and Save Condition-Level Graphs (Filtered)\n",
    "        print(\"\\n[Step 3/3] Creating Condition-Level Graphs...\")\n",
    "        cond_start_time = time.time()\n",
    "        create_condition_graphs(\n",
    "            feature_table,\n",
    "            metadata,\n",
    "            output_dir, # Pass the base directory\n",
    "            condition_id_col=condition_id_col,\n",
    "            edge_threshold=graph_edge_threshold\n",
    "        )\n",
    "        print(f\"[Step 3/3] Condition-level graph generation finished ({time.time() - cond_start_time:.2f}s).\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "\n",
    "        overall_end_time = time.time()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"--- All processing finished successfully ---\")\n",
    "        print(f\"Total execution time: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "        print(f\"Output saved in directory: {os.path.abspath(output_dir)}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    # --- Error Handling ---\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n--- FATAL ERROR: Input file not found ---\", flush=True)\n",
    "        print(e)\n",
    "        print(\"Please check the paths provided in the Configuration section.\", flush=True)\n",
    "    except ValueError as ve:\n",
    "        print(f\"\\n--- FATAL ERROR: Data validation or processing error ---\", flush=True)\n",
    "        print(ve)\n",
    "        print(\"Please check the format and content of your input files and the column names specified.\", flush=True)\n",
    "    except KeyError as ke:\n",
    "        print(f\"\\n--- FATAL ERROR: Column not found ---\", flush=True)\n",
    "        print(f\"Column '{ke}' was not found. Please check `sample_id_col` and `condition_id_col` configuration match your metadata file headers.\", flush=True)\n",
    "    except MemoryError:\n",
    "         print(f\"\\n--- FATAL ERROR: Insufficient memory ---\", flush=True)\n",
    "         print(\"The process ran out of memory. This might happen with very large datasets.\", flush=True)\n",
    "         print(\"Consider running on a machine with more RAM or processing data in chunks.\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected FATAL error occurred ---\", flush=True)\n",
    "        # Print the full traceback for unexpected errors to help debugging\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        print(f\"\\nScript finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting Simplified Analysis in: graph_outputs_combined_final/individual_graphs\n",
      "==================================================\n",
      "Found 58 items in the directory. Checking for sample folders...\n",
      "\n",
      "--- Processing Sample: Sample1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 231\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 207\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 28\n",
      "     Edges: 378\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 28\n",
      "     Edges: 350\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample11 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 24\n",
      "     Edges: 276\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 24\n",
      "     Edges: 268\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample12 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 23\n",
      "     Edges: 253\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 23\n",
      "     Edges: 240\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample13 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 31\n",
      "     Edges: 465\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 31\n",
      "     Edges: 449\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample14 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 30\n",
      "     Edges: 435\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 30\n",
      "     Edges: 406\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample15 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample16 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 335\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample17 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 325\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 312\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample18 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 332\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample19 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 231\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 216\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 325\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 257\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample20 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 325\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 310\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 190\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 170\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 325\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 26\n",
      "     Edges: 290\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 231\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 22\n",
      "     Edges: 186\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 24\n",
      "     Edges: 276\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 24\n",
      "     Edges: 222\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 190\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 158\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 30\n",
      "     Edges: 435\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 30\n",
      "     Edges: 347\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: Sample9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 28\n",
      "     Edges: 378\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 28\n",
      "     Edges: 325\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 120\n",
      "     Edges: 7140\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 120\n",
      "     Edges: 6159\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 82\n",
      "     Edges: 3321\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 82\n",
      "     Edges: 3108\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 108\n",
      "     Edges: 5778\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 108\n",
      "     Edges: 4994\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 5\n",
      "     Edges: 10\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 5\n",
      "     Edges: 10\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2740\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 59\n",
      "     Edges: 1711\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 59\n",
      "     Edges: 1585\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 23\n",
      "     Edges: 253\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 23\n",
      "     Edges: 247\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 6216\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 5602\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 137\n",
      "     Edges: 9316\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 137\n",
      "     Edges: 8162\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 90\n",
      "     Edges: 4005\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 90\n",
      "     Edges: 3598\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 74\n",
      "     Edges: 2701\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 74\n",
      "     Edges: 2341\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 80\n",
      "     Edges: 3160\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 80\n",
      "     Edges: 2857\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 95\n",
      "     Edges: 4465\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 95\n",
      "     Edges: 3828\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3655\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3092\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 98\n",
      "     Edges: 4753\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 98\n",
      "     Edges: 4228\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2550\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3291\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 78\n",
      "     Edges: 3003\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 78\n",
      "     Edges: 2593\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3309\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3299\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 83\n",
      "     Edges: 3403\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 83\n",
      "     Edges: 3084\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 111\n",
      "     Edges: 6105\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 111\n",
      "     Edges: 5291\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 104\n",
      "     Edges: 5356\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 104\n",
      "     Edges: 4950\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3396\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 85\n",
      "     Edges: 3570\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 85\n",
      "     Edges: 3256\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 118\n",
      "     Edges: 6903\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 118\n",
      "     Edges: 6145\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 6216\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 5379\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 100\n",
      "     Edges: 4950\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 100\n",
      "     Edges: 4432\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 109\n",
      "     Edges: 5886\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 109\n",
      "     Edges: 5445\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 16\n",
      "     Edges: 120\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 16\n",
      "     Edges: 117\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 17\n",
      "     Edges: 136\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 17\n",
      "     Edges: 131\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 190\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 188\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3655\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3076\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2564\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 44\n",
      "     Edges: 946\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 44\n",
      "     Edges: 817\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 49\n",
      "     Edges: 1176\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 49\n",
      "     Edges: 968\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 40\n",
      "     Edges: 780\n",
      "     'relab' node attribute found: True\n",
      "  -> Analyzing 'filtered' graph...\n",
      "     Nodes: 40\n",
      "     Edges: 731\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "==================================================\n",
      "Analysis Finished: Checked 58 sample directories.\n",
      "  Successfully loaded and analyzed basic info for: 116 graphs.\n",
      "Total execution time: 0.10 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Paste all of this into one Jupyter Notebook cell\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if they become noisy, e.g., from pickle\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def simple_analyze_graphs(individual_graphs_dir):\n",
    "    \"\"\"\n",
    "    Simplified function to analyze individual graph pickle files.\n",
    "    Prints node/edge counts and checks if the 'relab' node attribute exists.\n",
    "    Does NOT generate plots.\n",
    "\n",
    "    Args:\n",
    "        individual_graphs_dir (str): Path to the directory containing the\n",
    "                                     individual sample subdirectories\n",
    "                                     (e.g., 'output_dir/individual_graphs').\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Starting Simplified Analysis in: {individual_graphs_dir}\")\n",
    "    print(\"=\"*50)\n",
    "    start_run_time = time.time()\n",
    "\n",
    "    # Validate input directory\n",
    "    if not os.path.isdir(individual_graphs_dir):\n",
    "        print(f\"ERROR: Input directory not found or is not a directory.\")\n",
    "        print(f\"Please ensure this path exists: {individual_graphs_dir}\")\n",
    "        return\n",
    "\n",
    "    sample_dirs_found = 0\n",
    "    graphs_analyzed = 0\n",
    "    load_errors = 0\n",
    "\n",
    "    # Iterate through items (potential sample directories)\n",
    "    try:\n",
    "        all_items = sorted(os.listdir(individual_graphs_dir))\n",
    "    except FileNotFoundError:\n",
    "         print(f\"ERROR: Cannot list contents of directory (check permissions?): {individual_graphs_dir}\")\n",
    "         return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR listing directory contents: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(all_items)} items in the directory. Checking for sample folders...\")\n",
    "\n",
    "    for item_name in all_items:\n",
    "        item_path = os.path.join(individual_graphs_dir, item_name)\n",
    "\n",
    "        # Process only if it's a directory\n",
    "        if os.path.isdir(item_path):\n",
    "            sample_id = item_name\n",
    "            sample_dirs_found += 1\n",
    "            print(f\"\\n--- Processing Sample: {sample_id} ---\")\n",
    "\n",
    "            # Define expected pickle file names\n",
    "            files_to_check = {\n",
    "                \"unfiltered\": os.path.join(item_path, \"graph_unfiltered.pkl\"),\n",
    "                \"filtered\": os.path.join(item_path, \"graph_filtered.pkl\")\n",
    "            }\n",
    "\n",
    "            # Loop through unfiltered and filtered graph types for the current sample\n",
    "            for graph_type, pickle_path in files_to_check.items():\n",
    "                if os.path.exists(pickle_path):\n",
    "                    print(f\"  -> Analyzing '{graph_type}' graph...\")\n",
    "                    graph_loaded = False\n",
    "                    try:\n",
    "                        # Load graph from pickle file\n",
    "                        with open(pickle_path, 'rb') as f:\n",
    "                            G = pickle.load(f)\n",
    "\n",
    "                        # Basic validation\n",
    "                        if not isinstance(G, nx.Graph):\n",
    "                             print(f\"  ERROR: File content is not a NetworkX Graph object: {pickle_path}\")\n",
    "                             load_errors += 1\n",
    "                             continue # Skip to next file type or sample\n",
    "\n",
    "                        graph_loaded = True\n",
    "                        graphs_analyzed += 1\n",
    "\n",
    "                        # 1. Get Node and Edge Counts\n",
    "                        num_nodes = G.number_of_nodes()\n",
    "                        num_edges = G.number_of_edges()\n",
    "\n",
    "                        # 2. Simple check for 'relab' attribute on *any* node\n",
    "                        relab_found = False\n",
    "                        # Only check nodes if the graph has nodes\n",
    "                        if num_nodes > 0:\n",
    "                            # Iterate through nodes with their data dictionaries\n",
    "                            for _, node_data in G.nodes(data=True):\n",
    "                                # If 'relab' key exists in the dictionary for this node\n",
    "                                if 'relab' in node_data:\n",
    "                                    relab_found = True\n",
    "                                    break # Found it, no need to check other nodes\n",
    "\n",
    "                        # 3. Print results for this graph\n",
    "                        print(f\"     Nodes: {num_nodes}\")\n",
    "                        print(f\"     Edges: {num_edges}\")\n",
    "                        print(f\"     'relab' node attribute found: {relab_found}\")\n",
    "\n",
    "                    # Handle errors during file loading or processing\n",
    "                    except (pickle.UnpicklingError, EOFError, TypeError, ModuleNotFoundError) as e:\n",
    "                        load_errors += 1\n",
    "                        print(f\"  ERROR loading pickle file {pickle_path}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        load_errors += 1\n",
    "                        # Print more detail for unexpected errors\n",
    "                        print(f\"  UNEXPECTED ERROR processing file {pickle_path}: {e}\")\n",
    "                        # import traceback\n",
    "                        # traceback.print_exc() # Uncomment for full traceback if needed\n",
    "\n",
    "                else:\n",
    "                    # File doesn't exist, print a message\n",
    "                    print(f\"  -> Skipping '{graph_type}' graph (file not found).\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    end_run_time = time.time()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if sample_dirs_found == 0:\n",
    "         print(\"Analysis Finished: No sample subdirectories found.\")\n",
    "    else:\n",
    "        print(f\"Analysis Finished: Checked {sample_dirs_found} sample directories.\")\n",
    "        print(f\"  Successfully loaded and analyzed basic info for: {graphs_analyzed} graphs.\")\n",
    "        if load_errors > 0:\n",
    "             print(f\"  Encountered errors loading/processing: {load_errors} graphs.\")\n",
    "    print(f\"Total execution time: {end_run_time - start_run_time:.2f} seconds\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "\n",
    "graphs_directory = \"graph_outputs_combined_final/individual_graphs\"\n",
    "\n",
    "simple_analyze_graphs(graphs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting Analysis in: graph_outputs_combined_final/individual_graphs\n",
      "(Will plot edge weight distribution for the first graph found)\n",
      "==================================================\n",
      "Found 38 items. Checking for sample folders...\n",
      "\n",
      "--- Processing Sample: T1-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 27\n",
      "     Edges: 351\n",
      "     'relab' node attribute found: True\n",
      "     Attempting to plot edge weights for this graph (as it's the first)...\n",
      "     Plotting distribution of 351 edge weights...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Plot displayed above.\n",
      "\n",
      "--- Processing Sample: T1-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 120\n",
      "     Edges: 7140\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 82\n",
      "     Edges: 3321\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 108\n",
      "     Edges: 5778\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 5\n",
      "     Edges: 10\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 59\n",
      "     Edges: 1711\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 23\n",
      "     Edges: 253\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 6216\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T1-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 137\n",
      "     Edges: 9316\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 90\n",
      "     Edges: 4005\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 74\n",
      "     Edges: 2701\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 80\n",
      "     Edges: 3160\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 95\n",
      "     Edges: 4465\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3655\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 98\n",
      "     Edges: 4753\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 78\n",
      "     Edges: 3003\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T2-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-10 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 83\n",
      "     Edges: 3403\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 111\n",
      "     Edges: 6105\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 104\n",
      "     Edges: 5356\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 87\n",
      "     Edges: 3741\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 85\n",
      "     Edges: 3570\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 118\n",
      "     Edges: 6903\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 112\n",
      "     Edges: 6216\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 100\n",
      "     Edges: 4950\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T4-9 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 109\n",
      "     Edges: 5886\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-1 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 16\n",
      "     Edges: 120\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-2 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 17\n",
      "     Edges: 136\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-3 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 20\n",
      "     Edges: 190\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-4 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 86\n",
      "     Edges: 3655\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-5 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 76\n",
      "     Edges: 2850\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-6 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 44\n",
      "     Edges: 946\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-7 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 49\n",
      "     Edges: 1176\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "--- Processing Sample: T5-8 ---\n",
      "  -> Analyzing 'unfiltered' graph...\n",
      "     Nodes: 40\n",
      "     Edges: 780\n",
      "     'relab' node attribute found: True\n",
      "\n",
      "==================================================\n",
      "Analysis Finished: Checked 38 sample directories.\n",
      "  Loaded and analyzed basic info for: 38 graphs.\n",
      "Total execution time: 0.14 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Paste all of this into one Jupyter Notebook cell\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt # Added for plotting\n",
    "import numpy as np             # Added for numerical operations\n",
    "\n",
    "# Optional: Suppress warnings if needed\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def analyze_and_plot_first_graph(individual_graphs_dir):\n",
    "    \"\"\"\n",
    "    Simplified analysis: Prints counts/relab check for all graphs,\n",
    "    but PLOTS edge weight distribution ONLY for the FIRST graph loaded successfully.\n",
    "\n",
    "    Args:\n",
    "        individual_graphs_dir (str): Path to the directory containing the\n",
    "                                     individual sample subdirectories.\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Starting Analysis in: {individual_graphs_dir}\")\n",
    "    print(\"(Will plot edge weight distribution for the first graph found)\")\n",
    "    print(\"=\"*50)\n",
    "    start_run_time = time.time()\n",
    "\n",
    "    # Validate input directory\n",
    "    if not os.path.isdir(individual_graphs_dir):\n",
    "        print(f\"ERROR: Input directory not found: {individual_graphs_dir}\")\n",
    "        return\n",
    "\n",
    "    sample_dirs_found = 0\n",
    "    graphs_analyzed = 0\n",
    "    load_errors = 0\n",
    "    first_graph_plotted = False # Flag to plot only once\n",
    "\n",
    "    # Iterate through items (potential sample directories)\n",
    "    try:\n",
    "        # Sort to ensure consistent \"first\" graph if directory listing order varies\n",
    "        all_items = sorted(os.listdir(individual_graphs_dir))\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR listing directory contents: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(all_items)} items. Checking for sample folders...\")\n",
    "\n",
    "    # --- Loop through samples ---\n",
    "    for item_name in all_items:\n",
    "        # If we have already plotted, no need to check further samples for plotting\n",
    "        # We still continue to print stats for other graphs if needed.\n",
    "        # To stop entirely after plotting, uncomment the break statement at the end of the outer loop.\n",
    "\n",
    "        item_path = os.path.join(individual_graphs_dir, item_name)\n",
    "\n",
    "        # Process only if it's a directory\n",
    "        if os.path.isdir(item_path):\n",
    "            sample_id = item_name\n",
    "            sample_dirs_found += 1\n",
    "            print(f\"\\n--- Processing Sample: {sample_id} ---\")\n",
    "\n",
    "            files_to_check = {\n",
    "                \"unfiltered\": os.path.join(item_path, \"graph_unfiltered.pkl\"),\n",
    "                \"filtered\": os.path.join(item_path, \"graph_filtered.pkl\")\n",
    "            }\n",
    "\n",
    "            # --- Loop through graph types (unfiltered/filtered) ---\n",
    "            for graph_type, pickle_path in files_to_check.items():\n",
    "                if os.path.exists(pickle_path):\n",
    "                    print(f\"  -> Analyzing '{graph_type}' graph...\")\n",
    "                    try:\n",
    "                        # Load graph from pickle file\n",
    "                        with open(pickle_path, 'rb') as f:\n",
    "                            G = pickle.load(f)\n",
    "\n",
    "                        if not isinstance(G, nx.Graph):\n",
    "                             print(f\"  ERROR: File content is not a NetworkX Graph object: {pickle_path}\")\n",
    "                             load_errors += 1\n",
    "                             continue # Skip to next file\n",
    "\n",
    "                        graphs_analyzed += 1\n",
    "\n",
    "                        # --- Basic Analysis (for ALL graphs) ---\n",
    "                        num_nodes = G.number_of_nodes()\n",
    "                        num_edges = G.number_of_edges()\n",
    "\n",
    "                        relab_found = False\n",
    "                        if num_nodes > 0:\n",
    "                            for _, node_data in G.nodes(data=True):\n",
    "                                if 'relab' in node_data:\n",
    "                                    relab_found = True\n",
    "                                    break\n",
    "\n",
    "                        print(f\"     Nodes: {num_nodes}\")\n",
    "                        print(f\"     Edges: {num_edges}\")\n",
    "                        print(f\"     'relab' node attribute found: {relab_found}\")\n",
    "\n",
    "                        # --- Plotting (ONLY for the FIRST graph successfully loaded) ---\n",
    "                        if not first_graph_plotted:\n",
    "                            print(f\"     Attempting to plot edge weights for this graph (as it's the first)...\")\n",
    "                            if num_edges > 0:\n",
    "                                # Extract finite edge weights safely\n",
    "                                edge_weights = []\n",
    "                                weight_key_found = False\n",
    "                                for u, v, data in G.edges(data=True):\n",
    "                                     if 'weight' in data:\n",
    "                                         weight_key_found = True # Confirm the key exists at least once\n",
    "                                         weight_val = data['weight']\n",
    "                                         # Check if it's a number and finite\n",
    "                                         if isinstance(weight_val, (int, float, np.number)) and np.isfinite(weight_val):\n",
    "                                             edge_weights.append(weight_val)\n",
    "\n",
    "                                if not weight_key_found:\n",
    "                                     print(\"     'weight' attribute key not found on any edges. Cannot plot.\")\n",
    "                                elif not edge_weights:\n",
    "                                    print(\"     Found 'weight' attribute, but no valid finite weights to plot.\")\n",
    "                                else:\n",
    "                                    # Create the plot\n",
    "                                    try:\n",
    "                                        print(f\"     Plotting distribution of {len(edge_weights)} edge weights...\")\n",
    "                                        plt.figure(figsize=(8, 5))\n",
    "                                        plt.hist(edge_weights, bins=30, color='lightcoral', edgecolor='black')\n",
    "                                        plt.title(f'Edge Weight Distribution (First Graph Found)\\nSample: {sample_id} ({graph_type})')\n",
    "                                        plt.xlabel('Edge Weight')\n",
    "                                        plt.ylabel('Frequency')\n",
    "                                        plt.tight_layout()\n",
    "                                        plt.show() # Display the plot inline in Jupyter\n",
    "                                        print(f\"     Plot displayed above.\")\n",
    "                                        first_graph_plotted = True # Set flag so we don't plot again\n",
    "                                    except Exception as plot_e:\n",
    "                                        print(f\"     ERROR during plotting: {plot_e}\")\n",
    "                                        # Still set flag to true to avoid repeated errors if plotting fails consistently\n",
    "                                        first_graph_plotted = True\n",
    "                            else:\n",
    "                                 print(\"     Graph has no edges, cannot plot weights.\")\n",
    "                                 # Mark plotting as \"done\" for the first graph attempt, even if empty\n",
    "                                 first_graph_plotted = True\n",
    "\n",
    "                    # Handle errors during file loading or processing\n",
    "                    except Exception as e:\n",
    "                        load_errors += 1\n",
    "                        print(f\"  ERROR loading or processing {pickle_path}: {e}\")\n",
    "\n",
    "                # If we've already plotted, we can break this inner loop to move to the next sample faster\n",
    "                if first_graph_plotted:\n",
    "                   break # Stop checking filtered if unfiltered was plotted in this sample dir\n",
    "\n",
    "        # If we only wanted to analyze until the first plot was done, uncomment the next lines\n",
    "        # if first_graph_plotted:\n",
    "        #    print(\"\\nFirst graph plotted. Stopping analysis of further samples.\")\n",
    "        #    break\n",
    "\n",
    "    # --- Summary ---\n",
    "    end_run_time = time.time()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if sample_dirs_found == 0:\n",
    "         print(\"Analysis Finished: No sample subdirectories found.\")\n",
    "    else:\n",
    "        print(f\"Analysis Finished: Checked {sample_dirs_found} sample directories.\")\n",
    "        print(f\"  Loaded and analyzed basic info for: {graphs_analyzed} graphs.\")\n",
    "        if load_errors > 0:\n",
    "             print(f\"  Encountered errors loading/processing: {load_errors} graphs.\")\n",
    "        if not first_graph_plotted and graphs_analyzed > 0 :\n",
    "             print(\"  NOTE: No edge weight plot generated (first graph had no edges/weights or failed plotting).\")\n",
    "        elif not first_graph_plotted and graphs_analyzed == 0:\n",
    "             print(\"  NOTE: No graphs were successfully loaded to generate a plot from.\")\n",
    "\n",
    "    print(f\"Total execution time: {end_run_time - start_run_time:.2f} seconds\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "# --- !!! SET THIS PATH !!! ---\n",
    "# Replace with the actual path to the directory holding all the sample folders\n",
    "graphs_dir = \"graph_outputs_combined_final/individual_graphs\" # <-- EXAMPLE PATH, CHANGE THIS\n",
    "# ---------------------------\n",
    "\n",
    "# Run the analysis function\n",
    "analyze_and_plot_first_graph(graphs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration Settings ---\n",
      "OTU Table Path: /Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_otu_table.csv\n",
      "Metadata Path: /Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_metadata.tsv\n",
      "Sample ID Column: 'Sample-ID'\n",
      "Condition ID Column: 'Group ID'\n",
      "Base Output Directory: 'microbiome_network_analysis_outputs'\n",
      "\n",
      "Workflow Selection:\n",
      "  Run Simple Individual Graphs: False (Threshold: 0.1)\n",
      "  Run Simple Aggregated Condition Graphs: False (Threshold: 0.1)\n",
      "  Run Bootstrap P-Value Filtering: True (Bootstraps: 100, P-Value: 0.05, Min Samples: 5)\n",
      "------------------------------\n",
      "\n",
      "--- Starting Data Processing and Analysis Workflow ---\n",
      "Script started at: 2025-04-19 19:43:06\n",
      "\n",
      "Output directory confirmed: '/Users/nandini.gadhia/Documents/projects/gp_omics/microbiome_network_analysis_outputs'\n",
      "\n",
      "[Step 1] Processing input data...\n",
      " Reading OTU table: /Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_otu_table.csv\n",
      " Reading metadata: /Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_metadata.tsv\n",
      "  OTU table shape: (20, 50)\n",
      "  Metadata shape before processing: (20, 2)\n",
      "  Metadata columns: ['Sample-ID', 'Group ID']\n",
      "  Using Sample ID column: 'Sample-ID'\n",
      "  Using Condition ID column: 'Group ID'\n",
      "  OTU table samples: 20, OTUs: 50\n",
      "  Metadata samples: 20\n",
      "  Found 20 common samples between OTU table and metadata.\n",
      "  Aligned feature table shape: (20, 50)\n",
      "  Aligned metadata shape: (20, 1)\n",
      "  Removed 1 OTUs that were all zero across samples.\n",
      "  Normalized OTU table to relative abundances. Final shape: (20, 49)\n",
      " Data processing function finished.\n",
      "[Step 1] Data processed successfully (0.01s).\n",
      "  Final feature table dimensions (Samples x Species): (20, 49)\n",
      "  Final metadata dimensions (Samples x Attributes): (20, 1)\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 2] Creating Simple Individual Sample Graphs... SKIPPED\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 3] Creating Simple Aggregated Condition-Level Graphs... SKIPPED\n",
      "--------------------------------------------------\n",
      "\n",
      "[Step 4] Running Bootstrap P-Value Filtering Workflow...\n",
      "\n",
      "--- Starting Bootstrap P-Value Filtering Workflow (2 conditions) ---\n",
      " Conditions based on: 'Group ID'\n",
      " Bootstraps per condition: 100\n",
      " P-value threshold for filtering: 0.05\n",
      " Min samples per condition: 5\n",
      " Outputs will be saved under: 'microbiome_network_analysis_outputs/bootstrap_pval_filtering'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Conditions (PVal Filter):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing Condition: Control ==========\n",
      " Condition 'Control' has 10 samples for analysis.\n",
      "Bootstrapping 100 replicates for condition: Control\n",
      " Bootstrap matrices will be saved in: microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Control/matrices\n",
      " Running 100 replicates in parallel (using joblib)...\n",
      " Calculating mean and std deviation across bootstrap matrices...\n",
      " Saved mean and std matrices to: microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Control\n",
      "\n",
      "Filtering individual samples for condition: Control (p < 0.05)\n",
      " Loaded 100 bootstrap matrices into memory.\n",
      "  Attempting to delete individual bootstrap matrix files from microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Control/matrices...\n",
      "  Successfully deleted 100 bootstrap matrix files.\n",
      " Processing 10 samples in condition Control...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Conditions (PVal Filter):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving summary and filtered OTU table for condition: Control\n",
      " Saved filtering summary: microbiome_network_analysis_outputs/bootstrap_pval_filtering/filtering_summary_Control.csv\n",
      " Filtered OTU table saved: microbiome_network_analysis_outputs/bootstrap_pval_filtering/filtered_otu_table_Control.csv\n",
      "\n",
      "========== Processing Condition: Treatment ==========\n",
      " Condition 'Treatment' has 10 samples for analysis.\n",
      "Bootstrapping 100 replicates for condition: Treatment\n",
      " Bootstrap matrices will be saved in: microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Treatment/matrices\n",
      " Running 100 replicates in parallel (using joblib)...\n",
      " Calculating mean and std deviation across bootstrap matrices...\n",
      " Saved mean and std matrices to: microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Treatment\n",
      "\n",
      "Filtering individual samples for condition: Treatment (p < 0.05)\n",
      " Loaded 100 bootstrap matrices into memory.\n",
      "  Attempting to delete individual bootstrap matrix files from microbiome_network_analysis_outputs/bootstrap_pval_filtering/bstrap_results_Treatment/matrices...\n",
      "  Successfully deleted 100 bootstrap matrix files.\n",
      " Processing 10 samples in condition Treatment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Conditions (PVal Filter): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving summary and filtered OTU table for condition: Treatment\n",
      " Saved filtering summary: microbiome_network_analysis_outputs/bootstrap_pval_filtering/filtering_summary_Treatment.csv\n",
      " Filtered OTU table saved: microbiome_network_analysis_outputs/bootstrap_pval_filtering/filtered_otu_table_Treatment.csv\n",
      "\n",
      "==================================================\n",
      "Bootstrap P-Value Filtering Workflow Finished.\n",
      " Conditions processed: 2\n",
      " Conditions skipped (due to sample size or errors): 0\n",
      "==================================================\n",
      "Bootstrap P-Value Filtering finished (5.84s).\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "--- All processing finished successfully ---\n",
      "Total execution time: 5.85 seconds\n",
      "Output saved in base directory: /Users/nandini.gadhia/Documents/projects/gp_omics/microbiome_network_analysis_outputs\n",
      "Check subdirectories for results from enabled analyses:\n",
      " - bootstrap_pval_filtering/\n",
      "==================================================\n",
      "\n",
      "Script finished at: 2025-04-19 19:43:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import random # Used in one version of bootstrapping logic provided\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# --- Plotting (optional, used by some analysis functions) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- User-provided Logic Dependencies ---\n",
    "from sklearn.preprocessing import normalize # For compute_all_weights\n",
    "from scipy import stats                   # For filtering_pvals_for_each_sample\n",
    "from joblib import Parallel, delayed      # For create_bootstrap_population\n",
    "\n",
    "# --- Progress Bar (optional but recommended) ---\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    print(\"Optional dependency 'tqdm' not found. Progress bars will be disabled.\")\n",
    "    # Define a dummy tqdm if not installed\n",
    "    def tqdm(iterable, *args, **kwargs):\n",
    "        return iterable\n",
    "\n",
    "# Optional: Suppress specific warnings if they become noisy\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # e.g., from divide by zero in numpy\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Processing Function (from earlier iterations)\n",
    "# -----------------------------------------------------------------------------\n",
    "def process_data(otu_file_path, metadata_file_path, distance_matrix_file_path=None,\n",
    "                 sample_id='Sample', condition_id='Study.Group', phylo=False):\n",
    "\n",
    "    \"\"\"Processes OTU table, metadata, and optionally a phylogenetic (or otherwise) distance matrix.\"\"\"\n",
    "\n",
    "    print(f\" Reading OTU table: {otu_file_path}\")\n",
    "    print(f\" Reading metadata: {metadata_file_path}\")\n",
    "\n",
    "    # Determine separators\n",
    "    sep_otu = '\\t' if otu_file_path.lower().endswith('.tsv') else ','\n",
    "    sep_meta = '\\t' if metadata_file_path.lower().endswith('.tsv') else ','\n",
    "\n",
    "    # Read files\n",
    "    try:\n",
    "        otu_table = pd.read_csv(otu_file_path, index_col=0, sep=sep_otu)\n",
    "        print(f\"  OTU table shape: {otu_table.shape}\")\n",
    "        metadata = pd.read_csv(metadata_file_path, sep=sep_meta)\n",
    "        print(f\"  Metadata shape before processing: {metadata.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error reading input files: {e}. Please check paths.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing input files: {e}\")\n",
    "\n",
    "    # --- Metadata Column Checks ---\n",
    "    original_metadata_cols = metadata.columns.tolist()\n",
    "    print(f\"  Metadata columns: {original_metadata_cols}\")\n",
    "\n",
    "    if sample_id not in metadata.columns:\n",
    "        print(f\"  Warning: Sample ID column '{sample_id}' not found directly. Checking if it's the index...\")\n",
    "        metadata_reset = metadata.reset_index()\n",
    "        if 'index' in metadata_reset.columns and sample_id == 'index':\n",
    "             print(f\"  Found sample ID '{sample_id}' as index.\")\n",
    "             metadata = metadata_reset\n",
    "        elif sample_id in metadata_reset.columns:\n",
    "            print(f\"  Found sample ID '{sample_id}' after resetting index.\")\n",
    "            metadata = metadata_reset\n",
    "        else:\n",
    "             original_metadata_cols_str = \", \".join(original_metadata_cols)\n",
    "             metadata_reset_cols_str = \", \".join(metadata_reset.columns.tolist())\n",
    "             raise ValueError(f\"Sample ID column '{sample_id}' not found in metadata. Original columns: [{original_metadata_cols_str}]. Columns after reset: [{metadata_reset_cols_str}]\")\n",
    "\n",
    "    if condition_id not in metadata.columns:\n",
    "        metadata_cols_str = \", \".join(metadata.columns.tolist())\n",
    "        raise ValueError(f\"Condition ID column '{condition_id}' not found in metadata. Available columns: [{metadata_cols_str}]\")\n",
    "\n",
    "    # --- Sample ID Handling ---\n",
    "    print(f\"  Using Sample ID column: '{sample_id}'\")\n",
    "    print(f\"  Using Condition ID column: '{condition_id}'\")\n",
    "\n",
    "    if not metadata[sample_id].is_unique:\n",
    "        num_duplicates = metadata[sample_id].duplicated().sum()\n",
    "        print(f\"  Warning: Sample ID column '{sample_id}' contains {num_duplicates} duplicate values. Keeping first occurrence of each.\")\n",
    "        metadata = metadata.drop_duplicates(subset=[sample_id], keep='first')\n",
    "        print(f\"  Metadata shape after dropping duplicates: {metadata.shape}\")\n",
    "\n",
    "    try:\n",
    "        metadata.set_index(sample_id, inplace=True)\n",
    "    except KeyError:\n",
    "         raise ValueError(f\"Failed to set index using Sample ID column '{sample_id}'. Column might be missing despite checks.\")\n",
    "\n",
    "    # --- Align OTU Table and Metadata ---\n",
    "    otu_table.index = otu_table.index.astype(str)\n",
    "    metadata.index = metadata.index.astype(str)\n",
    "    print(f\"  OTU table samples: {otu_table.shape[0]}, OTUs: {otu_table.shape[1]}\")\n",
    "    print(f\"  Metadata samples: {metadata.shape[0]}\")\n",
    "\n",
    "    common_samples = otu_table.index.intersection(metadata.index)\n",
    "    print(f\"  Found {len(common_samples)} common samples between OTU table and metadata.\")\n",
    "\n",
    "    if len(common_samples) == 0:\n",
    "        otu_samples_head = otu_table.index[:5].tolist()\n",
    "        meta_samples_head = metadata.index[:5].tolist()\n",
    "        raise ValueError(f\"No common samples found. Check Sample ID matching and formatting.\\n\"\n",
    "                         f\"  First 5 OTU index values: {otu_samples_head}\\n\"\n",
    "                         f\"  First 5 Metadata index values: {meta_samples_head}\")\n",
    "\n",
    "    otu_table = otu_table.loc[common_samples]\n",
    "    metadata = metadata.loc[common_samples]\n",
    "    print(f\"  Aligned feature table shape: {otu_table.shape}\")\n",
    "    print(f\"  Aligned metadata shape: {metadata.shape}\")\n",
    "\n",
    "    # --- OTU Table Processing ---\n",
    "    try:\n",
    "        otu_table = otu_table.astype(float)\n",
    "    except ValueError as e:\n",
    "        non_numeric_cols = otu_table.apply(lambda s: pd.to_numeric(s, errors='coerce').isna().any())\n",
    "        problem_cols = non_numeric_cols[non_numeric_cols].index.tolist()\n",
    "        raise ValueError(f\"OTU table contains non-numeric values. Problematic columns might include: {problem_cols}. Original error: {e}\")\n",
    "\n",
    "    initial_otus = otu_table.shape[1]\n",
    "    otu_table = otu_table.loc[:, (otu_table > 0).any(axis=0)] # Keep columns with at least one non-zero value\n",
    "    otus_removed = initial_otus - otu_table.shape[1]\n",
    "    if otus_removed > 0:\n",
    "        print(f\"  Removed {otus_removed} OTUs that were all zero across samples.\")\n",
    "\n",
    "    sample_sums = otu_table.sum(axis=1)\n",
    "    zero_sum_samples = sample_sums[sample_sums <= 1e-9].index # Use small threshold for float comparison\n",
    "    if not zero_sum_samples.empty:\n",
    "        print(f\"  Warning: Samples {zero_sum_samples.tolist()} have zero or near-zero total abundance. Removing these {len(zero_sum_samples)} samples before normalization.\")\n",
    "        otu_table = otu_table.drop(index=zero_sum_samples)\n",
    "        metadata = metadata.drop(index=zero_sum_samples)\n",
    "        sample_sums = sample_sums.drop(index=zero_sum_samples)\n",
    "\n",
    "    if otu_table.empty:\n",
    "        raise ValueError(\"OTU table is empty after filtering zero-sum samples. Cannot proceed.\")\n",
    "\n",
    "    # Perform safe division for normalization\n",
    "    otu_table = otu_table.div(sample_sums, axis=0)\n",
    "    otu_table = otu_table.fillna(0) # Replace any NaNs from division (shouldn't happen with check above)\n",
    "    print(f\"  Normalized OTU table to relative abundances. Final shape: {otu_table.shape}\")\n",
    "\n",
    "    # --- Grouping (Optional for return) ---\n",
    "    otu_table_grouped = None\n",
    "    if condition_id in metadata.columns:\n",
    "         try:\n",
    "            otu_table_grouped = otu_table.groupby(metadata[condition_id]).mean()\n",
    "         except Exception as group_e:\n",
    "             print(f\"  Warning: Could not group OTU table by condition '{condition_id}': {group_e}\")\n",
    "\n",
    "    # --- Phylogenetic Distances (Optional) ---\n",
    "    phylo_distances = None\n",
    "    if phylo and distance_matrix_file_path:\n",
    "        # (Phylo distance logic omitted for brevity, same as previous versions)\n",
    "        pass\n",
    "\n",
    "    print(\" Data processing function finished.\")\n",
    "    # Return only necessary items for subsequent steps documented here\n",
    "    return otu_table, metadata\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Functions Provided by User (Bootstrap P-Value Filtering Logic)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Compute sample weights for each sample\n",
    "def compute_sample_weights(sample_rel_otu):\n",
    "    \"\"\"Calculates pairwise weights within a single sample based on relative abundance.\"\"\"\n",
    "    # Ensure input is numpy array\n",
    "    sample_rel_otu = np.asarray(sample_rel_otu)\n",
    "    n_species = len(sample_rel_otu)\n",
    "\n",
    "    # Handle edge cases\n",
    "    if n_species < 2:\n",
    "        return np.zeros((n_species, n_species)), np.zeros((n_species, n_species))\n",
    "\n",
    "    binary_sample = (sample_rel_otu > 1e-9).astype(int) # Use threshold for float\n",
    "    sample_binary_matrix = np.outer(binary_sample, binary_sample) # Co-occurrence within sample\n",
    "    sample_matrix = np.tile(sample_rel_otu, (n_species, 1))\n",
    "\n",
    "    original_array = np.diag(sample_matrix)\n",
    "    # Use a small epsilon to prevent division by zero in edge cases\n",
    "    original_array_safe = original_array + np.finfo(float).eps\n",
    "    non_zero_mask = original_array > 1e-9\n",
    "\n",
    "    inverted_non_zero_elements = np.zeros_like(original_array, dtype=float)\n",
    "    inverted_non_zero_elements = np.divide(1.0, original_array_safe, where=non_zero_mask, out=inverted_non_zero_elements)\n",
    "    inv_diag = np.diag(inverted_non_zero_elements)\n",
    "\n",
    "    ratios = np.matmul(inv_diag, sample_matrix) # R = D^-1 * A\n",
    "\n",
    "    # Calculate weights W_ij based on reciprocal of ratios\n",
    "    weights = np.zeros_like(ratios)\n",
    "    non_diagonal_mask = ~np.eye(n_species, dtype=bool)\n",
    "    # Add epsilon to ratios denominator for safe division\n",
    "    ratios_safe = ratios + np.finfo(float).eps\n",
    "    valid_ratios_mask = (np.abs(ratios) > 1e-9) & non_diagonal_mask # Check if ratio is non-zero\n",
    "\n",
    "    weights = np.divide(2.0, ratios_safe, where=valid_ratios_mask, out=weights)\n",
    "\n",
    "    # Symmetrize using upper triangle\n",
    "    weights_new = np.triu(weights, k=1)\n",
    "    weights_new = weights_new + weights_new.T\n",
    "    weights_new[~np.isfinite(weights_new)] = 0 # Handle potential infinities/NaNs\n",
    "\n",
    "    return weights_new, sample_binary_matrix\n",
    "\n",
    "# Compute weights for all samples (Averaging logic)\n",
    "def compute_all_weights(raw_data):\n",
    "    \"\"\"Computes an average weight matrix across a set of samples.\"\"\"\n",
    "    raw_data = np.asarray(raw_data)\n",
    "    if raw_data.ndim != 2 or raw_data.shape[0] == 0 or raw_data.shape[1] == 0:\n",
    "         print(\"Warning: compute_all_weights received empty or invalid data.\")\n",
    "         if raw_data.ndim == 2 and raw_data.shape[1] > 0:\n",
    "             num_species = raw_data.shape[1]\n",
    "             return np.zeros((num_species, num_species)), np.zeros((num_species, num_species))\n",
    "         else:\n",
    "             raise ValueError(\"Cannot determine shape for empty/invalid input to compute_all_weights\")\n",
    "\n",
    "    # Normalize rows (samples) to relative abundance (L1 norm)\n",
    "    relative_raw = normalize(raw_data, axis=1, norm='l1')\n",
    "    num_samples, num_species = relative_raw.shape\n",
    "\n",
    "    combined_weights = np.zeros((num_species, num_species))\n",
    "    cooc_matrix = np.zeros((num_species, num_species)) # Sum of co-occurrences\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = relative_raw[i, :]\n",
    "        # Skip if sample has no valid data after normalization\n",
    "        if np.sum(sample) < 1e-9:\n",
    "            continue\n",
    "        w, cooc = compute_sample_weights(sample)\n",
    "        # Add only if shapes match (can happen if compute_sample_weights returns zeros)\n",
    "        if w.shape == combined_weights.shape:\n",
    "             combined_weights += w\n",
    "             cooc_matrix += cooc\n",
    "        else:\n",
    "            print(f\"Warning: Shape mismatch in compute_all_weights for sample {i}. Skipping.\")\n",
    "\n",
    "\n",
    "    # Average weights: Divide sum of weights by number of times species co-occurred\n",
    "    final_matrix = np.divide(combined_weights, cooc_matrix, where=cooc_matrix!=0, out=np.zeros_like(combined_weights))\n",
    "    final_matrix[~np.isfinite(final_matrix)] = 0 # Handle potential NaNs/Infs\n",
    "\n",
    "    return final_matrix, cooc_matrix\n",
    "\n",
    "# Create bootstrap population and calculate average weight matrices\n",
    "def create_bootstrap_population(observed_data_df, # Takes DataFrame for condition\n",
    "                                n=100,\n",
    "                                condition_group=\"Control\", # For output naming\n",
    "                                output_dir=\"bootstrap_analysis\"):\n",
    "    \"\"\"Generates bootstrap weight matrices for a given condition's data.\"\"\"\n",
    "    print(f\"Bootstrapping {n} replicates for condition: {condition_group}\")\n",
    "    directory = os.path.join(output_dir, f\"bstrap_results_{condition_group}\")\n",
    "    matrices_dir = os.path.join(directory, \"matrices\")\n",
    "    os.makedirs(matrices_dir, exist_ok=True)\n",
    "    print(f\" Bootstrap matrices will be saved in: {matrices_dir}\")\n",
    "\n",
    "    raw_data = observed_data_df.to_numpy()\n",
    "    n_samples, num_species = raw_data.shape\n",
    "    if n_samples == 0:\n",
    "        print(\" ERROR: No data provided for bootstrapping.\")\n",
    "        return None\n",
    "\n",
    "    # Create list of bootstrap datasets (resampling rows indices)\n",
    "    bootstrap_indices_list = [np.random.choice(n_samples, size=n_samples, replace=True) for _ in range(n)]\n",
    "    # Generate datasets based on indices\n",
    "    bstrap_otus_datasets = [raw_data[indices] for indices in bootstrap_indices_list]\n",
    "\n",
    "    # Function to process a single bootstrap replicate\n",
    "    def process_bootstrap_sample(b, otu_sample):\n",
    "        try:\n",
    "            # Compute the *average* weight matrix for this bootstrap replicate\n",
    "            w, _ = compute_all_weights(otu_sample)\n",
    "            # Save the matrix for this replicate\n",
    "            np.savetxt(os.path.join(matrices_dir, f\"bstrap_weight_matrix_{b}.csv\"), w, delimiter=\",\")\n",
    "            return w # Return the matrix\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing bootstrap replicate {b}: {e}\")\n",
    "            # Return matrix of zeros on error, with correct shape\n",
    "            return np.zeros((num_species, num_species))\n",
    "\n",
    "    # Run bootstrap processing in parallel\n",
    "    print(f\" Running {n} replicates in parallel (using joblib)...\")\n",
    "    results_matrices = Parallel(n_jobs=-1)(delayed(process_bootstrap_sample)(b, otu) for b, otu in enumerate(bstrap_otus_datasets))\n",
    "\n",
    "    # Filter out potential error results (e.g., None or incorrect shapes if added)\n",
    "    valid_results_matrices = [m for m in results_matrices if isinstance(m, np.ndarray) and m.shape == (num_species, num_species)]\n",
    "\n",
    "    if not valid_results_matrices or len(valid_results_matrices) < n * 0.5: # Check if at least half succeeded\n",
    "        print(f\" ERROR: Failed to generate sufficient valid bootstrap matrices ({len(valid_results_matrices)}/{n}). Check errors above.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate element-wise mean and std deviation across bootstrap matrices\n",
    "    print(\" Calculating mean and std deviation across bootstrap matrices...\")\n",
    "    try:\n",
    "        stacked_matrices = np.stack(valid_results_matrices, axis=0)\n",
    "        bstrap_means = np.mean(stacked_matrices, axis=0)\n",
    "        bstrap_stds = np.std(stacked_matrices, axis=0)\n",
    "\n",
    "        np.savetxt(os.path.join(directory, f\"_means_{condition_group}.csv\"), bstrap_means, delimiter=\",\")\n",
    "        np.savetxt(os.path.join(directory, f\"_stds_{condition_group}.csv\"), bstrap_stds, delimiter=\",\")\n",
    "        print(f\" Saved mean and std matrices to: {directory}\")\n",
    "        return directory # Return path to results for filtering step\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR calculating/saving mean/std matrices: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- MODIFIED Function to Filter Samples and Delete Matrices ---\n",
    "\n",
    "def filtering_pvals_for_each_sample(df_cond, # Feature table for condition\n",
    "                                    condition_group, # Name for output\n",
    "                                    bstrap_results_dir, # Directory with bootstrap matrices\n",
    "                                    thresh=0.05, # p-value threshold\n",
    "                                    output_dir=\"bootstrap_analysis\", # Base output directory\n",
    "                                    delete_bootstrap_matrices=True): # Flag to control deletion\n",
    "    \"\"\"\n",
    "    Filters each sample's graph based on comparison to bootstrap distributions\n",
    "    and optionally deletes the individual bootstrap matrix files after use.\n",
    "    \"\"\"\n",
    "    print(f\"\\nFiltering individual samples for condition: {condition_group} (p < {thresh})\")\n",
    "    matrices_dir = os.path.join(bstrap_results_dir, \"matrices\") # Path to saved bootstrap matrices\n",
    "    unfiltered_matrices_dir = os.path.join(output_dir, \"per_sample_unfiltered_matrices\", condition_group)\n",
    "    os.makedirs(unfiltered_matrices_dir, exist_ok=True)\n",
    "    filtered_matrices_dir = os.path.join(output_dir, \"per_sample_filtered_matrices\", condition_group)\n",
    "    os.makedirs(filtered_matrices_dir, exist_ok=True)\n",
    "    filtered_graphs_dir = os.path.join(output_dir, \"per_sample_filtered_graphs\", condition_group)\n",
    "    os.makedirs(filtered_graphs_dir, exist_ok=True)\n",
    "    # (Optional: print output directories)\n",
    "    # print(f\" Unfiltered sample matrices will be saved in: {unfiltered_matrices_dir}\")\n",
    "    # print(f\" Filtered sample matrices will be saved in: {filtered_matrices_dir}\")\n",
    "    # print(f\" Filtered sample graphs will be saved in: {filtered_graphs_dir}\")\n",
    "\n",
    "\n",
    "    # Load bootstrap results (individual weight matrices)\n",
    "    bs_data_3d = None # Initialize\n",
    "    try:\n",
    "        bootstrap_files = [f for f in os.listdir(matrices_dir) if f.startswith('bstrap_weight_matrix_') and f.endswith('.csv')]\n",
    "        if not bootstrap_files:\n",
    "            print(f\" ERROR: No bootstrap matrix files found in {matrices_dir}\")\n",
    "            return None, None # Indicate failure\n",
    "        # Load all matrices into a list\n",
    "        bootstrap_matrices = [np.loadtxt(os.path.join(matrices_dir, f), delimiter=\",\") for f in bootstrap_files]\n",
    "        # Filter potentially corrupted files\n",
    "        bootstrap_matrices = [m for m in bootstrap_matrices if isinstance(m, np.ndarray) and m.ndim == 2]\n",
    "        if not bootstrap_matrices:\n",
    "             print(f\" ERROR: Could not load any valid bootstrap matrices from {matrices_dir}\")\n",
    "             return None, None\n",
    "        bs_data_3d = np.stack(bootstrap_matrices, axis=0)\n",
    "        print(f\" Loaded {bs_data_3d.shape[0]} bootstrap matrices into memory.\")\n",
    "\n",
    "        # <<< --- ADDED: Delete individual bootstrap matrix files --- >>>\n",
    "        if delete_bootstrap_matrices:\n",
    "            print(f\"  Attempting to delete individual bootstrap matrix files from {matrices_dir}...\")\n",
    "            deletion_errors = 0\n",
    "            files_deleted_count = 0\n",
    "            try:\n",
    "                # Use the list of files we already identified\n",
    "                for f in bootstrap_files:\n",
    "                    file_path = os.path.join(matrices_dir, f)\n",
    "                    try:\n",
    "                        if os.path.exists(file_path): # Check if it still exists\n",
    "                            os.remove(file_path)\n",
    "                            files_deleted_count += 1\n",
    "                    except OSError as delete_e: # Catch specific OS errors like permission denied\n",
    "                        print(f\"    Warning: Could not delete file {f}: {delete_e}\")\n",
    "                        deletion_errors += 1\n",
    "                if deletion_errors == 0:\n",
    "                    print(f\"  Successfully deleted {files_deleted_count} bootstrap matrix files.\")\n",
    "                else:\n",
    "                    print(f\"  Finished deletion attempt: {files_deleted_count} deleted, {deletion_errors} errors.\")\n",
    "            except Exception as list_delete_e:\n",
    "                # Catch errors during listing/path joining if files were somehow modified\n",
    "                print(f\"  ERROR during deletion of bootstrap matrices: {list_delete_e}\")\n",
    "        else:\n",
    "             print(\"  Skipping deletion of individual bootstrap matrix files (delete_bootstrap_matrices=False).\")\n",
    "        # <<< --- END OF DELETION CODE --- >>>\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         print(f\" ERROR: Bootstrap matrices directory not found: {matrices_dir}\")\n",
    "         return None, None\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR loading bootstrap matrices: {e}\")\n",
    "        # If loading fails, bs_data_3d will be None, handled below\n",
    "        return None, None\n",
    "\n",
    "    # Check if loading succeeded before proceeding\n",
    "    if bs_data_3d is None:\n",
    "         print(f\" Cannot proceed with filtering for {condition_group} due to bootstrap loading errors.\")\n",
    "         return None, None\n",
    "\n",
    "    # Get original data for the condition as numpy array\n",
    "    data_cond = df_cond.to_numpy()\n",
    "    num_samples, num_species = data_cond.shape\n",
    "    species_names = df_cond.columns # Get species names\n",
    "\n",
    "    # Store bootstrap weights distribution for each edge (i, j)\n",
    "    bs_weight_distributions = defaultdict(list)\n",
    "    num_replicates = bs_data_3d.shape[0] # Use actual loaded count\n",
    "    # Populate the dictionary from the 3D array\n",
    "    for i in range(num_species):\n",
    "        for j in range(i + 1, num_species):\n",
    "            # Check if indices are within bounds of loaded data\n",
    "            if i < bs_data_3d.shape[1] and j < bs_data_3d.shape[2]:\n",
    "                 weights_for_edge = bs_data_3d[:, i, j]\n",
    "                 # Store only finite weights\n",
    "                 bs_weight_distributions[i, j] = weights_for_edge[np.isfinite(weights_for_edge)].tolist()\n",
    "\n",
    "\n",
    "    # --- Process each original sample in the condition ---\n",
    "    print(f\" Processing {num_samples} samples in condition {condition_group}...\")\n",
    "    filtering_summary_info = {} # Dictionary to store filtering summary per sample\n",
    "    filtered_otu_table_data = df_cond.copy() # Start with original relative abundances\n",
    "\n",
    "    sample_iterator = tqdm(range(num_samples), desc=\" Filtering Samples\", leave=False, ncols=100) if 'tqdm' in globals() else range(num_samples)\n",
    "\n",
    "    for counter in sample_iterator:\n",
    "        sample_data_rel = data_cond[counter, :] # Get relative abundances for sample\n",
    "        sample_name = df_cond.index[counter] # Get sample ID\n",
    "        safe_sample_name = str(sample_name).replace('/', '-').replace('\\\\', '-').replace(':', '-').replace(' ', '_') # Sanitize\n",
    "\n",
    "        # --- Calculate weights for the individual sample ---\n",
    "        sample_weights, _ = compute_sample_weights(sample_data_rel)\n",
    "\n",
    "        # Save unfiltered weights\n",
    "        unfiltered_matrix_path = os.path.join(unfiltered_matrices_dir, f\"unfiltered_weights_{safe_sample_name}.csv\")\n",
    "        np.savetxt(unfiltered_matrix_path, sample_weights, delimiter=\",\")\n",
    "\n",
    "        # --- Filter edges based on t-test against bootstrap distribution ---\n",
    "        filtered_sample_weights = sample_weights.copy() # Start with unfiltered weights\n",
    "        total_edges_in_sample = 0\n",
    "        edges_filtered_count = 0\n",
    "\n",
    "        for i in range(num_species):\n",
    "            for j in range(i + 1, num_species):\n",
    "                sample_w_ij = sample_weights[i, j]\n",
    "                # Consider only edges present (non-zero, finite) in the original sample's graph\n",
    "                if sample_w_ij != 0 and np.isfinite(sample_w_ij):\n",
    "                    total_edges_in_sample += 1\n",
    "\n",
    "                    # Get bootstrap distribution for this edge\n",
    "                    if (i, j) in bs_weight_distributions:\n",
    "                        bs_dist = bs_weight_distributions[i, j]\n",
    "                        # Perform t-test only if bootstrap distribution has variance\n",
    "                        if len(bs_dist) > 1 and np.std(bs_dist) > 1e-9: # Avoid division by zero / float issues\n",
    "                            with warnings.catch_warnings(): # Suppress potential warnings\n",
    "                                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                                try:\n",
    "                                    t_stat, p_val = stats.ttest_1samp(\n",
    "                                        a=bs_dist,\n",
    "                                        popmean=sample_w_ij,\n",
    "                                        alternative='two-sided',\n",
    "                                        nan_policy='omit'\n",
    "                                    )\n",
    "                                     # Filter if p-value is BELOW threshold (significantly different)\n",
    "                                    if p_val < thresh:\n",
    "                                        filtered_sample_weights[i, j] = filtered_sample_weights[j, i] = 0\n",
    "                                        edges_filtered_count += 1\n",
    "                                except ValueError as ttest_e:\n",
    "                                     # Handle cases where t-test fails unexpectedly (e.g., all values equal popmean)\n",
    "                                     # print(f\"  T-test warning for edge ({i},{j}) in sample {sample_name}: {ttest_e}. Keeping edge.\")\n",
    "                                     pass # Keep the edge if test fails\n",
    "\n",
    "\n",
    "        # --- Save filtered results and collect summary ---\n",
    "        filtering_summary_info[sample_name] = {\n",
    "            \"nodes_in\": np.count_nonzero(sample_data_rel > 1e-9),\n",
    "            \"edges_in\": total_edges_in_sample,\n",
    "            \"edges_filtered_out\": edges_filtered_count,\n",
    "            \"prop_filtered\": (edges_filtered_count / total_edges_in_sample if total_edges_in_sample > 0 else 0)\n",
    "        }\n",
    "\n",
    "        filtered_matrix_path = os.path.join(filtered_matrices_dir, f\"filtered_weights_{safe_sample_name}.csv\")\n",
    "        np.savetxt(filtered_matrix_path, filtered_sample_weights, delimiter=\",\")\n",
    "\n",
    "        # --- Create and Save Filtered Graph ---\n",
    "        G_filtered = nx.Graph()\n",
    "        present_species_indices = np.where(sample_data_rel > 1e-9)[0]\n",
    "        nodes_added_to_graph = set()\n",
    "        for idx in present_species_indices:\n",
    "             node_name = species_names[idx]\n",
    "             relab = sample_data_rel[idx]\n",
    "             G_filtered.add_node(node_name, relab=relab)\n",
    "             nodes_added_to_graph.add(node_name)\n",
    "\n",
    "        for i in range(num_species):\n",
    "            for j in range(i + 1, num_species):\n",
    "                if filtered_sample_weights[i, j] != 0:\n",
    "                    node_i = species_names[i]; node_j = species_names[j]\n",
    "                    if node_i in nodes_added_to_graph and node_j in nodes_added_to_graph:\n",
    "                        G_filtered.add_edge(node_i, node_j, weight=filtered_sample_weights[i, j])\n",
    "\n",
    "        filtering_summary_info[sample_name][\"nodes_out\"] = G_filtered.number_of_nodes()\n",
    "        filtered_graph_path = os.path.join(filtered_graphs_dir, f\"filtered_graph_{safe_sample_name}.graphml\")\n",
    "        try:\n",
    "            nx.write_graphml(G_filtered, filtered_graph_path)\n",
    "        except Exception as graph_e:\n",
    "             print(f\"  ERROR saving filtered graph for sample {sample_name}: {graph_e}\")\n",
    "\n",
    "        # --- Update the filtered OTU table ---\n",
    "        remaining_nodes = list(G_filtered.nodes())\n",
    "        cols_to_zero_out = df_cond.columns[~df_cond.columns.isin(remaining_nodes)]\n",
    "        filtered_otu_table_data.loc[sample_name, cols_to_zero_out] = 0\n",
    "\n",
    "    # --- Save summary and filtered OTU table for the condition ---\n",
    "    print(f\"\\nSaving summary and filtered OTU table for condition: {condition_group}\")\n",
    "    summary_df = pd.DataFrame.from_dict(filtering_summary_info, orient=\"index\")\n",
    "    summary_csv_path = os.path.join(output_dir, f\"filtering_summary_{condition_group}.csv\")\n",
    "    try: summary_df.to_csv(summary_csv_path); print(f\" Saved filtering summary: {summary_csv_path}\")\n",
    "    except Exception as summary_e: print(f\" ERROR saving filtering summary CSV: {summary_e}\")\n",
    "\n",
    "    filtered_otu_table_path = os.path.join(output_dir, f\"filtered_otu_table_{condition_group}.csv\")\n",
    "    try: filtered_otu_table_data.to_csv(filtered_otu_table_path); print(f\" Filtered OTU table saved: {filtered_otu_table_path}\")\n",
    "    except Exception as otu_e: print(f\" ERROR saving filtered OTU table CSV: {otu_e}\")\n",
    "\n",
    "    return filtered_otu_table_data, summary_df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Orchestrating Function for Bootstrap P-Value Filtering\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_bootstrap_filtering_per_condition(feature_table,\n",
    "                                          metadata,\n",
    "                                          condition_id_col,\n",
    "                                          output_dir,\n",
    "                                          num_bootstraps=100,\n",
    "                                          p_value_threshold=0.1,\n",
    "                                          min_samples_bootstrap=5):\n",
    "    \"\"\"\n",
    "    Orchestrates bootstrapping and p-value filtering for each condition.\n",
    "    Uses the user-provided functions.\n",
    "    \"\"\"\n",
    "    all_conditions = metadata[condition_id_col].unique()\n",
    "    print(f\"\\n--- Starting Bootstrap P-Value Filtering Workflow ({len(all_conditions)} conditions) ---\")\n",
    "    print(f\" Conditions based on: '{condition_id_col}'\")\n",
    "    print(f\" Bootstraps per condition: {num_bootstraps}\")\n",
    "    print(f\" P-value threshold for filtering: {p_value_threshold}\")\n",
    "    print(f\" Min samples per condition: {min_samples_bootstrap}\")\n",
    "\n",
    "    # Base directory for all outputs of this specific workflow\n",
    "    bootstrap_output_base_dir = os.path.join(output_dir, \"bootstrap_pval_filtering\")\n",
    "    os.makedirs(bootstrap_output_base_dir, exist_ok=True)\n",
    "    print(f\" Outputs will be saved under: '{bootstrap_output_base_dir}'\")\n",
    "\n",
    "    all_filtered_otus = {}\n",
    "    all_summaries = {}\n",
    "    conditions_processed_count = 0\n",
    "    conditions_skipped_count = 0\n",
    "\n",
    "    condition_iterator = tqdm(all_conditions, desc=\"Processing Conditions (PVal Filter)\", leave=True) if 'tqdm' in globals() else all_conditions\n",
    "\n",
    "    for condition_group in condition_iterator:\n",
    "        safe_condition_name = str(condition_group).replace(' ', '_').replace('/', '-').replace('\\\\', '-')\n",
    "        print(f\"\\n{'='*10} Processing Condition: {condition_group} {'='*10}\")\n",
    "\n",
    "        condition_sample_ids = metadata[metadata[condition_id_col] == condition_group].index\n",
    "        valid_condition_sample_ids = condition_sample_ids.intersection(feature_table.index)\n",
    "\n",
    "        if len(valid_condition_sample_ids) < min_samples_bootstrap:\n",
    "            print(f\" Skipping condition '{condition_group}': Only {len(valid_condition_sample_ids)} valid samples found, need at least {min_samples_bootstrap}.\")\n",
    "            conditions_skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        df_cond = feature_table.loc[valid_condition_sample_ids]\n",
    "        print(f\" Condition '{condition_group}' has {len(df_cond)} samples for analysis.\")\n",
    "\n",
    "        # 1. Run bootstrapping to generate reference weight distributions\n",
    "        bstrap_results_dir = create_bootstrap_population(\n",
    "            observed_data_df=df_cond,\n",
    "            n=num_bootstraps,\n",
    "            condition_group=safe_condition_name,\n",
    "            output_dir=bootstrap_output_base_dir\n",
    "        )\n",
    "\n",
    "        if bstrap_results_dir is None:\n",
    "             print(f\" ERROR: Bootstrapping failed for condition '{condition_group}'. Skipping filtering.\")\n",
    "             conditions_skipped_count += 1\n",
    "             continue\n",
    "\n",
    "        # 2. Run filtering for each sample within the condition\n",
    "        filtered_otu_table, summary_df = filtering_pvals_for_each_sample(\n",
    "            df_cond=df_cond, # Pass the condition's feature table (relative abundances)\n",
    "            condition_group=safe_condition_name,\n",
    "            bstrap_results_dir=bstrap_results_dir,\n",
    "            thresh=p_value_threshold,\n",
    "            output_dir=bootstrap_output_base_dir\n",
    "        )\n",
    "\n",
    "        # Store results if successful\n",
    "        if filtered_otu_table is not None:\n",
    "             all_filtered_otus[condition_group] = filtered_otu_table\n",
    "        if summary_df is not None:\n",
    "             all_summaries[condition_group] = summary_df\n",
    "        conditions_processed_count += 1\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Bootstrap P-Value Filtering Workflow Finished.\")\n",
    "    print(f\" Conditions processed: {conditions_processed_count}\")\n",
    "    print(f\" Conditions skipped (due to sample size or errors): {conditions_skipped_count}\")\n",
    "    print(\"=\"*50)\n",
    "    # Optionally return collected results for further use\n",
    "    return all_filtered_otus, all_summaries\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optional: Functions for other graph types (Individual, Aggregated Condition)\n",
    "# (Copied from previous versions for completeness, can be gated by flags)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# --- Function to create graph for a single sample ---\n",
    "def create_individual_sample_graph(sample_data_series, threshold=None):\n",
    "    \"\"\"Creates a NetworkX graph for a single sample's abundance data.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    numeric_sample_data = pd.to_numeric(sample_data_series, errors='coerce').fillna(0)\n",
    "    nonzero_species = numeric_sample_data[numeric_sample_data > 1e-9] # Use threshold\n",
    "\n",
    "    if nonzero_species.empty: return G\n",
    "    elif len(nonzero_species) == 1:\n",
    "        species, abundance = nonzero_species.index[0], nonzero_species.iloc[0]\n",
    "        G.add_node(species, relab=abundance)\n",
    "        return G\n",
    "\n",
    "    for species, abundance in nonzero_species.items(): G.add_node(species, relab=abundance)\n",
    "\n",
    "    species_list = nonzero_species.index.tolist()\n",
    "    abundances_array = nonzero_species.to_numpy()\n",
    "    weights_matrix, _ = compute_sample_weights(abundances_array)\n",
    "\n",
    "    for i in range(len(species_list)):\n",
    "        for j in range(i + 1, len(species_list)):\n",
    "            weight = weights_matrix[i, j]\n",
    "            if np.isfinite(weight) and weight != 0: # Check non-zero finite weight\n",
    "                if threshold is None or threshold <= 0: # Unfiltered\n",
    "                    G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "                elif abs(weight) > threshold: # Filtered\n",
    "                    G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "    return G\n",
    "\n",
    "# --- Function to loop through samples and save individual graphs ---\n",
    "def create_and_save_individual_graphs(feature_table, base_output_dir, edge_threshold):\n",
    "    \"\"\"Creates and saves filtered and unfiltered graphs for each individual sample.\"\"\"\n",
    "    print(f\"\\n--- Starting Simple INDIVIDUAL Sample Graph Generation ---\")\n",
    "    individual_dir = os.path.join(base_output_dir, \"individual_graphs_simple\") # Changed dir name\n",
    "    os.makedirs(individual_dir, exist_ok=True)\n",
    "    print(f\" Individual graphs will be saved in: '{individual_dir}'\")\n",
    "    num_samples = len(feature_table)\n",
    "    start_time = time.time()\n",
    "    samples_processed = 0\n",
    "    errors_encountered = 0\n",
    "\n",
    "    sample_iterator = tqdm(feature_table.iterrows(), total=num_samples, desc=\" Individual Graphs\", leave=False, ncols=100) if 'tqdm' in globals() else feature_table.iterrows()\n",
    "\n",
    "    for i, (sample_id, sample_data) in enumerate(sample_iterator):\n",
    "        safe_sample_id = str(sample_id).replace('/', '-').replace('\\\\', '-').replace(':', '-').replace(' ', '_')\n",
    "        sample_output_dir = os.path.join(individual_dir, safe_sample_id)\n",
    "        os.makedirs(sample_output_dir, exist_ok=True)\n",
    "        try:\n",
    "            G_unfiltered = create_individual_sample_graph(sample_data, threshold=None)\n",
    "            if G_unfiltered.number_of_nodes() > 0:\n",
    "                unfiltered_path = os.path.join(sample_output_dir, \"graph_unfiltered.pkl\")\n",
    "                with open(unfiltered_path, 'wb') as f: pickle.dump(G_unfiltered, f)\n",
    "\n",
    "            G_filtered = create_individual_sample_graph(sample_data, threshold=edge_threshold)\n",
    "            if G_filtered.number_of_edges() > 0:\n",
    "                 filtered_path = os.path.join(sample_output_dir, \"graph_filtered.pkl\")\n",
    "                 with open(filtered_path, 'wb') as f: pickle.dump(G_filtered, f)\n",
    "            samples_processed += 1\n",
    "        except Exception as e:\n",
    "            errors_encountered += 1\n",
    "            print(f\"\\n ERROR processing individual graph for sample {sample_id}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"--- Finished Simple INDIVIDUAL Sample Graph Generation ({end_time - start_time:.2f}s) ---\")\n",
    "    print(f\"  Samples processed: {samples_processed}, Errors: {errors_encountered}\")\n",
    "\n",
    "# --- Functions for Simple Aggregated Condition Graphs ---\n",
    "def aggregate_samples_for_condition(feature_table, metadata, condition_group, condition_id_col):\n",
    "    \"\"\"Aggregate samples by averaging for each condition.\"\"\"\n",
    "    samples_in_condition = metadata[metadata[condition_id_col] == condition_group].index\n",
    "    valid_samples = samples_in_condition.intersection(feature_table.index)\n",
    "    if valid_samples.empty: return pd.Series(dtype=float), pd.Index([])\n",
    "    condition_feature_table = feature_table.loc[valid_samples]\n",
    "    if condition_feature_table.empty: return pd.Series(dtype=float), valid_samples\n",
    "    aggregated_feature_table = condition_feature_table.mean(axis=0).fillna(0)\n",
    "    return aggregated_feature_table, valid_samples\n",
    "\n",
    "def create_graph_from_aggregated_data(aggregated_data, threshold):\n",
    "    \"\"\"Create a graph based on aggregated data and edge weight threshold.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    nonzero_species = aggregated_data[aggregated_data > 1e-9]\n",
    "    if nonzero_species.empty: return G\n",
    "    elif len(nonzero_species) == 1:\n",
    "        sp, ab = nonzero_species.index[0], nonzero_species.iloc[0]; G.add_node(sp, relab=ab); return G\n",
    "    for species, abundance in nonzero_species.items(): G.add_node(species, relab=abundance)\n",
    "    species_list = nonzero_species.index.tolist(); abundance_array = nonzero_species.to_numpy()\n",
    "    weights_matrix, _ = compute_sample_weights(abundance_array)\n",
    "    for i in range(len(species_list)):\n",
    "        for j in range(i + 1, len(species_list)):\n",
    "            weight = weights_matrix[i, j]\n",
    "            if np.isfinite(weight) and abs(weight) > threshold:\n",
    "                G.add_edge(species_list[i], species_list[j], weight=weight)\n",
    "    return G\n",
    "\n",
    "def filter_feature_table(feature_table, graph_nodes):\n",
    "    \"\"\"Filter feature table columns to include only graph nodes.\"\"\"\n",
    "    valid_nodes = [node for node in graph_nodes if node in feature_table.columns]\n",
    "    if not valid_nodes: return pd.DataFrame(index=feature_table.index)\n",
    "    return feature_table[valid_nodes]\n",
    "\n",
    "def create_simple_condition_graphs(feature_table, metadata, output_dir, condition_id_col, edge_threshold):\n",
    "    \"\"\"Creates and saves simple AGGREGATED graphs for each condition.\"\"\"\n",
    "    print(f\"\\n--- Starting Simple AGGREGATED Condition Graph Generation ---\")\n",
    "    base_condition_dir = os.path.join(output_dir, \"condition_graphs_aggregated\")\n",
    "    os.makedirs(base_condition_dir, exist_ok=True)\n",
    "    print(f\" Aggregated condition graphs will be saved in: '{base_condition_dir}'\")\n",
    "    conditions = metadata[condition_id_col].unique()\n",
    "    cond_iterator = tqdm(conditions, desc=\"Aggregated Conditions\", leave=False) if 'tqdm' in globals() else conditions\n",
    "    for group in cond_iterator:\n",
    "        safe_name = str(group).replace(' ', '_').replace('/', '-').replace('\\\\', '-')\n",
    "        cond_out_dir = os.path.join(base_condition_dir, safe_name); os.makedirs(cond_out_dir, exist_ok=True)\n",
    "        agg_data, samples = aggregate_samples_for_condition(feature_table, metadata, group, condition_id_col)\n",
    "        if agg_data.empty: continue\n",
    "        G_agg = create_graph_from_aggregated_data(agg_data, threshold=edge_threshold)\n",
    "        if G_agg.number_of_nodes() == 0: continue\n",
    "        try:\n",
    "            pkl_path = os.path.join(cond_out_dir, f\"graph_agg_{safe_name}.pkl\")\n",
    "            gml_path = os.path.join(cond_out_dir, f\"graph_agg_{safe_name}.graphml\")\n",
    "            with open(pkl_path, 'wb') as f: pickle.dump(G_agg, f)\n",
    "            nx.write_graphml(G_agg, gml_path)\n",
    "        except Exception as e: print(f\" Error saving aggregated graph for {group}: {e}\")\n",
    "        # Optional: Save filtered table based on this aggregated graph\n",
    "        try:\n",
    "            nodes = list(G_agg.nodes()); valid_samples = samples.intersection(feature_table.index)\n",
    "            if not valid_samples.empty:\n",
    "                ft_filt = filter_feature_table(feature_table.loc[valid_samples], nodes)\n",
    "                if not ft_filt.empty:\n",
    "                    ft_filt_path = os.path.join(cond_out_dir, f\"ft_agg_filt_{safe_name}.csv\")\n",
    "                    ft_filt.to_csv(ft_filt_path)\n",
    "        except Exception as e: print(f\" Error saving filtered table for aggregated {group}: {e}\")\n",
    "    print(f\"--- Finished Simple AGGREGATED Condition Graph Generation ---\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Execution Block\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Configuration ---\n",
    "    print(\"--- Configuration Settings ---\")\n",
    "    # File paths (UPDATE THESE)\n",
    "    otu_file_path = \"/Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_otu_table.csv\"\n",
    "    metadata_file_path = \"/Users/nandini.gadhia/Documents/projects/gp_omics/data/toy_metadata.tsv\"\n",
    "\n",
    "    # Column names (UPDATE THESE)\n",
    "    sample_id_col = 'Sample-ID'     # Column name for sample IDs in metadata\n",
    "    condition_id_col = 'Group ID'   # Column name for the condition/group in metadata\n",
    "\n",
    "    # Output directory (UPDATE IF NEEDED)\n",
    "    output_dir = \"microbiome_network_analysis_outputs\" # Base directory for all outputs\n",
    "\n",
    "    # --- Analysis Workflow Flags ---\n",
    "    # Set these to True or False to enable/disable specific analyses\n",
    "    run_simple_individual_graphs = False    # Generate basic filtered/unfiltered graphs per sample?\n",
    "    run_simple_aggregated_graphs = False    # Generate graphs from simple condition averages?\n",
    "    run_bootstrap_pval_filtering = True     # Run the user-provided bootstrap p-value filtering?\n",
    "\n",
    "    # --- Parameters for Analyses ---\n",
    "    # Edge weight threshold for SIMPLE filtered graphs (individual & aggregated)\n",
    "    simple_graph_edge_threshold = 0.1\n",
    "\n",
    "    # Parameters for Bootstrap P-Value Filtering\n",
    "    num_bootstraps_pval = 100     # Number of bootstrap replicates\n",
    "    pval_threshold = 0.05         # p-value cutoff for filtering sample edges\n",
    "    min_samples_pval = 5          # Min samples per condition required for this workflow\n",
    "\n",
    "    print(f\"OTU Table Path: {otu_file_path}\")\n",
    "    print(f\"Metadata Path: {metadata_file_path}\")\n",
    "    print(f\"Sample ID Column: '{sample_id_col}'\")\n",
    "    print(f\"Condition ID Column: '{condition_id_col}'\")\n",
    "    print(f\"Base Output Directory: '{output_dir}'\")\n",
    "    print(f\"\\nWorkflow Selection:\")\n",
    "    print(f\"  Run Simple Individual Graphs: {run_simple_individual_graphs} (Threshold: {simple_graph_edge_threshold})\")\n",
    "    print(f\"  Run Simple Aggregated Condition Graphs: {run_simple_aggregated_graphs} (Threshold: {simple_graph_edge_threshold})\")\n",
    "    print(f\"  Run Bootstrap P-Value Filtering: {run_bootstrap_pval_filtering} (Bootstraps: {num_bootstraps_pval}, P-Value: {pval_threshold}, Min Samples: {min_samples_pval})\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Start Workflow ---\n",
    "    print(\"\\n--- Starting Data Processing and Analysis Workflow ---\")\n",
    "    print(f\"Script started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Ensure base output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nOutput directory confirmed: '{os.path.abspath(output_dir)}'\")\n",
    "\n",
    "        # 1. Process Data\n",
    "        print(\"\\n[Step 1] Processing input data...\")\n",
    "        data_start_time = time.time()\n",
    "        # Ensure process_data returns only feature_table and metadata if others aren't needed\n",
    "        feature_table, metadata = process_data(\n",
    "            otu_file_path, metadata_file_path,\n",
    "            sample_id=sample_id_col,\n",
    "            condition_id=condition_id_col\n",
    "        )[:2] # Take only first two return values\n",
    "        print(f\"[Step 1] Data processed successfully ({time.time() - data_start_time:.2f}s).\")\n",
    "        print(f\"  Final feature table dimensions (Samples x Species): {feature_table.shape}\")\n",
    "        print(f\"  Final metadata dimensions (Samples x Attributes): {metadata.shape}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        # 2. Create and Save Simple Individual Sample Graphs (Optional)\n",
    "        if run_simple_individual_graphs:\n",
    "            print(\"\\n[Step 2] Creating Simple Individual Sample Graphs...\")\n",
    "            create_and_save_individual_graphs(\n",
    "                feature_table,\n",
    "                output_dir,\n",
    "                edge_threshold=simple_graph_edge_threshold\n",
    "            )\n",
    "            print(\"[Step 2] Simple individual graph generation finished.\")\n",
    "            print(\"-\"*50)\n",
    "        else:\n",
    "             print(\"\\n[Step 2] Creating Simple Individual Sample Graphs... SKIPPED\")\n",
    "             print(\"-\"*50)\n",
    "\n",
    "\n",
    "        # 3. Create and Save Simple Aggregated Condition-Level Graphs (Optional)\n",
    "        if run_simple_aggregated_graphs:\n",
    "            print(\"\\n[Step 3] Creating Simple Aggregated Condition-Level Graphs...\")\n",
    "            create_simple_condition_graphs(\n",
    "                feature_table,\n",
    "                metadata,\n",
    "                output_dir,\n",
    "                condition_id_col=condition_id_col,\n",
    "                edge_threshold=simple_graph_edge_threshold\n",
    "            )\n",
    "            print(\"[Step 3] Simple aggregated condition graph generation finished.\")\n",
    "            print(\"-\"*50)\n",
    "        else:\n",
    "             print(\"\\n[Step 3] Creating Simple Aggregated Condition-Level Graphs... SKIPPED\")\n",
    "             print(\"-\"*50)\n",
    "\n",
    "\n",
    "        # 4. Run Bootstrap P-Value Filtering (User-Provided Logic, Optional)\n",
    "        if run_bootstrap_pval_filtering:\n",
    "            print(\"\\n[Step 4] Running Bootstrap P-Value Filtering Workflow...\")\n",
    "            pval_filter_start_time = time.time()\n",
    "            # This function handles looping through conditions internally\n",
    "            filtered_otu_tables_by_cond, filter_summaries_by_cond = run_bootstrap_filtering_per_condition(\n",
    "                feature_table=feature_table, # Pass the main processed feature table\n",
    "                metadata=metadata,\n",
    "                condition_id_col=condition_id_col,\n",
    "                output_dir=output_dir, # Base output directory\n",
    "                num_bootstraps=num_bootstraps_pval,\n",
    "                p_value_threshold=pval_threshold,\n",
    "                min_samples_bootstrap=min_samples_pval\n",
    "            )\n",
    "            print(f\"Bootstrap P-Value Filtering finished ({time.time() - pval_filter_start_time:.2f}s).\")\n",
    "            # Example: Access results if needed later in the script\n",
    "            # if 'ConditionA' in filtered_otu_tables_by_cond:\n",
    "            #     print(\"Filtered OTU table for ConditionA:\", filtered_otu_tables_by_cond['ConditionA'].shape)\n",
    "            print(\"-\"*50)\n",
    "        else:\n",
    "             print(\"\\n[Step 4] Bootstrap P-Value Filtering Workflow... SKIPPED\")\n",
    "             print(\"-\"*50)\n",
    "\n",
    "\n",
    "        # --- End Workflow ---\n",
    "        overall_end_time = time.time()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"--- All processing finished successfully ---\")\n",
    "        print(f\"Total execution time: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "        print(f\"Output saved in base directory: {os.path.abspath(output_dir)}\")\n",
    "        print(\"Check subdirectories for results from enabled analyses:\")\n",
    "        if run_simple_individual_graphs: print(\" - individual_graphs_simple/\")\n",
    "        if run_simple_aggregated_graphs: print(\" - condition_graphs_aggregated/\")\n",
    "        if run_bootstrap_pval_filtering: print(\" - bootstrap_pval_filtering/\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    # --- Error Handling ---\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n--- FATAL ERROR: Input file not found ---\", flush=True)\n",
    "        print(e); print(\"Please check paths in the Configuration section.\", flush=True)\n",
    "    except ValueError as ve:\n",
    "        print(f\"\\n--- FATAL ERROR: Data validation or processing error ---\", flush=True)\n",
    "        print(ve); print(\"Check input file formats and column names.\", flush=True)\n",
    "    except KeyError as ke:\n",
    "        print(f\"\\n--- FATAL ERROR: Column not found ---\", flush=True)\n",
    "        print(f\"Column '{ke}' not found. Check `sample_id_col`/`condition_id_col` config.\", flush=True)\n",
    "    except MemoryError:\n",
    "         print(f\"\\n--- FATAL ERROR: Insufficient memory ---\", flush=True)\n",
    "         print(\"Consider using a machine with more RAM or processing smaller datasets.\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected FATAL error occurred ---\", flush=True)\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "\n",
    "    finally:\n",
    "        print(f\"\\nScript finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

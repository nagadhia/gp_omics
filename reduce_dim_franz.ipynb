{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction in various ways to lower the number of nodes into one that works for Gaussian processes\n",
    "- UMAPs / TSNE\n",
    "- GP latent variable model https://pyro.ai/examples/gplvm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.1 or less. Got NumPy 2.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/umap/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/umap/umap_.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/numba/__init__.py:59\u001b[0m\n\u001b[1;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/numba/__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.1 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 2.1 or less. Got NumPy 2.2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/nandini.gadhia/Documents/projects/gp_omics/data_rvc/OTU_table.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the first column is an identifier and rest are features\n",
    "if df.shape[1] > 1:\n",
    "    features = df.iloc[:, 1:]\n",
    "else:\n",
    "    raise ValueError(\"Dataset does not have enough columns for dimensionality reduction.\")\n",
    "\n",
    "# Fit UMAP with a large number of components\n",
    "initial_n_components = min(20, features.shape[1])\n",
    "umap_reducer = umap.UMAP(n_components=initial_n_components, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(features)\n",
    "\n",
    "# Determine the number of components that preserve 90% variance\n",
    "cumulative_variance = np.cumsum(np.var(umap_result, axis=0)) / np.sum(np.var(umap_result, axis=0))\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# Re-run UMAP with the optimal number of components\n",
    "umap_reducer = umap.UMAP(n_components=80, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(features)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "umap_columns = [f'UMAP_{i+1}' for i in range(n_components_90)]\n",
    "umap_df = pd.DataFrame(umap_result)\n",
    "\n",
    "\n",
    "# Save the reduced data\n",
    "umap_output_path = '/Users/nandini.gadhia/Documents/projects/gp_omics/data_rvc/OTU_table_umap.csv'\n",
    "\n",
    "umap_df.to_csv(umap_output_path, index=False)\n",
    "\n",
    "print(f\"UMAP reduced data with {n_components_90} components saved to {umap_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded and Processed Successfully.\n",
      "Feature matrix shape (features): (200, 100)\n",
      "Labels array shape (labels_binary): (200,)\n",
      "\n",
      "Data Loaded and Processed Successfully.\n",
      "Feature matrix shape (features): (200, 100)\n",
      "Labels array shape (labels_binary): (200,)\n",
      "Iter 10/200 - Loss: 141.674\n",
      "Iter 20/200 - Loss: 141.770\n",
      "Iter 30/200 - Loss: 141.695\n",
      "Iter 40/200 - Loss: 141.643\n",
      "Iter 50/200 - Loss: 141.651\n",
      "Iter 60/200 - Loss: 141.643\n",
      "Iter 70/200 - Loss: 141.644\n",
      "Iter 80/200 - Loss: 141.643\n",
      "Iter 90/200 - Loss: 141.643\n",
      "Iter 100/200 - Loss: 141.643\n",
      "Iter 110/200 - Loss: 141.643\n",
      "Iter 120/200 - Loss: 141.643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_train_data(inputs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mX, targets\u001b[38;5;241m=\u001b[39mtargets[j], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(model\u001b[38;5;241m.\u001b[39mX)\n\u001b[0;32m--> 128\u001b[0m     log_likelihood_loss \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#log_prior_loss = -student_t_prior.log_prob(model.X).sum()\u001b[39;00m\n\u001b[1;32m    131\u001b[0m log_prior_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlaplace_prior\u001b[38;5;241m.\u001b[39mlog_prob(model\u001b[38;5;241m.\u001b[39mX)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/module.py:82\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:66\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactMarginalLogLikelihood can only operate on Gaussian random variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Determine output likelihood\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Remove NaN values if enabled\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/likelihood.py:76\u001b[0m, in \u001b[0;36m_Likelihood.__call__\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Marginal\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, MultivariateNormal):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Error\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLikelihoods expects a MultivariateNormal input to make marginal predictions, or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor for conditional predictions. Got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:177\u001b[0m, in \u001b[0;36mGaussianLikelihood.marginal\u001b[0;34m(self, function_dist, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarginal\u001b[39m(\u001b[38;5;28mself\u001b[39m, function_dist: MultivariateNormal, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultivariateNormal:\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    :return: Analytic marginal :math:`p(\\mathbf y)`.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:116\u001b[0m, in \u001b[0;36m_GaussianLikelihoodBase.marginal\u001b[0;34m(self, function_dist, *params, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarginal\u001b[39m(\u001b[38;5;28mself\u001b[39m, function_dist: MultivariateNormal, \u001b[38;5;241m*\u001b[39mparams: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultivariateNormal:\n\u001b[1;32m    115\u001b[0m     mean, covar \u001b[38;5;241m=\u001b[39m function_dist\u001b[38;5;241m.\u001b[39mmean, function_dist\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\n\u001b[0;32m--> 116\u001b[0m     noise_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shaped_noise_covar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     full_covar \u001b[38;5;241m=\u001b[39m covar \u001b[38;5;241m+\u001b[39m noise_covar\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function_dist\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(mean, full_covar)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/gaussian_likelihood.py:39\u001b[0m, in \u001b[0;36m_GaussianLikelihoodBase._shaped_noise_covar\u001b[0;34m(self, base_shape, *params, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shaped_noise_covar\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_shape: torch\u001b[38;5;241m.\u001b[39mSize, \u001b[38;5;241m*\u001b[39mparams: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, LinearOperator]:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise_covar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/noise_models.py:23\u001b[0m, in \u001b[0;36mNoise.__call__\u001b[0;34m(self, shape, *params, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mparams: Any, shape: Optional[torch\u001b[38;5;241m.\u001b[39mSize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     21\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, LinearOperator]:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# For corredct typing\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/module.py:82\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 82\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/noise_models.py:81\u001b[0m, in \u001b[0;36m_HomoskedasticNoiseBase.forward\u001b[0;34m(self, shape, *params, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     p \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(params[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m params[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     80\u001b[0m     shape \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m p\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 81\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;241m*\u001b[39mbatch_shape, n \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m     83\u001b[0m noise_batch_shape \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m noise\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/gpytorch/likelihoods/noise_models.py:46\u001b[0m, in \u001b[0;36m_HomoskedasticNoiseBase.noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnoise\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_noise_constraint\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_noise)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/otomics-hC3faaue-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import gmean\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_data(\n",
    "    otu_file_path: str,\n",
    "    metadata_file_path: str,\n",
    "    sample_id: str = 'Sample',\n",
    "    condition_id: str = 'Study.Group'\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes OTU table and metadata, and applies a CLR transformation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        otu_table_raw = pd.read_csv(otu_file_path, index_col=0, sep='\\t' if otu_file_path.endswith('.tsv') else ',')\n",
    "        metadata = pd.read_csv(metadata_file_path, sep='\\t' if metadata_file_path.endswith('.tsv') else ',')\n",
    "\n",
    "        if sample_id not in metadata.columns:\n",
    "            raise ValueError(f\"Sample ID '{sample_id}' not found in metadata.\")\n",
    "        if condition_id not in metadata.columns:\n",
    "            raise ValueError(f\"Condition ID '{condition_id}' not found in metadata.\")\n",
    "\n",
    "        metadata.set_index(sample_id, inplace=True)\n",
    "        common_samples = otu_table_raw.index.intersection(metadata.index)\n",
    "        otu_table_aligned = otu_table_raw.loc[common_samples]\n",
    "        metadata = metadata.loc[common_samples]\n",
    "        otu_table_aligned = otu_table_aligned.reindex(metadata.index)\n",
    "\n",
    "        # Add a pseudo-count to handle zeros before CLR transformation\n",
    "        otu_table_no_zeros = otu_table_aligned + 1\n",
    "        # Calculate geometric mean for each sample (row-wise)\n",
    "        geometric_means = gmean(otu_table_no_zeros, axis=1)\n",
    "        # Apply the CLR transformation\n",
    "        otu_table_clr = np.log(otu_table_no_zeros.div(geometric_means, axis=0))\n",
    "\n",
    "        feature_table = otu_table_clr\n",
    "        feature_table = feature_table.loc[:, (feature_table != 0).any(axis=0)]\n",
    "\n",
    "        metadata['condition'] = metadata.index.str[0] + '-' + metadata[condition_id].astype(str)\n",
    "\n",
    "        return feature_table, metadata\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: One of the files was not found. Please check paths.\")\n",
    "        print(f\"OTU Path: {otu_file_path}\")\n",
    "        print(f\"Metadata Path: {metadata_file_path}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data processing: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "otu_file = \"/Users/nandini.gadhia/Documents/projects/ot_omics/notebooks/mock_counts.csv\"\n",
    "meta_file = \"/Users/nandini.gadhia/Documents/projects/ot_omics/notebooks/mock_metadata.csv\"\n",
    "\n",
    "feature_table, metadata = process_data(\n",
    "    otu_file, meta_file, sample_id='SampleID', condition_id='Group'\n",
    ")\n",
    "labels_binary = metadata['condition'].apply(lambda val: 0 if str(val).endswith('Control') else 1).values\n",
    "features = feature_table.values\n",
    "\n",
    "print(\"Data Loaded and Processed Successfully.\")\n",
    "print(f\"Feature matrix shape (features): {features.shape}\")\n",
    "print(f\"Labels array shape (labels_binary): {labels_binary.shape}\")\n",
    "\n",
    "print(\"\\nData Loaded and Processed Successfully.\")\n",
    "print(f\"Feature matrix shape (features): {features.shape}\")\n",
    "print(f\"Labels array shape (labels_binary): {labels_binary.shape}\")\n",
    "\n",
    "\n",
    "Y = torch.tensor(features, dtype=torch.float32)\n",
    "Y = (Y - Y.mean(dim=0)) / Y.std(dim=0)\n",
    "\n",
    "n_points = Y.shape[0]\n",
    "data_dim = Y.shape[1]\n",
    "latent_dim = 80\n",
    "\n",
    "# GPLVM model definition\n",
    "class GPLVM(gpytorch.models.ExactGP):\n",
    "    def __init__(self, n_points, latent_dim):\n",
    "        # A placeholder for targets, which will be updated in the training loop\n",
    "        train_x_placeholder = torch.randn(n_points, latent_dim)\n",
    "        train_y_placeholder = torch.zeros(n_points)\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        \n",
    "        super().__init__(train_x_placeholder, train_y_placeholder, likelihood)\n",
    "        \n",
    "        self.X = torch.nn.Parameter(torch.randn(n_points, latent_dim))\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "model = GPLVM(n_points, latent_dim)\n",
    "targets = [Y[:, i] for i in range(data_dim)]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.2)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "\n",
    "student_t_prior = torch.distributions.StudentT(df=3.0, loc=0.0, scale=1.0)\n",
    "laplace_prior = torch.distributions.Laplace(loc=0.0, scale=1.0)\n",
    "\n",
    "n_iterations = 200\n",
    "losses = []\n",
    "for i in range(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate the log-likelihood part of the loss\n",
    "    # This is the sum of MLLs over all output dimensions\n",
    "    log_likelihood_loss = 0\n",
    "    for j in range(data_dim):\n",
    "        model.set_train_data(inputs=model.X, targets=targets[j], strict=False)\n",
    "        output = model(model.X)\n",
    "        log_likelihood_loss -= mll(output, targets[j])\n",
    "\n",
    "    #log_prior_loss = -student_t_prior.log_prob(model.X).sum()\n",
    "    log_prior_loss = -laplace_prior.log_prob(model.X).sum()\n",
    "\n",
    "\n",
    "    # The total loss is the MAP objective: -log p(Y|X) - log p(X)\n",
    "    #total_loss = log_likelihood_loss + log_prior_loss #use this one for laplace or student's t prior\n",
    "    total_loss = log_likelihood_loss #use this one for Gaussian prior\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Iter {i+1}/{n_iterations} - Loss: {total_loss.item():.3f}\")\n",
    "        losses.append(total_loss.item())\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Plotting the training loss curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Curve (MAP Objective)\")\n",
    "plt.xlabel(\"Iteration (x10)\")\n",
    "plt.ylabel(\"Loss (-Log Posterior)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_latent = model.X.detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "controls = X_latent[labels_binary == 0]\n",
    "cases = X_latent[labels_binary == 1]\n",
    "\n",
    "plt.scatter(controls[:, 0], controls[:, 1], c='blue', label='Control (0)', alpha=0.7)\n",
    "plt.scatter(cases[:, 0], cases[:, 1], c='red', label='Case (1)', alpha=0.7)\n",
    "\n",
    "plt.title(\"GPLVM: Learned 2D Latent Space with Student's T Prior\", fontsize=16)\n",
    "plt.xlabel(\"Latent Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"Latent Dimension 2\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_latent_df = pd.DataFrame(X_latent, columns=[f'Latent_{i+1}' for i in range(latent_dim)])\n",
    "X_latent_df.to_csv('/Users/nandini.gadhia/Documents/projects/gp_omics/data_for_yue/OTU_table_mock_data_latent_Normal.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otomics-hC3faaue-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
